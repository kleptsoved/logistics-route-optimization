{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94acb33-ecb4-46bd-8119-dd4e34d76b14",
   "metadata": {},
   "source": [
    "# Customer Data Aggregation for Route Optimization\n",
    "\n",
    "This notebook extends the customer clustering functionality to include data aggregation capabilities. It allows you to:\n",
    "1. Load customer data from CSV files\n",
    "2. Select specific variables to aggregate\n",
    "3. Apply various aggregation methods (sum, mean, count, etc.)\n",
    "4. Export the aggregated data to CSV or pickle format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135a8c8-46b1-4f19-826d-b019cd3d0144",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e35e19-9dd6-436d-bbfa-18d94cbda4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress non-critical warnings\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('route_optimization')\n",
    "\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10936df7-d1a6-424b-8498-0b6802882791",
   "metadata": {},
   "source": [
    "## Cell 2: Set up project paths and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e044ba-9d7b-4884-8e49-f008a2e2b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_project():\n",
    "    \"\"\"Set up project paths and folders\"\"\"\n",
    "    project_root = Path.cwd()  # Current working directory\n",
    "    input_path = project_root.parent /  '02 Data' / '01_processed_data' / '01_clean_data'\n",
    "    output_path = project_root.parent /  '02 Data' / '01_processed_data' / '04_agregated_data'\n",
    "    \n",
    "    # Check if input directory exists\n",
    "    if not input_path.exists():\n",
    "        print(f\"Error: Input directory '{input_path}' does not exist.\")\n",
    "        print(\"Please create this directory or modify the path.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(f\"Project setup complete. \\n Input path: {input_path} \\n Output path: {output_path}\")\n",
    "    \n",
    "    return input_path, output_path\n",
    "\n",
    "def load_api_key(file_path=\"api_keys.json\"):\n",
    "    \"\"\"Load the HERE API key from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            api_keys = json.load(f)\n",
    "        api_key = api_keys.get(\"HERE_API_KEY\")\n",
    "        if not api_key:\n",
    "            print(\"⚠️ No HERE API key found in the JSON file\")\n",
    "            return None\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading API key: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667388b-65dc-446e-a204-d4f3f1ca2eb5",
   "metadata": {},
   "source": [
    "## Cell 3: Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba2fe25-955f-4c45-92e4-02da3558ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_path):\n",
    "    \"\"\"Load and parse customer data file\"\"\"\n",
    "    # List available CSV files in the input directory\n",
    "    available_files = list(input_path.glob(\"*.csv\"))\n",
    "    if not available_files:\n",
    "        print(f\"No CSV files found in {input_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"Available files:\")\n",
    "    for i, f in enumerate(available_files, start=1):\n",
    "        print(f\"{i}: {f.name}\")\n",
    "    \n",
    "    # Prompt user to choose a file by number\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(f\"Choose file number (1-{len(available_files)}): \").strip()) - 1\n",
    "            if 0 <= choice < len(available_files):\n",
    "                break\n",
    "            print(f\"Please enter a number between 1 and {len(available_files)}\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    file_path = available_files[choice]\n",
    "    \n",
    "    # Detect file encoding\n",
    "    print(f\"Detecting encoding for {file_path.name}...\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    confidence = result['confidence']\n",
    "    print(f\"Detected encoding: {encoding} (confidence: {confidence:.1%})\")\n",
    "    \n",
    "    # Analyze delimiter options\n",
    "    print(\"\\nAnalyzing potential delimiters:\\n\")\n",
    "    delimiters = [',', ';', r'\\t', '|']  # Raw string for tab to avoid escape issues\n",
    "    delimiter_options = {}\n",
    "    for i, delim in enumerate(delimiters, start=1):\n",
    "        try:\n",
    "            preview_df = pd.read_csv(file_path, engine='python', encoding=encoding, sep=delim, nrows=3)\n",
    "            col_count = len(preview_df.columns)\n",
    "            delimiter_options[i] = (delim, col_count)\n",
    "            print(f\"{i}: Delimiter '{delim}'\\n   Found {col_count} columns\")\n",
    "            print(f\"   Preview with option {i}:\")\n",
    "            display(preview_df.head(3))\n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"{i}: Error with delimiter '{delim}': {e}\")\n",
    "    \n",
    "    # Suggest the delimiter with the most columns\n",
    "    if delimiter_options:\n",
    "        suggested = max(delimiter_options, key=lambda k: delimiter_options[k][1])\n",
    "        print(f\"Suggested option: {suggested} ('{delimiter_options[suggested][0]}') with {delimiter_options[suggested][1]} columns\")\n",
    "    else:\n",
    "        print(\"No valid delimiters found. Please check the file format.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Prompt user to choose delimiter option\n",
    "    while True:\n",
    "        try:\n",
    "            delim_choice = input(f\"\\nChoose delimiter option (1-{len(delimiter_options)}) [default: {suggested}]: \").strip()\n",
    "            if not delim_choice:\n",
    "                delim_choice = suggested\n",
    "            else:\n",
    "                delim_choice = int(delim_choice)\n",
    "            if delim_choice in delimiter_options:\n",
    "                break\n",
    "            print(f\"Please enter a number between 1 and {len(delimiter_options)} or press Enter for default.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number or press Enter for default.\")\n",
    "    \n",
    "    chosen_delim, _ = delimiter_options[delim_choice]\n",
    "    print(f\"Using delimiter: '{chosen_delim}'\")\n",
    "    \n",
    "    # Load the full CSV with chosen delimiter and encoding\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=encoding, sep=chosen_delim)\n",
    "        print(f\"\\n✅ Loaded {df.shape[0]} rows × {df.shape[1]} columns from {file_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Display data overview\n",
    "    print(\"\\nData Overview:\")\n",
    "    print(f\"Column names: {', '.join(df.columns[:5])}, ... (and {len(df.columns)-5} more columns)\" if len(df.columns) > 5 else f\"Column names: {', '.join(df.columns)}\")\n",
    "    print(f\"\\nData types (first 5 columns):\\n{df.dtypes[:5]}\")\n",
    "    print(f\"... (and {len(df.columns)-5} more columns)\" if len(df.columns) > 5 else \"\")\n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head(3))\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return df, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3a7c8-8e52-4801-ae22-69dc456f4c2a",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f7e891-7c4b-4f0c-b1a7-7694acf4af14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project setup complete. \n",
      " Input path: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Notebook folder\\02 Data\\01_processed_data\\01_clean_data \n",
      " Output path: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Notebook folder\\02 Data\\01_processed_data\\02_agregated_data\n",
      "Available files:\n",
      "1: work_time_and_km_clean.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose file number (1-1):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting encoding for work_time_and_km_clean.csv...\n",
      "Detected encoding: ascii (confidence: 100.0%)\n",
      "\n",
      "Analyzing potential delimiters:\n",
      "\n",
      "1: Delimiter ','\n",
      "   Found 10 columns\n",
      "   Preview with option 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Route</th>\n",
       "      <th>Route_id</th>\n",
       "      <th>Start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>time</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 09:00:00</td>\n",
       "      <td>2025-05-16 19:25:00</td>\n",
       "      <td>10.42</td>\n",
       "      <td>115.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 09:34:00</td>\n",
       "      <td>2025-05-16 19:02:00</td>\n",
       "      <td>9.47</td>\n",
       "      <td>53.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-19</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>302</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 08:22:00</td>\n",
       "      <td>2025-05-16 17:36:00</td>\n",
       "      <td>9.23</td>\n",
       "      <td>49.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Year  Month  Day  Route  Route_id           Start_time  \\\n",
       "0  2025-03-17  2025      3   17    102         2  2025-05-16 09:00:00   \n",
       "1  2025-03-18  2025      3   18    202         2  2025-05-16 09:34:00   \n",
       "2  2025-03-19  2025      3   19    302         2  2025-05-16 08:22:00   \n",
       "\n",
       "              end_time   time  distance  \n",
       "0  2025-05-16 19:25:00  10.42    115.00  \n",
       "1  2025-05-16 19:02:00   9.47     53.19  \n",
       "2  2025-05-16 17:36:00   9.23     49.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2: Delimiter ';'\n",
      "   Found 1 columns\n",
      "   Preview with option 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance\n",
       "0  2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...                  \n",
       "1  2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...                  \n",
       "2  2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3: Delimiter '\\t'\n",
      "   Found 1 columns\n",
      "   Preview with option 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance\n",
       "0  2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...                  \n",
       "1  2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...                  \n",
       "2  2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4: Delimiter '|'\n",
      "   Found 1 columns\n",
      "   Preview with option 4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date,Year,Month,Day,Route,Route_id,Start_time,end_time,time,distance\n",
       "0  2025-03-17,2025,3,17,102,2,2025-05-16 09:00:00...                  \n",
       "1  2025-03-18,2025,3,18,202,2,2025-05-16 09:34:00...                  \n",
       "2  2025-03-19,2025,3,19,302,2,2025-05-16 08:22:00...                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Suggested option: 1 (',') with 10 columns\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose delimiter option (1-4) [default: 1]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using delimiter: ','\n",
      "\n",
      "✅ Loaded 20 rows × 10 columns from work_time_and_km_clean.csv\n",
      "\n",
      "Data Overview:\n",
      "Column names: Date, Year, Month, Day, Route, ... (and 5 more columns)\n",
      "\n",
      "Data types (first 5 columns):\n",
      "Date     object\n",
      "Year      int64\n",
      "Month     int64\n",
      "Day       int64\n",
      "Route     int64\n",
      "dtype: object\n",
      "... (and 5 more columns)\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Route</th>\n",
       "      <th>Route_id</th>\n",
       "      <th>Start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>time</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 09:00:00</td>\n",
       "      <td>2025-05-16 19:25:00</td>\n",
       "      <td>10.42</td>\n",
       "      <td>115.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 09:34:00</td>\n",
       "      <td>2025-05-16 19:02:00</td>\n",
       "      <td>9.47</td>\n",
       "      <td>53.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-19</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>302</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-16 08:22:00</td>\n",
       "      <td>2025-05-16 17:36:00</td>\n",
       "      <td>9.23</td>\n",
       "      <td>49.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Year  Month  Day  Route  Route_id           Start_time  \\\n",
       "0  2025-03-17  2025      3   17    102         2  2025-05-16 09:00:00   \n",
       "1  2025-03-18  2025      3   18    202         2  2025-05-16 09:34:00   \n",
       "2  2025-03-19  2025      3   19    302         2  2025-05-16 08:22:00   \n",
       "\n",
       "              end_time   time  distance  \n",
       "0  2025-05-16 19:25:00  10.42    115.00  \n",
       "1  2025-05-16 19:02:00   9.47     53.19  \n",
       "2  2025-05-16 17:36:00   9.23     49.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize project and load data\n",
    "input_path, output_path = setup_project()\n",
    "df, file_path = load_data(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6f82e-f5c2-49a8-9de3-61e64c77e7d0",
   "metadata": {},
   "source": [
    "## Cell 5: Data Aggregation - Introduction\n",
    "\n",
    "This section allows you to aggregate your data based on specific variables. Aggregation is useful for:\n",
    "- Summarizing data by groups (e.g., customers by region)\n",
    "- Computing statistics (sums, averages, etc.) for each group\n",
    "- Preparing data for further analysis or visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9def07c-d88a-48b3-871a-9e2cfb6d95b5",
   "metadata": {},
   "source": [
    "## Cell 6: Adding New Variables (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89da8fb-a4d6-487a-a1bc-fc16cad0ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to add new calculated variables before aggregation? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the addition of new variables.\n"
     ]
    }
   ],
   "source": [
    "def add_new_variables(df):\n",
    "    \"\"\"Allow the user to add new calculated variables to the DataFrame\"\"\"\n",
    "    \n",
    "    add_vars = input(\"Would you like to add new calculated variables before aggregation? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if add_vars not in ['yes', 'y']:\n",
    "        print(\"Skipping the addition of new variables.\")\n",
    "        return df\n",
    "    \n",
    "    print(\"\\n=== Adding New Variables ===\")\n",
    "    print(\"This feature allows you to create new columns based on existing data.\")\n",
    "    \n",
    "    # Make a copy of the dataframe to avoid modifying the original\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nAvailable columns:\")\n",
    "        for i, col in enumerate(df_modified.columns, 1):\n",
    "            print(f\"{i}: {col}\")\n",
    "        \n",
    "        new_var_name = input(\"\\nEnter name for the new variable (or 'done' to finish): \").strip()\n",
    "        \n",
    "        if new_var_name.lower() == 'done':\n",
    "            break\n",
    "            \n",
    "        if new_var_name in df_modified.columns:\n",
    "            print(f\"⚠️ Column '{new_var_name}' already exists. Please choose a different name.\")\n",
    "            continue\n",
    "            \n",
    "        print(\"\\nChoose operation type:\")\n",
    "        print(\"1: Simple arithmetic on one column (e.g., multiply a column by a value)\")\n",
    "        print(\"2: Operation between two columns (e.g., sum of two columns)\")\n",
    "        print(\"3: Apply a condition (e.g., if column X > 10 then 'High' else 'Low')\")\n",
    "        \n",
    "        try:\n",
    "            op_type = int(input(\"Enter operation type (1-3): \").strip())\n",
    "            \n",
    "            if op_type == 1:\n",
    "                col_idx = int(input(\"Choose column number: \").strip()) - 1\n",
    "                if 0 <= col_idx < len(df_modified.columns):\n",
    "                    col_name = df_modified.columns[col_idx]\n",
    "                    operation = input(\"Enter operation (+, -, *, / followed by a number, e.g., '*2'): \").strip()\n",
    "                    \n",
    "                    op_char = operation[0]\n",
    "                    try:\n",
    "                        value = float(operation[1:])\n",
    "                        \n",
    "                        if op_char == '+':\n",
    "                            df_modified[new_var_name] = df_modified[col_name] + value\n",
    "                        elif op_char == '-':\n",
    "                            df_modified[new_var_name] = df_modified[col_name] - value\n",
    "                        elif op_char == '*':\n",
    "                            df_modified[new_var_name] = df_modified[col_name] * value\n",
    "                        elif op_char == '/':\n",
    "                            df_modified[new_var_name] = df_modified[col_name] / value\n",
    "                        else:\n",
    "                            print(\"Invalid operation. Please use +, -, *, or /\")\n",
    "                            continue\n",
    "                            \n",
    "                        print(f\"✅ Created new column '{new_var_name}'\")\n",
    "                        print(df_modified[[col_name, new_var_name]].head())\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid number in operation\")\n",
    "                else:\n",
    "                    print(\"Invalid column number.\")\n",
    "            \n",
    "            elif op_type == 2:\n",
    "                col1_idx = int(input(\"Choose first column number: \").strip()) - 1\n",
    "                col2_idx = int(input(\"Choose second column number: \").strip()) - 1\n",
    "                \n",
    "                if 0 <= col1_idx < len(df_modified.columns) and 0 <= col2_idx < len(df_modified.columns):\n",
    "                    col1_name = df_modified.columns[col1_idx]\n",
    "                    col2_name = df_modified.columns[col2_idx]\n",
    "                    \n",
    "                    operation = input(\"Enter operation between columns (+, -, *, /): \").strip()\n",
    "                    \n",
    "                    if operation == '+':\n",
    "                        df_modified[new_var_name] = df_modified[col1_name] + df_modified[col2_name]\n",
    "                    elif operation == '-':\n",
    "                        df_modified[new_var_name] = df_modified[col1_name] - df_modified[col2_name]\n",
    "                    elif operation == '*':\n",
    "                        df_modified[new_var_name] = df_modified[col1_name] * df_modified[col2_name]\n",
    "                    elif operation == '/':\n",
    "                        df_modified[new_var_name] = df_modified[col1_name] / df_modified[col2_name]\n",
    "                    else:\n",
    "                        print(\"Invalid operation. Please use +, -, *, or /\")\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"✅ Created new column '{new_var_name}'\")\n",
    "                    print(df_modified[[col1_name, col2_name, new_var_name]].head())\n",
    "                else:\n",
    "                    print(\"Invalid column number(s).\")\n",
    "            \n",
    "            elif op_type == 3:\n",
    "                col_idx = int(input(\"Choose column number for condition: \").strip()) - 1\n",
    "                if 0 <= col_idx < len(df_modified.columns):\n",
    "                    col_name = df_modified.columns[col_idx]\n",
    "                    \n",
    "                    condition_type = input(\"Choose condition type (>, <, ==, >=, <=): \").strip()\n",
    "                    threshold = input(\"Enter threshold value: \").strip()\n",
    "                    \n",
    "                    try:\n",
    "                        threshold_val = float(threshold)\n",
    "                        true_value = input(\"Value if condition is true: \").strip()\n",
    "                        false_value = input(\"Value if condition is false: \").strip()\n",
    "                        \n",
    "                        # Try to convert true/false values to numbers if possible\n",
    "                        try:\n",
    "                            true_value = float(true_value)\n",
    "                            false_value = float(false_value)\n",
    "                        except ValueError:\n",
    "                            # Keep as strings if conversion fails\n",
    "                            pass\n",
    "                        \n",
    "                        if condition_type == '>':\n",
    "                            df_modified[new_var_name] = np.where(df_modified[col_name] > threshold_val, true_value, false_value)\n",
    "                        elif condition_type == '<':\n",
    "                            df_modified[new_var_name] = np.where(df_modified[col_name] < threshold_val, true_value, false_value)\n",
    "                        elif condition_type == '==':\n",
    "                            df_modified[new_var_name] = np.where(df_modified[col_name] == threshold_val, true_value, false_value)\n",
    "                        elif condition_type == '>=':\n",
    "                            df_modified[new_var_name] = np.where(df_modified[col_name] >= threshold_val, true_value, false_value)\n",
    "                        elif condition_type == '<=':\n",
    "                            df_modified[new_var_name] = np.where(df_modified[col_name] <= threshold_val, true_value, false_value)\n",
    "                        else:\n",
    "                            print(\"Invalid condition type.\")\n",
    "                            continue\n",
    "                            \n",
    "                        print(f\"✅ Created new column '{new_var_name}'\")\n",
    "                        print(df_modified[[col_name, new_var_name]].head())\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid threshold value\")\n",
    "                else:\n",
    "                    print(\"Invalid column number.\")\n",
    "            else:\n",
    "                print(\"Invalid operation type. Please enter a number between 1 and 3.\")\n",
    "                \n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"\\n✅ Added {len(df_modified.columns) - len(df.columns)} new variables to the dataset.\")\n",
    "    return df_modified\n",
    "\n",
    "# Execute the function\n",
    "df = add_new_variables(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d07c1c-8a18-4ac7-ba3b-42b75c15a7a1",
   "metadata": {},
   "source": [
    "## Cell 7: Variable Selection for Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e5ce96-a64e-47f0-b2fa-a6dafb980401",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def select_aggregation_variables(df):\n",
    "    \"\"\"Allow the user to select variables for aggregation\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Data Aggregation Setup ===\")\n",
    "    \n",
    "    # Select groupby variables\n",
    "    print(\"\\nStep 1: Select columns to group by (e.g., region, customer_type)\")\n",
    "    print(\"Available columns:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"{i}: {col}\")\n",
    "    \n",
    "    groupby_indices = input(\"\\nEnter column numbers to group by (comma-separated) or 'cancel' to skip: \").strip()\n",
    "    \n",
    "    if groupby_indices.lower() == 'cancel':\n",
    "        print(\"Aggregation cancelled.\")\n",
    "        return df, None, None  # Return 3 values consistently\n",
    "        \n",
    "    try:\n",
    "        groupby_indices = [int(idx.strip()) - 1 for idx in groupby_indices.split(',')]\n",
    "        groupby_cols = [df.columns[idx] for idx in groupby_indices if 0 <= idx < len(df.columns)]\n",
    "        \n",
    "        if not groupby_cols:\n",
    "            print(\"⚠️ No valid groupby columns selected. Skipping aggregation.\")\n",
    "            return df, None, None  # Return 3 values consistently\n",
    "            \n",
    "        print(f\"Selected groupby columns: {', '.join(groupby_cols)}\")\n",
    "    except ValueError:\n",
    "        print(\"⚠️ Invalid input. Skipping aggregation.\")\n",
    "        return df, None, None\n",
    "    \n",
    "    # Select value variables and aggregation methods\n",
    "    print(\"\\nStep 2: Select columns to aggregate and their aggregation methods\")\n",
    "    print(\"Available columns:\")\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"⚠️ No numeric columns found for aggregation.\")\n",
    "        return df, None, None\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols, 1):\n",
    "        print(f\"{i}: {col}\")\n",
    "    \n",
    "    agg_cols_input = input(\"\\nEnter column numbers to aggregate (comma-separated): \").strip()\n",
    "    try:\n",
    "        agg_indices = [int(idx.strip()) - 1 for idx in agg_cols_input.split(',')]\n",
    "        agg_cols = [numeric_cols[idx] for idx in agg_indices if 0 <= idx < len(numeric_cols)]\n",
    "        \n",
    "        if not agg_cols:\n",
    "            print(\"⚠️ No valid aggregation columns selected. Skipping aggregation.\")\n",
    "            return df, None, None\n",
    "            \n",
    "        print(f\"Selected aggregation columns: {', '.join(agg_cols)}\")\n",
    "    except ValueError:\n",
    "        print(\"⚠️ Invalid input. Skipping aggregation.\")\n",
    "        return df, None, None\n",
    "    \n",
    "    # For each selected column, ask for aggregation method\n",
    "    agg_methods = {}\n",
    "    print(\"\\nAvailable aggregation methods:\")\n",
    "    print(\"1: sum - Sum of values\")\n",
    "    print(\"2: mean - Average of values\")\n",
    "    print(\"3: median - Median of values\")\n",
    "    print(\"4: min - Minimum value\")\n",
    "    print(\"5: max - Maximum value\")\n",
    "    print(\"6: count - Count of values\")\n",
    "    print(\"7: std - Standard deviation\")\n",
    "    \n",
    "    for col in agg_cols:\n",
    "        while True:\n",
    "            method_input = input(f\"Choose aggregation method(s) for '{col}' (comma-separated numbers): \").strip()\n",
    "            try:\n",
    "                methods = []\n",
    "                for m in method_input.split(','):\n",
    "                    m = int(m.strip())\n",
    "                    if 1 <= m <= 7:\n",
    "                        if m == 1:\n",
    "                            methods.append('sum')\n",
    "                        elif m == 2:\n",
    "                            methods.append('mean')\n",
    "                        elif m == 3:\n",
    "                            methods.append('median')\n",
    "                        elif m == 4:\n",
    "                            methods.append('min')\n",
    "                        elif m == 5:\n",
    "                            methods.append('max')\n",
    "                        elif m == 6:\n",
    "                            methods.append('count')\n",
    "                        elif m == 7:\n",
    "                            methods.append('std')\n",
    "                \n",
    "                if methods:\n",
    "                    agg_methods[col] = methods\n",
    "                    print(f\"For '{col}', will calculate: {', '.join(methods)}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please select at least one valid method.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter valid method numbers.\")\n",
    "    \n",
    "    return df, groupby_cols, agg_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc59e0-6d58-454c-bed2-fc1dbbe24f5e",
   "metadata": {},
   "source": [
    "## Cell 8: Perform Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6843fa-26b3-4ffb-bba5-f11de766eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_aggregation(df, groupby_cols, agg_methods):\n",
    "    \"\"\"Perform aggregation and write results to specified columns in original DataFrame\"\"\"\n",
    "    \n",
    "    if groupby_cols is None or agg_methods is None:\n",
    "        print(\"Skipping aggregation due to incomplete setup.\")\n",
    "        return df\n",
    "    \n",
    "    print(\"\\n=== Performing Aggregation ===\")\n",
    "    \n",
    "    try:\n",
    "        # Create a copy to work with\n",
    "        df_original = df.copy()\n",
    "        \n",
    "        # Ask for target columns for each aggregation\n",
    "        target_columns = {}\n",
    "        for col, methods in agg_methods.items():\n",
    "            for method in methods:\n",
    "                desc = f\"{col}_{method}\"\n",
    "                print(f\"\\nWhere should the result of {desc} be stored?\")\n",
    "                print(\"1: Create a new column\")\n",
    "                print(\"2: Use an existing column\")\n",
    "                \n",
    "                choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "                \n",
    "                if choice == \"1\":\n",
    "                    suggested_name = f\"{col}_{method}\"\n",
    "                    col_name = input(f\"Enter new column name [default: {suggested_name}]: \").strip()\n",
    "                    if not col_name:\n",
    "                        col_name = suggested_name\n",
    "                    \n",
    "                    # Check if column already exists\n",
    "                    if col_name in df_original.columns:\n",
    "                        overwrite = input(f\"Column '{col_name}' already exists. Overwrite? (yes/no): \").strip().lower()\n",
    "                        if overwrite not in ['yes', 'y']:\n",
    "                            print(\"Skipping this aggregation.\")\n",
    "                            continue\n",
    "                else:\n",
    "                    # Let user select existing column\n",
    "                    print(\"\\nAvailable columns:\")\n",
    "                    for i, existing_col in enumerate(df_original.columns, 1):\n",
    "                        print(f\"{i}: {existing_col}\")\n",
    "                    \n",
    "                    while True:\n",
    "                        try:\n",
    "                            idx = int(input(\"\\nChoose column number: \").strip()) - 1\n",
    "                            if 0 <= idx < len(df_original.columns):\n",
    "                                col_name = df_original.columns[idx]\n",
    "                                break\n",
    "                            print(f\"Please enter a number between 1 and {len(df_original.columns)}\")\n",
    "                        except ValueError:\n",
    "                            print(\"Please enter a valid number.\")\n",
    "                \n",
    "                target_columns[(col, method)] = col_name\n",
    "        \n",
    "        # Perform the groupby operation to get aggregated values\n",
    "        grouped = df_original.groupby(groupby_cols)\n",
    "        \n",
    "        # For each aggregation method, compute and merge back\n",
    "        for (source_col, method), target_col in target_columns.items():\n",
    "            print(f\"Calculating {source_col}_{method} and storing in {target_col}...\")\n",
    "            \n",
    "            # Compute the specific aggregation\n",
    "            if method == 'sum':\n",
    "                agg_result = grouped[source_col].sum().reset_index()\n",
    "            elif method == 'mean':\n",
    "                agg_result = grouped[source_col].mean().reset_index()\n",
    "            elif method == 'median':\n",
    "                agg_result = grouped[source_col].median().reset_index()\n",
    "            elif method == 'min':\n",
    "                agg_result = grouped[source_col].min().reset_index()\n",
    "            elif method == 'max':\n",
    "                agg_result = grouped[source_col].max().reset_index()\n",
    "            elif method == 'count':\n",
    "                agg_result = grouped[source_col].count().reset_index()\n",
    "            elif method == 'std':\n",
    "                agg_result = grouped[source_col].std().reset_index()\n",
    "            \n",
    "            # Rename the aggregated column to match with our target\n",
    "            agg_result = agg_result.rename(columns={source_col: target_col})\n",
    "            \n",
    "            # Merge the aggregated result back to the original dataframe\n",
    "            df_original = pd.merge(\n",
    "                df_original, \n",
    "                agg_result,\n",
    "                on=groupby_cols, \n",
    "                how='left',\n",
    "                suffixes=('', '_aggregated')\n",
    "            )\n",
    "            \n",
    "            # If the column already existed and has a suffix now, replace the original\n",
    "            if f\"{target_col}_aggregated\" in df_original.columns:\n",
    "                df_original[target_col] = df_original[f\"{target_col}_aggregated\"]\n",
    "                df_original = df_original.drop(columns=[f\"{target_col}_aggregated\"])\n",
    "        \n",
    "        print(f\"\\n✅ Aggregation complete. Added {len(target_columns)} aggregated values to the original dataset.\")\n",
    "        print(\"\\nUpdated data preview:\")\n",
    "        print(df_original.head())\n",
    "        \n",
    "        return df_original\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during aggregation: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2b307-afa6-43cb-a5cc-f2f7afee2f81",
   "metadata": {},
   "source": [
    "## Cell 9 – Run Multiple Aggregation Cycles and Export Aggregated Data\n",
    "This section orchestrates repeated aggregation passes and writes the final dataset to disk.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6aaaac-fa06-4544-b981-124e73ee293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Aggregation Setup ===\n",
      "\n",
      "Step 1: Select columns to group by (e.g., region, customer_type)\n",
      "Available columns:\n",
      "1: Date\n",
      "2: Year\n",
      "3: Month\n",
      "4: Day\n",
      "5: Route\n",
      "6: Route_id\n",
      "7: Start_time\n",
      "8: end_time\n",
      "9: time\n",
      "10: distance\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter column numbers to group by (comma-separated) or 'cancel' to skip:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected groupby columns: Route_id\n",
      "\n",
      "Step 2: Select columns to aggregate and their aggregation methods\n",
      "Available columns:\n",
      "1: Year\n",
      "2: Month\n",
      "3: Day\n",
      "4: Route\n",
      "5: Route_id\n",
      "6: time\n",
      "7: distance\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter column numbers to aggregate (comma-separated):  6,7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected aggregation columns: time, distance\n",
      "\n",
      "Available aggregation methods:\n",
      "1: sum - Sum of values\n",
      "2: mean - Average of values\n",
      "3: median - Median of values\n",
      "4: min - Minimum value\n",
      "5: max - Maximum value\n",
      "6: count - Count of values\n",
      "7: std - Standard deviation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose aggregation method(s) for 'time' (comma-separated numbers):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'time', will calculate: sum\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose aggregation method(s) for 'distance' (comma-separated numbers):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'distance', will calculate: sum\n",
      "\n",
      "=== Performing Aggregation ===\n",
      "\n",
      "Where should the result of time_sum be stored?\n",
      "1: Create a new column\n",
      "2: Use an existing column\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1 or 2):  1\n",
      "Enter new column name [default: time_sum]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Where should the result of distance_sum be stored?\n",
      "1: Create a new column\n",
      "2: Use an existing column\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1 or 2):  1\n",
      "Enter new column name [default: distance_sum]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating time_sum and storing in time_sum...\n",
      "Calculating distance_sum and storing in distance_sum...\n",
      "\n",
      "✅ Aggregation complete. Added 2 aggregated values to the original dataset.\n",
      "\n",
      "Updated data preview:\n",
      "         Date  Year  Month  Day  Route  Route_id           Start_time  \\\n",
      "0  2025-03-17  2025      3   17    102         2  2025-05-16 09:00:00   \n",
      "1  2025-03-18  2025      3   18    202         2  2025-05-16 09:34:00   \n",
      "2  2025-03-19  2025      3   19    302         2  2025-05-16 08:22:00   \n",
      "3  2025-03-20  2025      3   20    402         2  2025-05-16 08:32:00   \n",
      "4  2025-03-21  2025      3   21    502         2  2025-05-16 07:30:00   \n",
      "\n",
      "              end_time   time  distance  time_sum  distance_sum  \n",
      "0  2025-05-16 19:25:00  10.42    115.00     47.77        361.73  \n",
      "1  2025-05-16 19:02:00   9.47     53.19     47.77        361.73  \n",
      "2  2025-05-16 17:36:00   9.23     49.32     47.77        361.73  \n",
      "3  2025-05-16 18:01:00   9.48     43.14     47.77        361.73  \n",
      "4  2025-05-16 16:40:00   9.17    101.08     47.77        361.73  \n",
      "\n",
      "=== Aggregation Cycle Complete ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run another aggregation cycle with different groupby variables? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing aggregation process.\n",
      "\n",
      "=== Data Export ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter output filename (without extension) [default: aggregated_data]:  work_time_and_km_clean_aggregated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose export format:\n",
      "1: CSV (.csv) - Readable text format, compatible with Excel and other tools\n",
      "2: Pickle (.pkl) - Preserves data types, faster for loading back into Python\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter format choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV Export Options:\n",
      "1: Default settings (comma separator, include index)\n",
      "2: Custom settings\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose CSV export option (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data exported to CSV: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Notebook folder\\02 Data\\01_processed_data\\02_agregated_data\\work_time_and_km_clean_aggregated.csv\n"
     ]
    }
   ],
   "source": [
    "def run_aggregation_cycles(df, input_path, output_path):\n",
    "    \"\"\"Run multiple aggregation cycles if the user requests it\"\"\"\n",
    "    \n",
    "    # Start with the original data\n",
    "    current_df = df.copy()\n",
    "    \n",
    "    # Track if any aggregation was performed\n",
    "    aggregation_performed = False\n",
    "    \n",
    "    while True:\n",
    "        # Run a single aggregation cycle\n",
    "        _, current_groupby_cols, current_agg_methods = select_aggregation_variables(current_df)\n",
    "        \n",
    "        if current_groupby_cols is None or current_agg_methods is None:\n",
    "            print(\"Skipping this aggregation cycle.\")\n",
    "            # If no aggregation was performed yet, return the original data\n",
    "            if not aggregation_performed:\n",
    "                return df, False\n",
    "            # Otherwise, use the current state with previous aggregations\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Perform the current aggregation\n",
    "        current_df = perform_aggregation(current_df, current_groupby_cols, current_agg_methods)\n",
    "        \n",
    "        # Mark that at least one aggregation was performed\n",
    "        aggregation_performed = True\n",
    "        \n",
    "        # Ask if another cycle is desired\n",
    "        print(\"\\n=== Aggregation Cycle Complete ===\")\n",
    "        another_cycle = input(\"\\nDo you want to run another aggregation cycle with different groupby variables? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if another_cycle not in ['yes', 'y']:\n",
    "            print(\"Completing aggregation process.\")\n",
    "            break\n",
    "        \n",
    "        print(\"\\n=== Starting New Aggregation Cycle ===\")\n",
    "    \n",
    "    return current_df, aggregation_performed\n",
    "\n",
    "def export_data(df, output_path):\n",
    "    \"\"\"Export the data to CSV or pickle format\"\"\"\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to export.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n=== Data Export ===\")\n",
    "    \n",
    "    # Ask for the output filename\n",
    "    default_filename = \"aggregated_data\"\n",
    "    filename = input(f\"Enter output filename (without extension) [default: {default_filename}]: \").strip()\n",
    "    if not filename:\n",
    "        filename = default_filename\n",
    "    \n",
    "    # Ask for export format\n",
    "    print(\"\\nChoose export format:\")\n",
    "    print(\"1: CSV (.csv) - Readable text format, compatible with Excel and other tools\")\n",
    "    print(\"2: Pickle (.pkl) - Preserves data types, faster for loading back into Python\")\n",
    "    \n",
    "    export_format = input(\"Enter format choice (1 or 2): \").strip()\n",
    "    \n",
    "    try:\n",
    "        if export_format == \"1\":\n",
    "            # CSV export options\n",
    "            print(\"\\nCSV Export Options:\")\n",
    "            print(\"1: Default settings (comma separator, include index)\")\n",
    "            print(\"2: Custom settings\")\n",
    "            \n",
    "            csv_option = input(\"Choose CSV export option (1 or 2): \").strip()\n",
    "            \n",
    "            if csv_option == \"2\":\n",
    "                # Custom CSV settings\n",
    "                sep = input(\"Enter separator character [default: ,]: \").strip() or \",\"\n",
    "                include_index = input(\"Include row indices? (yes/no) [default: no]: \").strip().lower() in [\"yes\", \"y\"]\n",
    "                include_header = input(\"Include column headers? (yes/no) [default: yes]: \").strip().lower() not in [\"no\", \"n\"]\n",
    "                \n",
    "                filepath = output_path / f\"{filename}.csv\"\n",
    "                df.to_csv(filepath, sep=sep, index=include_index, header=include_header)\n",
    "            else:\n",
    "                # Default CSV settings\n",
    "                filepath = output_path / f\"{filename}.csv\"\n",
    "                df.to_csv(filepath, index=False)\n",
    "            \n",
    "            print(f\"✅ Data exported to CSV: {filepath}\")\n",
    "            \n",
    "        elif export_format == \"2\":\n",
    "            # Pickle export\n",
    "            filepath = output_path / f\"{filename}.pkl\"\n",
    "            df.to_pickle(filepath)\n",
    "            print(f\"✅ Data exported to Pickle: {filepath}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid format choice. Defaulting to CSV.\")\n",
    "            filepath = output_path / f\"{filename}.csv\"\n",
    "            df.to_csv(filepath, index=False)\n",
    "            print(f\"✅ Data exported to CSV: {filepath}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during export: {e}\")\n",
    "\n",
    "# Execute the aggregation cycles\n",
    "df_aggregated, aggregation_was_performed = run_aggregation_cycles(df, input_path, output_path)\n",
    "\n",
    "# Decide whether to export based on whether aggregation was performed\n",
    "if aggregation_was_performed:\n",
    "    export_data(df_aggregated, output_path)\n",
    "else:\n",
    "    print(\"\\nNo data aggregation was performed. Skipping the export step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e7f57-89df-4bdb-a5a5-4c8eed651d16",
   "metadata": {},
   "source": [
    "## Cell 10: Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c76a40-7c14-4bc4-93ed-fccc75981cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Summary ===\n",
      "Input file: work_time_and_km_clean.csv\n",
      "Original data: 20 rows × 10 columns\n",
      "Final data after aggregation: 20 rows × 12 columns\n",
      "Number of new columns added: 2\n",
      "\n",
      "✅ Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of what was done\n",
    "print(\"\\n=== Processing Summary ===\")\n",
    "print(f\"Input file: {file_path.name}\")\n",
    "\n",
    "if aggregation_was_performed:\n",
    "    # If aggregation was performed\n",
    "    print(f\"Original data: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    print(f\"Final data after aggregation: {df_aggregated.shape[0]} rows × {df_aggregated.shape[1]} columns\")\n",
    "    print(f\"Number of new columns added: {df_aggregated.shape[1] - df.shape[1]}\")\n",
    "else:\n",
    "    # If no aggregation was performed\n",
    "    print(f\"Data loaded: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    print(\"No aggregation was performed.\")\n",
    "\n",
    "print(\"\\n✅ Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
