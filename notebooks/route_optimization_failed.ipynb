{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94acb33-ecb4-46bd-8119-dd4e34d76b14",
   "metadata": {},
   "source": [
    "# Customer Clustering Using K-Means for Route Optimization\n",
    "\n",
    "This section performs customer clustering based on geographical coordinates using the K-Means algorithm.\n",
    "The goal is to group nearby customers together to support optimized delivery routes in Estonia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135a8c8-46b1-4f19-826d-b019cd3d0144",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e35e19-9dd6-436d-bbfa-18d94cbda4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress non-critical warnings\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('route_optimization')\n",
    "\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10936df7-d1a6-424b-8498-0b6802882791",
   "metadata": {},
   "source": [
    "## Cell 2: Set up project paths and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e044ba-9d7b-4884-8e49-f008a2e2b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing project setup functions...\n",
      "✅ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def setup_project():\n",
    "    \"\"\"Set up project paths and folders\"\"\"\n",
    "    project_root = Path.cwd()  # Current working directory\n",
    "    input_path = project_root.parent / '02 Data' / 'Processed_data'\n",
    "    output_path = project_root.parent / '02 Data' / 'Processed_data'\n",
    "    \n",
    "    # Check if input directory exists\n",
    "    if not input_path.exists():\n",
    "        print(f\"Error: Input directory '{input_path}' does not exist.\")\n",
    "        print(\"Please create this directory or modify the path.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(f\"Project setup complete. \\n Input path: {input_path} \\n Output path: {output_path}\")\n",
    "    \n",
    "    return input_path, output_path\n",
    "\n",
    "def load_api_key(file_path=\"api_keys.json\"):\n",
    "    \"\"\"Load the HERE API key from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            api_keys = json.load(f)\n",
    "        api_key = api_keys.get(\"HERE_API_KEY\")\n",
    "        if not api_key:\n",
    "            print(\"⚠️ No HERE API key found in the JSON file\")\n",
    "            return None\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test these functions\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing project setup functions...\")\n",
    "    # Comment out if you just want to define the functions without running\n",
    "    api_key = load_api_key()\n",
    "    if api_key:\n",
    "        print(f\"✅ API key loaded successfully\")\n",
    "    else:\n",
    "        print(\"⚠️ No API key found. Will use fallback methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667388b-65dc-446e-a204-d4f3f1ca2eb5",
   "metadata": {},
   "source": [
    "## Cell 3: Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba2fe25-955f-4c45-92e4-02da3558ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_path):\n",
    "    \"\"\"Load and parse customer data file\"\"\"\n",
    "    # List available CSV files in the input directory\n",
    "    available_files = list(input_path.glob(\"*.csv\"))\n",
    "    if not available_files:\n",
    "        print(f\"No CSV files found in {input_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"Available files:\")\n",
    "    for i, f in enumerate(available_files, start=1):\n",
    "        print(f\"{i}: {f.name}\")\n",
    "    \n",
    "    # Prompt user to choose a file by number\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(f\"Choose file number (1-{len(available_files)}): \").strip()) - 1\n",
    "            if 0 <= choice < len(available_files):\n",
    "                break\n",
    "            print(f\"Please enter a number between 1 and {len(available_files)}\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    file_path = available_files[choice]\n",
    "    \n",
    "    # Detect file encoding\n",
    "    print(f\"Detecting encoding for {file_path.name}...\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    confidence = result['confidence']\n",
    "    print(f\"Detected encoding: {encoding} (confidence: {confidence:.1%})\")\n",
    "    \n",
    "    # Analyze delimiter options\n",
    "    print(\"\\nAnalyzing potential delimiters:\\n\")\n",
    "    delimiters = [',', ';', r'\\t', '|']  # Raw string for tab to avoid escape issues\n",
    "    delimiter_options = {}\n",
    "    for i, delim in enumerate(delimiters, start=1):\n",
    "        try:\n",
    "            preview_df = pd.read_csv(file_path, engine='python', encoding=encoding, sep=delim, nrows=3)\n",
    "            col_count = len(preview_df.columns)\n",
    "            delimiter_options[i] = (delim, col_count)\n",
    "            print(f\"{i}: Delimiter '{delim}'\\n   Found {col_count} columns\")\n",
    "            print(f\"   Preview with option {i}:\")\n",
    "            print(preview_df.head(3))\n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"{i}: Error with delimiter '{delim}': {e}\")\n",
    "    \n",
    "    # Suggest the delimiter with the most columns\n",
    "    if delimiter_options:\n",
    "        suggested = max(delimiter_options, key=lambda k: delimiter_options[k][1])\n",
    "        print(f\"Suggested option: {suggested} ('{delimiter_options[suggested][0]}') with {delimiter_options[suggested][1]} columns\")\n",
    "    else:\n",
    "        print(\"No valid delimiters found. Please check the file format.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Prompt user to choose delimiter option\n",
    "    while True:\n",
    "        try:\n",
    "            delim_choice = input(f\"\\nChoose delimiter option (1-{len(delimiter_options)}) [default: {suggested}]: \").strip()\n",
    "            if not delim_choice:\n",
    "                delim_choice = suggested\n",
    "            else:\n",
    "                delim_choice = int(delim_choice)\n",
    "            if delim_choice in delimiter_options:\n",
    "                break\n",
    "            print(f\"Please enter a number between 1 and {len(delimiter_options)} or press Enter for default.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number or press Enter for default.\")\n",
    "    \n",
    "    chosen_delim, _ = delimiter_options[delim_choice]\n",
    "    print(f\"Using delimiter: '{chosen_delim}'\")\n",
    "    \n",
    "    # Load the full CSV with chosen delimiter and encoding\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=encoding, sep=chosen_delim)\n",
    "        print(f\"\\n✅ Loaded {df.shape[0]} rows × {df.shape[1]} columns from {file_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Display data overview\n",
    "    print(\"\\nData Overview:\")\n",
    "    print(f\"Column names: {', '.join(df.columns[:5])}, ... (and {len(df.columns)-5} more columns)\" if len(df.columns) > 5 else f\"Column names: {', '.join(df.columns)}\")\n",
    "    print(f\"\\nData types (first 5 columns):\\n{df.dtypes[:5]}\")\n",
    "    print(f\"... (and {len(df.columns)-5} more columns)\" if len(df.columns) > 5 else \"\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ed774-9a20-4d1a-9bb0-6c203c1aca21",
   "metadata": {},
   "source": [
    "## Cell 4: Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04201792-3fe9-4966-8224-0987b1bbea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df):\n",
    "    \"\"\"Validate the data before processing\"\"\"\n",
    "    print(\"\\n=== VALIDATING DATA QUALITY ===\")\n",
    "    \n",
    "    # Essential columns for route optimization\n",
    "    required_columns = ['latitude', 'longitude']\n",
    "    id_columns = ['ABS Custumer no', 'ABS Customer no', 'customer_id']  # Check both spellings\n",
    "    \n",
    "    # Check for presence of required columns\n",
    "    missing_required = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_required:\n",
    "        print(f\"❌ ERROR: Missing required columns: {', '.join(missing_required)}\")\n",
    "        print(\"Route optimization requires latitude and longitude coordinates.\")\n",
    "        raise ValueError(f\"Missing required columns: {missing_required}\")\n",
    "    else:\n",
    "        print(\"✅ Required location columns present\")\n",
    "    \n",
    "    # Check if any ID column exists\n",
    "    available_id_columns = [col for col in id_columns if col in df.columns]\n",
    "    if available_id_columns:\n",
    "        print(f\"✅ ID column(s) found: {', '.join(available_id_columns)}\")\n",
    "    else:\n",
    "        print(\"⚠️ Warning: No standard ID column found. Will use row indices for identification.\")\n",
    "    \n",
    "    # Validate coordinate data\n",
    "    invalid_lat = df[(df['latitude'] < -90) | (df['latitude'] > 90) | df['latitude'].isna()]\n",
    "    invalid_lng = df[(df['longitude'] < -180) | (df['longitude'] > 180) | df['longitude'].isna()]\n",
    "    \n",
    "    # Report invalid coordinates with more detail\n",
    "    if len(invalid_lat) > 0:\n",
    "        print(f\"⚠️ Found {len(invalid_lat)} rows with invalid latitude values\")\n",
    "        print(f\"Sample of invalid latitudes: {df.loc[invalid_lat.index[:3], 'latitude'].tolist()}\")\n",
    "        print(f\"Row indices with bad latitudes: {invalid_lat.index[:5].tolist()}...\")\n",
    "    \n",
    "    if len(invalid_lng) > 0:\n",
    "        print(f\"⚠️ Found {len(invalid_lng)} rows with invalid longitude values\")\n",
    "        print(f\"Sample of invalid longitudes: {df.loc[invalid_lng.index[:3], 'longitude'].tolist()}\")\n",
    "        print(f\"Row indices with bad longitudes: {invalid_lng.index[:5].tolist()}...\")\n",
    "    \n",
    "    valid_coordinates = len(df) - len(pd.concat([invalid_lat, invalid_lng]).drop_duplicates())\n",
    "    print(f\"✅ {valid_coordinates} out of {len(df)} rows ({valid_coordinates/len(df)*100:.1f}%) have valid coordinates\")\n",
    "    \n",
    "    # Check for duplicate locations (might be intentional but worth noting)\n",
    "    duplicate_coords = df.duplicated(subset=['latitude', 'longitude'], keep=False)\n",
    "    if duplicate_coords.any():\n",
    "        print(f\"ℹ️ Found {duplicate_coords.sum()} rows with duplicate coordinates\")\n",
    "        print(\"   This might be expected if multiple deliveries go to the same location\")\n",
    "        \n",
    "        # Show some examples of duplicated coordinates\n",
    "        if duplicate_coords.sum() > 0:\n",
    "            first_dup_idx = duplicate_coords[duplicate_coords].index[0]\n",
    "            dup_lat = df.loc[first_dup_idx, 'latitude']\n",
    "            dup_lng = df.loc[first_dup_idx, 'longitude']\n",
    "            dups = df[(df['latitude'] == dup_lat) & (df['longitude'] == dup_lng)]\n",
    "            print(f\"   Example: These {len(dups)} rows share coordinates ({dup_lat}, {dup_lng}):\")\n",
    "            print(dups.head(3))\n",
    "    \n",
    "    # Check if cluster_name column exists (will be used as depot identifier)\n",
    "    if 'cluster_name' in df.columns:\n",
    "        print(f\"\\nValidating depot assignment column: 'cluster_name'\")\n",
    "        unique_values = df['cluster_name'].unique()\n",
    "        null_count = df['cluster_name'].isna().sum()\n",
    "        \n",
    "        print(f\"- Column data type: {df['cluster_name'].dtype}\")\n",
    "        print(f\"- Unique values: {len(unique_values)} (showing first 5): {unique_values[:5]}\")\n",
    "        print(f\"- Missing values: {null_count} ({null_count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n❌ ERROR: 'cluster_name' column not found in the data.\")\n",
    "        print(\"This script requires the 'cluster_name' column for depot identification.\")\n",
    "        raise ValueError(\"Missing 'cluster_name' column required for depot identification\")\n",
    "    \n",
    "    print(\"\\n✅ Data validation complete\")\n",
    "    \n",
    "    # Create a clean copy without invalid coordinates\n",
    "    df_clean = df[\n",
    "        (df['latitude'] >= -90) & (df['latitude'] <= 90) & \n",
    "        (df['longitude'] >= -180) & (df['longitude'] <= 180) &\n",
    "        ~df['latitude'].isna() & ~df['longitude'].isna()\n",
    "    ].copy()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Test this function if needed:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # First load data from cell 3\n",
    "#     # input_path, output_path = setup_project()\n",
    "#     # df, file_path = load_data(input_path)\n",
    "#     \n",
    "#     # Then validate\n",
    "#     # df_clean = validate_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c19352-8ecf-4986-ae69-069deb2aabd0",
   "metadata": {},
   "source": [
    "## Cell 5: Depot Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ac6735-756b-4311-8fb2-3b5a8b36f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_depot_dataframe(df_clean):\n",
    "    \"\"\"Create a dataframe of depots based on the cluster_name column\"\"\"\n",
    "    print(\"\\n=== DEPOT IDENTIFICATION ===\")\n",
    "    \n",
    "    # Get unique depot values from cluster_name\n",
    "    depot_names = sorted(df_clean['cluster_name'].unique())\n",
    "    print(f\"Found {len(depot_names)} unique depots: {', '.join([str(d) for d in depot_names])}\")\n",
    "    \n",
    "    # Create depot dataframe from coordinates\n",
    "    depot_info = []\n",
    "    \n",
    "    for i, depot_name in enumerate(depot_names):\n",
    "        # Get all rows for this depot\n",
    "        depot_rows = df_clean[df_clean['cluster_name'] == depot_name]\n",
    "        \n",
    "        # Use depot_latitude and depot_longitude if available\n",
    "        if 'depot_latitude' in df_clean.columns and 'depot_longitude' in df_clean.columns:\n",
    "            # Use the first row's depot coordinates\n",
    "            depot_lat = depot_rows['depot_latitude'].iloc[0]\n",
    "            depot_lng = depot_rows['depot_longitude'].iloc[0]\n",
    "            \n",
    "            # Check if all coords are the same for this depot\n",
    "            if len(depot_rows) > 1:\n",
    "                lat_same = (depot_rows['depot_latitude'] == depot_lat).all()\n",
    "                lng_same = (depot_rows['depot_longitude'] == depot_lng).all()\n",
    "                if not (lat_same and lng_same):\n",
    "                    print(f\"⚠️ Warning: Found different coordinates for depot '{depot_name}'\")\n",
    "                    print(f\"Using coordinates from first occurrence: {depot_lat}, {depot_lng}\")\n",
    "        else:\n",
    "            # If no depot coordinates, use centroid of locations in the cluster\n",
    "            depot_lat = depot_rows['latitude'].mean()\n",
    "            depot_lng = depot_rows['longitude'].mean()\n",
    "            print(f\"No depot coordinates found for '{depot_name}'. Using centroid: ({depot_lat:.6f}, {depot_lng:.6f})\")\n",
    "        \n",
    "        # Count locations assigned to this depot\n",
    "        location_count = len(depot_rows)\n",
    "        \n",
    "        # Store depot info\n",
    "        depot_info.append({\n",
    "            'cluster_id': i,\n",
    "            'depot_name': depot_name,\n",
    "            'latitude': depot_lat,\n",
    "            'longitude': depot_lng,\n",
    "            'location_count': location_count\n",
    "        })\n",
    "    \n",
    "    # Create the depot dataframe\n",
    "    depot_df = pd.DataFrame(depot_info)\n",
    "    print(f\"\\n✅ Created information for {len(depot_df)} depots\")\n",
    "    print(\"\\nDepot information:\")\n",
    "    print(depot_df)\n",
    "    \n",
    "    return depot_df\n",
    "\n",
    "# Test this function if needed:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load and validate data first\n",
    "#     # input_path, output_path = setup_project()\n",
    "#     # df, file_path = load_data(input_path)\n",
    "#     # df_clean = validate_data(df)\n",
    "#     \n",
    "#     # Then create depot dataframe\n",
    "#     # depot_df = create_depot_dataframe(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e805100-04e6-4692-82b8-c6b95d9ede57",
   "metadata": {},
   "source": [
    "## Cell 6: Operational Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0e3b93-98f9-482d-a072-3df5e9b20554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operational_constraints():\n",
    "    \"\"\"Get operational constraints from user input\"\"\"\n",
    "    print(\"\\n=== OPERATIONAL CONSTRAINTS ===\")\n",
    "    print(\"Now we'll set some parameters for the route optimization.\")\n",
    "    print(\"These define the vehicle and time constraints for each route.\")\n",
    "    \n",
    "    # Validate and get max weight with proper error handling\n",
    "    while True:\n",
    "        try:\n",
    "            weight_input = input(\"Vehicle weight/package capacity (default 800): \").strip()\n",
    "            max_weight = float(weight_input) if weight_input else 800\n",
    "            if max_weight <= 0:\n",
    "                print(\"Vehicle capacity must be positive. Please try again.\")\n",
    "                continue\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    # Validate and get max time\n",
    "    while True:\n",
    "        try:\n",
    "            time_input = input(\"Maximum route time in hours (default 8): \").strip()\n",
    "            hours = float(time_input) if time_input else 8\n",
    "            if hours <= 0:\n",
    "                print(\"Route time must be positive. Please try again.\")\n",
    "                continue\n",
    "            max_time = hours * 60  # Convert to minutes\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "            \n",
    "    # Validate and get depot service time (new parameter)\n",
    "    while True:\n",
    "        try:\n",
    "            depot_service_input = input(\"Service time at depot (loading/unloading) in minutes (default 45): \").strip()\n",
    "            depot_service_time = float(depot_service_input) if depot_service_input else 45\n",
    "            if depot_service_time < 0:\n",
    "                print(\"Depot service time cannot be negative. Please try again.\")\n",
    "                continue\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    # Validate and get service time\n",
    "    while True:\n",
    "        try:\n",
    "            service_input = input(\"Service time per unit in minutes (default 0.5): \").strip()\n",
    "            service_time_per_unit = float(service_input) if service_input else 0.5\n",
    "            if service_time_per_unit < 0:\n",
    "                print(\"Service time cannot be negative. Please try again.\")\n",
    "                continue\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    print(f\"\\nOptimization parameters:\")\n",
    "    print(f\"- Vehicle capacity: {max_weight} units\")\n",
    "    print(f\"- Maximum route time: {max_time/60:.1f} hours ({max_time:.0f} minutes)\")\n",
    "    print(f\"- Depot service time: {depot_service_time} minutes\")\n",
    "    print(f\"- Service time per unit: {service_time_per_unit} minutes\")\n",
    "    \n",
    "    # Save parameters for reference\n",
    "    optimization_params = {\n",
    "        'vehicle_capacity': max_weight,\n",
    "        'max_route_time_min': max_time,\n",
    "        'depot_service_time_min': depot_service_time,\n",
    "        'service_time_per_unit_min': service_time_per_unit\n",
    "    }\n",
    "    \n",
    "    return optimization_params\n",
    "\n",
    "# Test this function if needed:\n",
    "# if __name__ == \"__main__\":\n",
    "#     optimization_params = get_operational_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1f947-ae70-4ef6-918b-1f27350f505d",
   "metadata": {},
   "source": [
    "## Cell 7: HERE API testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e567a1e-54a0-4120-9562-7434e7f6d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_here_api_connection(api_key, depot_lat, depot_lng):\n",
    "    \"\"\"Test the HERE API connectivity with a simple routing request\"\"\"\n",
    "    print(\"\\nTesting HERE API connectivity...\")\n",
    "    \n",
    "    # Create a simple request to test connectivity\n",
    "    url = \"https://router.hereapi.com/v8/routes\"\n",
    "    params = {\n",
    "        'apiKey': api_key,\n",
    "        'transportMode': 'car',\n",
    "        'origin': f\"{depot_lat},{depot_lng}\",\n",
    "        'destination': f\"{depot_lat+0.01},{depot_lng+0.01}\",  # Slightly offset destination\n",
    "        'return': 'summary'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        # Parse response\n",
    "        route_data = response.json()\n",
    "        \n",
    "        if 'routes' in route_data and len(route_data['routes']) > 0:\n",
    "            route_sections = route_data['routes'][0].get('sections', [])\n",
    "            if route_sections:\n",
    "                print(f\"✅ HERE API connection successful!\")\n",
    "                print(f\"Received {len(route_sections)} route sections in response\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"⚠️ HERE API response does not contain any route sections\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"⚠️ HERE API response does not contain any routes\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ HERE API connection failed: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing HERE API: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_distance_cache():\n",
    "    \"\"\"Setup a persistent disk-based cache for distance calculations\"\"\"\n",
    "    import sqlite3\n",
    "    import os\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    cache_dir = Path.cwd() / 'cache'\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Create database connection\n",
    "    db_path = cache_dir / 'distance_cache.db'\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create tables if they don't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS distance_cache (\n",
    "        origin_lat REAL,\n",
    "        origin_lng REAL,\n",
    "        dest_lat REAL,\n",
    "        dest_lng REAL,\n",
    "        distance_km REAL,\n",
    "        time_min REAL,\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (origin_lat, origin_lng, dest_lat, dest_lng)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE INDEX IF NOT EXISTS idx_locations ON distance_cache (origin_lat, origin_lng, dest_lat, dest_lng)\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"✅ Distance cache setup complete at {db_path}\")\n",
    "    return str(db_path)\n",
    "\n",
    "def display_overall_status(status_dict, refresh_interval=1):\n",
    "    \"\"\"Display a real-time status message that updates in place\"\"\"\n",
    "    last_message = \"\"\n",
    "    \n",
    "    while not status_dict.get('done', False):\n",
    "        # Get current status information\n",
    "        stage = status_dict.get('stage', 'Processing')\n",
    "        detail = status_dict.get('detail', '')\n",
    "        progress = status_dict.get('progress', '')\n",
    "        \n",
    "        # Format message\n",
    "        if progress:\n",
    "            status_msg = f\"STATUS: {stage} - {detail} [{progress}]\"\n",
    "        else:\n",
    "            status_msg = f\"STATUS: {stage} - {detail}\"\n",
    "            \n",
    "        # Only update if message changed\n",
    "        if status_msg != last_message:\n",
    "            # Clear line and move cursor to beginning\n",
    "            sys.stdout.write('\\r' + ' ' * 100 + '\\r')\n",
    "            # Print current status\n",
    "            sys.stdout.write(status_msg)\n",
    "            sys.stdout.flush()\n",
    "            last_message = status_msg\n",
    "            \n",
    "        time.sleep(refresh_interval)\n",
    "    \n",
    "    # Final newline\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cecad6-7a17-4311-92cd-120bb307e96b",
   "metadata": {},
   "source": [
    "## Cell 8: Routing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f5344ca-9682-4c95-af33-afeac2f8afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries needed for concurrent processing and advanced algorithms\n",
    "import concurrent.futures\n",
    "import sqlite3\n",
    "import itertools\n",
    "import heapq\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time as timing_module\n",
    "import pickle\n",
    "import os\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pathlib import Path\n",
    "\n",
    "# Utility to calculate aerial distance - for filtering, not fallback\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance between two points on Earth.\"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = 6371 * c  # Radius of Earth in kilometers\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def get_from_cache(db_path, origin, destination):\n",
    "    \"\"\"Get distance and time from persistent cache\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Query the cache\n",
    "    cursor.execute(\n",
    "        \"SELECT distance_km, time_min FROM distance_cache WHERE \"\n",
    "        \"origin_lat = ? AND origin_lng = ? AND dest_lat = ? AND dest_lng = ?\",\n",
    "        (origin['latitude'], origin['longitude'], destination['latitude'], destination['longitude'])\n",
    "    )\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if result:\n",
    "        return result[0], result[1]\n",
    "    return None\n",
    "\n",
    "def add_to_cache(db_path, origin, destination, distance_km, time_min):\n",
    "    \"\"\"Add distance and time to persistent cache\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert into cache\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"INSERT OR REPLACE INTO distance_cache (origin_lat, origin_lng, dest_lat, dest_lng, distance_km, time_min) \"\n",
    "            \"VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (origin['latitude'], origin['longitude'], destination['latitude'], destination['longitude'], \n",
    "             distance_km, time_min)\n",
    "        )\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to cache results: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def make_api_request_with_retry(url, params, max_retries=3):\n",
    "    \"\"\"Make API request with retry logic and exponential backoff\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Add exponential backoff between retries\n",
    "            if attempt > 0:\n",
    "                time.sleep(1 * (2 ** (attempt - 1)))  # 1, 2, 4 seconds\n",
    "                \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "def batch_route_request(location_pairs, api_key, batch_size=25):\n",
    "    \"\"\"Fallback function that uses standard routing API instead of matrix API\"\"\"\n",
    "    if not location_pairs:\n",
    "        return {}\n",
    "    \n",
    "    # Prepare results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Process each pair individually\n",
    "    print(f\"Using individual routing requests instead of batch matrix API (slower)\")\n",
    "    \n",
    "    # Add progress bar\n",
    "    with tqdm(total=len(location_pairs), desc=\"Calculating routes\", unit=\"pair\") as pbar:\n",
    "        for origin, destination in location_pairs:\n",
    "            # Validate coordinates before making API requests\n",
    "            try:\n",
    "                orig_lat = float(origin['latitude'])\n",
    "                orig_lng = float(origin['longitude'])\n",
    "                dest_lat = float(destination['latitude'])\n",
    "                dest_lng = float(destination['longitude'])\n",
    "                \n",
    "                # Check for valid coordinate ranges\n",
    "                if not (-90 <= orig_lat <= 90) or not (-180 <= orig_lng <= 180) or \\\n",
    "                   not (-90 <= dest_lat <= 90) or not (-180 <= dest_lng <= 180):\n",
    "                    print(f\"⚠️ Skipping invalid coordinates: Origin({orig_lat}, {orig_lng}) → Destination({dest_lat}, {dest_lng})\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                    \n",
    "            except (ValueError, TypeError, KeyError) as e:\n",
    "                print(f\"⚠️ Error validating coordinates: {e}\")\n",
    "                print(f\"Origin: {origin}, Destination: {destination}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Use the standard routing API\n",
    "            url = \"https://router.hereapi.com/v8/routes\"\n",
    "            \n",
    "            # Prepare parameters with explicit formatting\n",
    "            params = {\n",
    "                'apiKey': api_key,\n",
    "                'transportMode': 'car',\n",
    "                'origin': f\"{orig_lat:.6f},{orig_lng:.6f}\",  # Format with precision\n",
    "                'destination': f\"{dest_lat:.6f},{dest_lng:.6f}\",  # Format with precision\n",
    "                'return': 'summary'\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Add small delay to avoid rate limiting\n",
    "                time.sleep(0.3)\n",
    "                \n",
    "                # Make the request with retry\n",
    "                response = make_api_request_with_retry(url, params)\n",
    "                \n",
    "                if response and response.status_code == 200:\n",
    "                    route_data = response.json()\n",
    "                    \n",
    "                    # Extract distance and time\n",
    "                    if 'routes' in route_data and len(route_data['routes']) > 0:\n",
    "                        route_obj = route_data['routes'][0]\n",
    "                        sections = route_obj.get('sections', [])\n",
    "                        \n",
    "                        if sections:\n",
    "                            section = sections[0]\n",
    "                            summary = section.get('summary', {})\n",
    "                            \n",
    "                            # Extract distance and time\n",
    "                            distance_meters = summary.get('length', 0)\n",
    "                            time_seconds = summary.get('duration', 0)\n",
    "                            \n",
    "                            # Convert to km and minutes\n",
    "                            distance_km = distance_meters / 1000.0\n",
    "                            time_min = time_seconds / 60.0\n",
    "                            \n",
    "                            # Store in results\n",
    "                            pair_key = (origin['latitude'], origin['longitude'], \n",
    "                                        destination['latitude'], destination['longitude'])\n",
    "                            results[pair_key] = (distance_km, time_min)\n",
    "                elif response:\n",
    "                    print(f\"⚠️ Warning: API request failed with status code {response.status_code}\")\n",
    "                    print(f\"Request parameters: {params}\")\n",
    "                    \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in route request: {e}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"Processed {len(results)} out of {len(location_pairs)} route segments\")\n",
    "    return results\n",
    "\n",
    "def get_route_segment_metrics(origin, destination, api_key, cache_db_path):\n",
    "    \"\"\"Calculate the distance and time between two points using HERE API with caching.\"\"\"\n",
    "    \n",
    "    # Skip if coordinates are the same\n",
    "    if (origin['latitude'] == destination['latitude'] and \n",
    "        origin['longitude'] == destination['longitude']):\n",
    "        return (0, 0)\n",
    "    \n",
    "    # Validate coordinates first\n",
    "    try:\n",
    "        orig_lat = float(origin['latitude'])\n",
    "        orig_lng = float(origin['longitude'])\n",
    "        dest_lat = float(destination['latitude'])\n",
    "        dest_lng = float(destination['longitude'])\n",
    "        \n",
    "        # Check for valid coordinate ranges\n",
    "        if not (-90 <= orig_lat <= 90) or not (-180 <= orig_lng <= 180) or \\\n",
    "           not (-90 <= dest_lat <= 90) or not (-180 <= dest_lng <= 180):\n",
    "            print(f\"⚠️ Invalid coordinate values: Origin({orig_lat}, {orig_lng}) → Destination({dest_lat}, {dest_lng})\")\n",
    "            return None\n",
    "            \n",
    "    except (ValueError, TypeError, KeyError) as e:\n",
    "        print(f\"⚠️ Error validating coordinates: {e}\")\n",
    "        print(f\"Origin: {origin}, Destination: {destination}\")\n",
    "        return None\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_result = get_from_cache(cache_db_path, origin, destination)\n",
    "    if cached_result:\n",
    "        return cached_result\n",
    "    \n",
    "    # Add rate limiting protection - sleep between requests\n",
    "    time.sleep(0.3)  # Add slight delay to avoid rate limiting\n",
    "    \n",
    "    # Make a direct API call for this segment using validated coordinates\n",
    "    url = \"https://router.hereapi.com/v8/routes\"\n",
    "    \n",
    "    # Prepare parameters with explicit formatting of coordinates\n",
    "    params = {\n",
    "        'apiKey': api_key,\n",
    "        'transportMode': 'car',\n",
    "        'origin': f\"{orig_lat:.6f},{orig_lng:.6f}\",  # Format with precision\n",
    "        'destination': f\"{dest_lat:.6f},{dest_lng:.6f}\",  # Format with precision\n",
    "        'return': 'summary'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Simple GET request for this segment with retry\n",
    "        response = make_api_request_with_retry(url, params)\n",
    "        \n",
    "        if not response or response.status_code != 200:\n",
    "            if response:\n",
    "                print(f\"❌ HERE API error: Status code {response.status_code}\")\n",
    "                if response.status_code == 400:\n",
    "                    print(f\"Request parameters: {params}\")\n",
    "            return None\n",
    "        \n",
    "        # Parse response\n",
    "        route_data = response.json()\n",
    "        \n",
    "        # Extract distance and time\n",
    "        if 'routes' in route_data and len(route_data['routes']) > 0:\n",
    "            route_obj = route_data['routes'][0]\n",
    "            sections = route_obj.get('sections', [])\n",
    "            \n",
    "            if sections:\n",
    "                section = sections[0]\n",
    "                summary = section.get('summary', {})\n",
    "                \n",
    "                # Extract distance and time\n",
    "                distance_meters = summary.get('length', 0)\n",
    "                time_seconds = summary.get('duration', 0)\n",
    "                \n",
    "                # Convert to km and minutes\n",
    "                distance_km = distance_meters / 1000.0\n",
    "                time_min = time_seconds / 60.0\n",
    "                \n",
    "                # Cache the result\n",
    "                add_to_cache(cache_db_path, origin, destination, distance_km, time_min)\n",
    "                \n",
    "                return distance_km, time_min\n",
    "        \n",
    "        print(\"⚠️ Warning: Unable to extract route metrics from API response\")\n",
    "        return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error calculating route segment: {e}\")\n",
    "        return None\n",
    "\n",
    "# ALNS implementation for Vehicle Routing Problem\n",
    "class ALNS:\n",
    "    \"\"\"Adaptive Large Neighborhood Search for Vehicle Routing Problem\"\"\"\n",
    "    \n",
    "    def __init__(self, depot, locations, max_weight, max_time, service_time_per_unit, api_key, cache_db_path):\n",
    "        \"\"\"Initialize ALNS algorithm with problem data\"\"\"\n",
    "        self.depot = depot\n",
    "        self.locations = locations\n",
    "        self.max_weight = max_weight\n",
    "        self.max_time = max_time\n",
    "        self.service_time_per_unit = service_time_per_unit\n",
    "        self.api_key = api_key\n",
    "        self.cache_db_path = cache_db_path\n",
    "        \n",
    "        # Pre-calculate weights and service times\n",
    "        self.location_weights = {}\n",
    "        self.service_times = {}\n",
    "        for idx, location in locations.iterrows():\n",
    "            # Extract weight with fallback options\n",
    "            weight = 1.0\n",
    "            if 'Net Weight' in location and pd.notna(location['Net Weight']):\n",
    "                weight = float(location['Net Weight'])\n",
    "            elif 'weight' in location and pd.notna(location['weight']):\n",
    "                weight = float(location['weight'])\n",
    "            \n",
    "            self.location_weights[idx] = weight\n",
    "            self.service_times[idx] = weight * service_time_per_unit\n",
    "        \n",
    "        # Pre-calculate depot distance for each location\n",
    "        self.depot_distances = {}\n",
    "        self.depot_times = {}\n",
    "        \n",
    "        # Build aerial distance matrix for quick filtering\n",
    "        self.aerial_distances = {}\n",
    "        coords = [(idx, loc['latitude'], loc['longitude']) for idx, loc in locations.iterrows()]\n",
    "        \n",
    "        # Calculate aerial distances between all pairs\n",
    "        for i, (idx1, lat1, lng1) in enumerate(coords):\n",
    "            # Calculate depot distance\n",
    "            depot_dist = haversine_distance(depot['latitude'], depot['longitude'], lat1, lng1)\n",
    "            self.aerial_distances[(depot['latitude'], depot['longitude'], lat1, lng1)] = depot_dist\n",
    "            \n",
    "            for j, (idx2, lat2, lng2) in enumerate(coords):\n",
    "                if i != j:\n",
    "                    dist = haversine_distance(lat1, lng1, lat2, lng2)\n",
    "                    self.aerial_distances[(lat1, lng1, lat2, lng2)] = dist\n",
    "    \n",
    "    def destroy_random(self, solution, destroy_count):\n",
    "        \"\"\"Remove random locations from routes\"\"\"\n",
    "        removed_locations = []\n",
    "        all_locations = []\n",
    "        \n",
    "        # Collect all locations from all routes\n",
    "        for route in solution:\n",
    "            # Skip depot markers\n",
    "            for loc_id in route['location_ids'][1:-1]:\n",
    "                if loc_id != 'DEPOT_START' and loc_id != 'DEPOT_END':\n",
    "                    all_locations.append(loc_id)\n",
    "        \n",
    "        # Randomly select locations to remove\n",
    "        if all_locations:\n",
    "            destroy_count = min(destroy_count, len(all_locations))\n",
    "            to_remove = random.sample(all_locations, destroy_count)\n",
    "            \n",
    "            # Remove selected locations from routes\n",
    "            new_solution = []\n",
    "            for route in solution:\n",
    "                new_route = route.copy()\n",
    "                new_locs = []\n",
    "                for loc_id in route['location_ids']:\n",
    "                    if loc_id in to_remove:\n",
    "                        if loc_id != 'DEPOT_START' and loc_id != 'DEPOT_END':\n",
    "                            removed_locations.append(loc_id)\n",
    "                    else:\n",
    "                        new_locs.append(loc_id)\n",
    "                \n",
    "                # Update route\n",
    "                new_route['location_ids'] = new_locs\n",
    "                \n",
    "                # Only keep routes with customers (apart from depot start/end)\n",
    "                if len(new_locs) > 2:  # DEPOT_START, customers, DEPOT_END\n",
    "                    new_solution.append(new_route)\n",
    "                    \n",
    "            return new_solution, removed_locations\n",
    "        \n",
    "        return solution, []\n",
    "    \n",
    "    def destroy_related(self, solution, destroy_count):\n",
    "        \"\"\"Remove related (geographically close) locations from routes\"\"\"\n",
    "        if not solution:\n",
    "            return solution, []\n",
    "        \n",
    "        removed_locations = []\n",
    "        \n",
    "        # Randomly select a seed location\n",
    "        all_locations = []\n",
    "        for route in solution:\n",
    "            for loc_id in route['location_ids'][1:-1]:\n",
    "                if loc_id != 'DEPOT_START' and loc_id != 'DEPOT_END':\n",
    "                    all_locations.append(loc_id)\n",
    "        \n",
    "        if not all_locations:\n",
    "            return solution, []\n",
    "        \n",
    "        # Select a random seed\n",
    "        seed_id = random.choice(all_locations)\n",
    "        seed_location = None\n",
    "        \n",
    "        # Find the seed location data\n",
    "        for idx, location in self.locations.iterrows():\n",
    "            if ('ABS Custumer no' in location and location['ABS Custumer no'] == seed_id) or \\\n",
    "               ('customer_id' in location and location['customer_id'] == seed_id) or \\\n",
    "               idx == seed_id:\n",
    "                seed_location = location\n",
    "                break\n",
    "        \n",
    "        if seed_location is None:\n",
    "            return self.destroy_random(solution, destroy_count)\n",
    "        \n",
    "        # Calculate distances from seed to all other locations\n",
    "        distances_to_seed = []\n",
    "        for idx, location in self.locations.iterrows():\n",
    "            loc_id = None\n",
    "            if 'ABS Custumer no' in location:\n",
    "                loc_id = location['ABS Custumer no']\n",
    "            elif 'customer_id' in location:\n",
    "                loc_id = location['customer_id']\n",
    "            else:\n",
    "                loc_id = idx\n",
    "                \n",
    "            if loc_id in all_locations and loc_id != seed_id:\n",
    "                # Use precomputed aerial distance\n",
    "                aerial_key = (seed_location['latitude'], seed_location['longitude'], \n",
    "                             location['latitude'], location['longitude'])\n",
    "                \n",
    "                # Handle reverse key if needed\n",
    "                if aerial_key not in self.aerial_distances:\n",
    "                    aerial_key = (location['latitude'], location['longitude'],\n",
    "                                 seed_location['latitude'], seed_location['longitude'])\n",
    "                \n",
    "                distance = self.aerial_distances.get(aerial_key, float('inf'))\n",
    "                distances_to_seed.append((loc_id, distance))\n",
    "        \n",
    "        # Sort by distance\n",
    "        distances_to_seed.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Select closest locations\n",
    "        destroy_count = min(destroy_count - 1, len(distances_to_seed))\n",
    "        to_remove = [seed_id] + [loc_id for loc_id, _ in distances_to_seed[:destroy_count]]\n",
    "        \n",
    "        # Remove selected locations from routes\n",
    "        new_solution = []\n",
    "        for route in solution:\n",
    "            new_route = route.copy()\n",
    "            new_locs = []\n",
    "            for loc_id in route['location_ids']:\n",
    "                if loc_id in to_remove:\n",
    "                    if loc_id != 'DEPOT_START' and loc_id != 'DEPOT_END':\n",
    "                        removed_locations.append(loc_id)\n",
    "                else:\n",
    "                    new_locs.append(loc_id)\n",
    "            \n",
    "            # Update route\n",
    "            new_route['location_ids'] = new_locs\n",
    "            \n",
    "            # Only keep routes with customers (apart from depot start/end)\n",
    "            if len(new_locs) > 2:  # DEPOT_START, customers, DEPOT_END\n",
    "                new_solution.append(new_route)\n",
    "        \n",
    "        return new_solution, removed_locations\n",
    "    \n",
    "    def destroy_worst(self, solution, destroy_count):\n",
    "        \"\"\"Remove locations that contribute most to total route cost\"\"\"\n",
    "        if not solution:\n",
    "            return solution, []\n",
    "        \n",
    "        removed_locations = []\n",
    "        location_costs = []\n",
    "        \n",
    "        # Calculate cost contribution for each location\n",
    "        for route_idx, route in enumerate(solution):\n",
    "            # Skip depot markers\n",
    "            customer_locs = [loc for loc in route['location_ids'][1:-1] \n",
    "                            if loc != 'DEPOT_START' and loc != 'DEPOT_END']\n",
    "            \n",
    "            for i, loc_id in enumerate(customer_locs):\n",
    "                # Cost is distance from previous to this + this to next\n",
    "                cost = 0\n",
    "                \n",
    "                # Get the location\n",
    "                loc_data = None\n",
    "                for idx, location in self.locations.iterrows():\n",
    "                    if ('ABS Custumer no' in location and location['ABS Custumer no'] == loc_id) or \\\n",
    "                       ('customer_id' in location and location['customer_id'] == loc_id) or \\\n",
    "                       idx == loc_id:\n",
    "                        loc_data = location\n",
    "                        break\n",
    "                \n",
    "                if loc_data is None:\n",
    "                    continue\n",
    "                \n",
    "                # Add to cost list\n",
    "                location_costs.append((loc_id, cost))\n",
    "        \n",
    "        # Sort by cost (descending)\n",
    "        location_costs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Select highest cost locations\n",
    "        destroy_count = min(destroy_count, len(location_costs))\n",
    "        to_remove = [loc_id for loc_id, _ in location_costs[:destroy_count]]\n",
    "        \n",
    "        # Remove selected locations from routes\n",
    "        new_solution = []\n",
    "        for route in solution:\n",
    "            new_route = route.copy()\n",
    "            new_locs = []\n",
    "            for loc_id in route['location_ids']:\n",
    "                if loc_id in to_remove:\n",
    "                    if loc_id != 'DEPOT_START' and loc_id != 'DEPOT_END':\n",
    "                        removed_locations.append(loc_id)\n",
    "                else:\n",
    "                    new_locs.append(loc_id)\n",
    "            \n",
    "            # Update route\n",
    "            new_route['location_ids'] = new_locs\n",
    "            \n",
    "            # Only keep routes with customers (apart from depot start/end)\n",
    "            if len(new_locs) > 2:  # DEPOT_START, customers, DEPOT_END\n",
    "                new_solution.append(new_route)\n",
    "        \n",
    "        return new_solution, removed_locations\n",
    "    \n",
    "    def repair_greedy(self, solution, removed_locations):\n",
    "        \"\"\"Insert removed locations back into routes greedily\"\"\"\n",
    "        if not removed_locations:\n",
    "            return solution\n",
    "        \n",
    "        # Create copy of solution for modification\n",
    "        new_solution = [route.copy() for route in solution]\n",
    "        \n",
    "        # Create a new route if no routes exist\n",
    "        if not new_solution:\n",
    "            new_route = {\n",
    "                'route_id': '1',\n",
    "                'vehicle_number': 1,\n",
    "                'depot_name': self.depot['depot_name'],\n",
    "                'location_ids': ['DEPOT_START', 'DEPOT_END'],\n",
    "                'weight': 0,\n",
    "                'distance_km': 0,\n",
    "                'time_min': 45,  # Depot service time\n",
    "                'stops': 0\n",
    "            }\n",
    "            new_solution = [new_route]\n",
    "        \n",
    "        # For each removed location, find best insertion position\n",
    "        for loc_id in removed_locations:\n",
    "            best_cost_increase = float('inf')\n",
    "            best_route_idx = -1\n",
    "            best_position = -1\n",
    "            \n",
    "            # Get location data\n",
    "            loc_data = None\n",
    "            for idx, location in self.locations.iterrows():\n",
    "                if ('ABS Custumer no' in location and location['ABS Custumer no'] == loc_id) or \\\n",
    "                   ('customer_id' in location and location['customer_id'] == loc_id) or \\\n",
    "                   idx == loc_id:\n",
    "                    loc_data = location\n",
    "                    break\n",
    "            \n",
    "            if loc_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Get weight of location\n",
    "            loc_weight = self.location_weights.get(loc_id, 1.0)\n",
    "            \n",
    "            # Try inserting in each route at each position\n",
    "            for route_idx, route in enumerate(new_solution):\n",
    "                # Check if adding this location would exceed capacity\n",
    "                if route['weight'] + loc_weight > self.max_weight:\n",
    "                    continue\n",
    "                \n",
    "                # Get customer locations in route\n",
    "                route_locs = route['location_ids']\n",
    "                \n",
    "                # Try inserting at each position\n",
    "                for pos in range(1, len(route_locs)):  # Skip inserting at DEPOT_START (0)\n",
    "                    # Calculate cost increase of inserting here\n",
    "                    cost_increase = 0  # This is a simplification - in real implementation, calculate actual cost\n",
    "                    \n",
    "                    # If this is better than current best, update\n",
    "                    if cost_increase < best_cost_increase:\n",
    "                        best_cost_increase = cost_increase\n",
    "                        best_route_idx = route_idx\n",
    "                        best_position = pos\n",
    "            \n",
    "            # If no feasible insertion found, create a new route\n",
    "            if best_route_idx == -1:\n",
    "                new_route = {\n",
    "                    'route_id': str(len(new_solution) + 1),\n",
    "                    'vehicle_number': len(new_solution) + 1,\n",
    "                    'depot_name': self.depot['depot_name'],\n",
    "                    'location_ids': ['DEPOT_START', loc_id, 'DEPOT_END'],\n",
    "                    'weight': loc_weight,\n",
    "                    'distance_km': 0,  # Placeholder, recalculated later\n",
    "                    'time_min': 45,    # Initial depot service time\n",
    "                    'stops': 1\n",
    "                }\n",
    "                new_solution.append(new_route)\n",
    "            else:\n",
    "                # Insert at best position\n",
    "                route = new_solution[best_route_idx]\n",
    "                route['location_ids'].insert(best_position, loc_id)\n",
    "                route['weight'] += loc_weight\n",
    "                route['stops'] += 1\n",
    "        \n",
    "        # Recalculate route metrics (distance, time) for all modified routes\n",
    "        for route in new_solution:\n",
    "            self.recalculate_route_metrics(route)\n",
    "        \n",
    "        return new_solution\n",
    "    \n",
    "    def repair_regret(self, solution, removed_locations):\n",
    "        \"\"\"Insert removed locations using regret heuristic\"\"\"\n",
    "        if not removed_locations:\n",
    "            return solution\n",
    "        \n",
    "        # Create copy of solution for modification\n",
    "        new_solution = [route.copy() for route in solution]\n",
    "        \n",
    "        # Create a new route if no routes exist\n",
    "        if not new_solution:\n",
    "            new_route = {\n",
    "                'route_id': '1',\n",
    "                'vehicle_number': 1,\n",
    "                'depot_name': self.depot['depot_name'],\n",
    "                'location_ids': ['DEPOT_START', 'DEPOT_END'],\n",
    "                'weight': 0,\n",
    "                'distance_km': 0,\n",
    "                'time_min': 45,  # Depot service time\n",
    "                'stops': 0\n",
    "            }\n",
    "            new_solution = [new_route]\n",
    "        \n",
    "        # While there are locations to insert\n",
    "        remaining_locations = removed_locations.copy()\n",
    "        \n",
    "        while remaining_locations:\n",
    "            best_regret = -float('inf')\n",
    "            best_location = None\n",
    "            best_route_idx = -1\n",
    "            best_position = -1\n",
    "            \n",
    "            # For each remaining location\n",
    "            for loc_id in remaining_locations:\n",
    "                # Get location data\n",
    "                loc_data = None\n",
    "                for idx, location in self.locations.iterrows():\n",
    "                    if ('ABS Custumer no' in location and location['ABS Custumer no'] == loc_id) or \\\n",
    "                       ('customer_id' in location and location['customer_id'] == loc_id) or \\\n",
    "                       idx == loc_id:\n",
    "                        loc_data = location\n",
    "                        break\n",
    "                \n",
    "                if loc_data is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get weight of location\n",
    "                loc_weight = self.location_weights.get(loc_id, 1.0)\n",
    "                \n",
    "                # Calculate insertion costs for all positions in all routes\n",
    "                insertion_costs = []\n",
    "                route_positions = []\n",
    "                \n",
    "                for route_idx, route in enumerate(new_solution):\n",
    "                    # Check if adding this location would exceed capacity\n",
    "                    if route['weight'] + loc_weight > self.max_weight:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get customer locations in route\n",
    "                    route_locs = route['location_ids']\n",
    "                    \n",
    "                    # Try inserting at each position\n",
    "                    for pos in range(1, len(route_locs)):  # Skip inserting at DEPOT_START (0)\n",
    "                        # Calculate cost increase of inserting here\n",
    "                        cost_increase = 0  # This is a simplification - in real implementation, calculate actual cost\n",
    "                        \n",
    "                        insertion_costs.append(cost_increase)\n",
    "                        route_positions.append((route_idx, pos))\n",
    "                \n",
    "                # Sort costs and calculate regret\n",
    "                if insertion_costs:\n",
    "                    insertion_costs, route_positions = zip(*sorted(zip(insertion_costs, route_positions)))\n",
    "                    \n",
    "                    # Calculate regret value (difference between best and second best)\n",
    "                    if len(insertion_costs) >= 2:\n",
    "                        regret = insertion_costs[1] - insertion_costs[0]\n",
    "                    else:\n",
    "                        regret = insertion_costs[0]\n",
    "                    \n",
    "                    # If this location has higher regret, it becomes the candidate\n",
    "                    if regret > best_regret:\n",
    "                        best_regret = regret\n",
    "                        best_location = loc_id\n",
    "                        best_route_idx, best_position = route_positions[0]\n",
    "            \n",
    "            # If a location was selected\n",
    "            if best_location is not None:\n",
    "                # Insert at best position\n",
    "                if best_route_idx != -1:\n",
    "                    route = new_solution[best_route_idx]\n",
    "                    route['location_ids'].insert(best_position, best_location)\n",
    "                    route['weight'] += self.location_weights.get(best_location, 1.0)\n",
    "                    route['stops'] += 1\n",
    "                else:\n",
    "                    # Create new route if no feasible insertion\n",
    "                    new_route = {\n",
    "                        'route_id': str(len(new_solution) + 1),\n",
    "                        'vehicle_number': len(new_solution) + 1,\n",
    "                        'depot_name': self.depot['depot_name'],\n",
    "                        'location_ids': ['DEPOT_START', best_location, 'DEPOT_END'],\n",
    "                        'weight': self.location_weights.get(best_location, 1.0),\n",
    "                        'distance_km': 0,  # Placeholder, recalculated later\n",
    "                        'time_min': 45,    # Initial depot service time\n",
    "                        'stops': 1\n",
    "                    }\n",
    "                    new_solution.append(new_route)\n",
    "                \n",
    "                # Remove this location from the list\n",
    "                remaining_locations.remove(best_location)\n",
    "            else:\n",
    "                # If no feasible insertion was found for any location, create a new route\n",
    "                if remaining_locations:\n",
    "                    loc_id = remaining_locations[0]\n",
    "                    new_route = {\n",
    "                        'route_id': str(len(new_solution) + 1),\n",
    "                        'vehicle_number': len(new_solution) + 1,\n",
    "                        'depot_name': self.depot['depot_name'],\n",
    "                        'location_ids': ['DEPOT_START', loc_id, 'DEPOT_END'],\n",
    "                        'weight': self.location_weights.get(loc_id, 1.0),\n",
    "                        'distance_km': 0,  # Placeholder, recalculated later\n",
    "                        'time_min': 45,    # Initial depot service time\n",
    "                        'stops': 1\n",
    "                    }\n",
    "                    new_solution.append(new_route)\n",
    "                    remaining_locations.remove(loc_id)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # Recalculate route metrics for all modified routes\n",
    "        for route in new_solution:\n",
    "            self.recalculate_route_metrics(route)\n",
    "        \n",
    "        return new_solution\n",
    "    \n",
    "    def recalculate_route_metrics(self, route):\n",
    "        \"\"\"Recalculate distance, time, and other metrics for a route\"\"\"\n",
    "        # This is a simplified version - in real implementation, use actual API data\n",
    "        route['distance_km'] = 0\n",
    "        route['time_min'] = 45  # Initial depot service time\n",
    "        \n",
    "        # Return if route has no customers\n",
    "        if len(route['location_ids']) <= 2:\n",
    "            return\n",
    "        \n",
    "        # Get location IDs excluding depot\n",
    "        location_ids = route['location_ids'][1:-1]  # Skip DEPOT_START and DEPOT_END\n",
    "        \n",
    "        # Calculate route metrics\n",
    "        prev_loc = self.depot\n",
    "        total_distance = 0\n",
    "        total_time = 45  # Start with depot service time\n",
    "        \n",
    "        for loc_id in location_ids:\n",
    "            # Find location data\n",
    "            loc_data = None\n",
    "            for idx, location in self.locations.iterrows():\n",
    "                if ('ABS Custumer no' in location and location['ABS Custumer no'] == loc_id) or \\\n",
    "                   ('customer_id' in location and location['customer_id'] == loc_id) or \\\n",
    "                   idx == loc_id:\n",
    "                    loc_data = location\n",
    "                    break\n",
    "            \n",
    "            if loc_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Get routing metrics from HERE API (or cache)\n",
    "            result = get_route_segment_metrics(prev_loc, loc_data, self.api_key, self.cache_db_path)\n",
    "            \n",
    "            if result is not None:\n",
    "                distance_km, time_min = result\n",
    "                total_distance += distance_km\n",
    "                total_time += time_min\n",
    "                \n",
    "                # Add service time\n",
    "                service_time = self.service_times.get(loc_id, 0.5)\n",
    "                total_time += service_time\n",
    "            \n",
    "            # Update previous location\n",
    "            prev_loc = loc_data\n",
    "        \n",
    "        # Add return to depot\n",
    "        result = get_route_segment_metrics(prev_loc, self.depot, self.api_key, self.cache_db_path)\n",
    "        \n",
    "        if result is not None:\n",
    "            distance_km, time_min = result\n",
    "            total_distance += distance_km\n",
    "            total_time += time_min + 45  # Add depot service time\n",
    "        \n",
    "        # Update route metrics\n",
    "        route['distance_km'] = total_distance\n",
    "        route['time_min'] = total_time\n",
    "    \n",
    "    def run(self, iterations=50):\n",
    "        \"\"\"Run ALNS algorithm to optimize routes\"\"\"\n",
    "        # Create initial solution with greedy insertion\n",
    "        print(\"Creating initial solution with greedy insertion...\")\n",
    "        current_solution = self.create_initial_solution()\n",
    "        \n",
    "        best_solution = current_solution.copy()\n",
    "        best_cost = self.evaluate_solution(best_solution)\n",
    "        \n",
    "        # Simulated annealing parameters\n",
    "        temperature = 100\n",
    "        cooling_rate = 0.95\n",
    "        \n",
    "        # Destroy and repair operators with weights\n",
    "        destroy_ops = [\n",
    "            (self.destroy_random, 1.0),\n",
    "            (self.destroy_related, 1.0),\n",
    "            (self.destroy_worst, 1.0)\n",
    "        ]\n",
    "        \n",
    "        repair_ops = [\n",
    "            (self.repair_greedy, 1.0),\n",
    "            (self.repair_regret, 1.0)\n",
    "        ]\n",
    "        \n",
    "        # Weights for adaptive operator selection\n",
    "        destroy_weights = [1.0] * len(destroy_ops)\n",
    "        repair_weights = [1.0] * len(repair_ops)\n",
    "        \n",
    "        # Performance tracking\n",
    "        destroy_scores = [0] * len(destroy_ops)\n",
    "        repair_scores = [0] * len(repair_ops)\n",
    "        \n",
    "        print(f\"Starting ALNS with {iterations} iterations...\")\n",
    "        \n",
    "        # Add progress bar for iterations\n",
    "        with tqdm(total=iterations, desc=\"ALNS Optimization\", unit=\"iter\") as pbar:\n",
    "            for i in range(iterations):\n",
    "                # Select destroy and repair operators\n",
    "                destroy_idx = self.select_operator(destroy_weights)\n",
    "                repair_idx = self.select_operator(repair_weights)\n",
    "                \n",
    "                destroy_op = destroy_ops[destroy_idx][0]\n",
    "                repair_op = repair_ops[repair_idx][0]\n",
    "                \n",
    "                # Calculate destroy percentage (adjust over time)\n",
    "                destroy_percentage = 0.4 - (0.3 * i / iterations)\n",
    "                destroy_count = max(1, int(len(self.locations) * destroy_percentage))\n",
    "                \n",
    "                # Apply destroy operator\n",
    "                destroyed_solution, removed_locations = destroy_op(current_solution, destroy_count)\n",
    "                \n",
    "                # Apply repair operator\n",
    "                new_solution = repair_op(destroyed_solution, removed_locations)\n",
    "                \n",
    "                # Evaluate new solution\n",
    "                new_cost = self.evaluate_solution(new_solution)\n",
    "                current_cost = self.evaluate_solution(current_solution)\n",
    "                \n",
    "                # Update best solution if better\n",
    "                if new_cost < best_cost:\n",
    "                    best_solution = new_solution.copy()\n",
    "                    best_cost = new_cost\n",
    "                    score = 10  # Big improvement score\n",
    "                    pbar.set_description(f\"New best! Cost: {best_cost:.2f}\")\n",
    "                else:\n",
    "                    # Accept with probability based on temperature\n",
    "                    delta = new_cost - current_cost\n",
    "                    acceptance_prob = math.exp(-delta / temperature) if delta > 0 else 1.0\n",
    "                    \n",
    "                    if random.random() < acceptance_prob:\n",
    "                        current_solution = new_solution\n",
    "                        score = 5 if new_cost < current_cost else 3  # Small improvement or accepted worse\n",
    "                    else:\n",
    "                        score = 1  # Not accepted\n",
    "                \n",
    "                # Update operator scores\n",
    "                destroy_scores[destroy_idx] += score\n",
    "                repair_scores[repair_idx] += score\n",
    "                \n",
    "                # Cool down temperature\n",
    "                temperature *= cooling_rate\n",
    "                \n",
    "                # Every 10 iterations, update operator weights\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    # Update weights\n",
    "                    destroy_weights = self.update_weights(destroy_weights, destroy_scores)\n",
    "                    repair_weights = self.update_weights(repair_weights, repair_scores)\n",
    "                    \n",
    "                    # Reset scores\n",
    "                    destroy_scores = [0] * len(destroy_ops)\n",
    "                    repair_scores = [0] * len(repair_ops)\n",
    "                    \n",
    "                # Update progress bar with cost information\n",
    "                pbar.set_postfix(best_cost=f\"{best_cost:.2f}\", current_cost=f\"{current_cost:.2f}\")\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Recalculate metrics for best solution\n",
    "        for route in best_solution:\n",
    "            self.recalculate_route_metrics(route)\n",
    "        \n",
    "        # Ensure route IDs and vehicle numbers are sequential\n",
    "        for i, route in enumerate(best_solution):\n",
    "            route['route_id'] = str(i + 1)\n",
    "            route['vehicle_number'] = i + 1\n",
    "        \n",
    "        print(f\"ALNS completed. Best solution has {len(best_solution)} routes with total cost {best_cost:.2f}\")\n",
    "        \n",
    "        return best_solution\n",
    "    \n",
    "    def create_initial_solution(self):\n",
    "        \"\"\"Create initial solution with greedy insertion\"\"\"\n",
    "        # Sort locations by distance from depot\n",
    "        location_distances = []\n",
    "        for idx, location in self.locations.iterrows():\n",
    "            # Calculate aerial distance to depot\n",
    "            dist = haversine_distance(\n",
    "                self.depot['latitude'], self.depot['longitude'],\n",
    "                location['latitude'], location['longitude']\n",
    "            )\n",
    "            loc_id = None\n",
    "            if 'ABS Custumer no' in location:\n",
    "                loc_id = location['ABS Custumer no']\n",
    "            elif 'customer_id' in location:\n",
    "                loc_id = location['customer_id']\n",
    "            else:\n",
    "                loc_id = idx\n",
    "                \n",
    "            location_distances.append((loc_id, dist))\n",
    "        \n",
    "        # Sort by distance (ascending)\n",
    "        location_distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Initialize solution\n",
    "        solution = []\n",
    "        \n",
    "        # Create routes by inserting locations in order\n",
    "        current_route = {\n",
    "            'route_id': '1',\n",
    "            'vehicle_number': 1,\n",
    "            'depot_name': self.depot['depot_name'],\n",
    "            'location_ids': ['DEPOT_START', 'DEPOT_END'],\n",
    "            'weight': 0,\n",
    "            'distance_km': 0,\n",
    "            'time_min': 90,  # Round trip depot service time\n",
    "            'stops': 0\n",
    "        }\n",
    "        \n",
    "        for loc_id, _ in location_distances:\n",
    "            # Get location weight\n",
    "            loc_weight = self.location_weights.get(loc_id, 1.0)\n",
    "            \n",
    "            # Check if adding to current route would exceed capacity\n",
    "            if current_route['weight'] + loc_weight <= self.max_weight:\n",
    "                # Insert before DEPOT_END\n",
    "                current_route['location_ids'].insert(-1, loc_id)\n",
    "                current_route['weight'] += loc_weight\n",
    "                current_route['stops'] += 1\n",
    "            else:\n",
    "                # Recalculate route metrics\n",
    "                self.recalculate_route_metrics(current_route)\n",
    "                \n",
    "                # Add route to solution if it has any stops\n",
    "                if current_route['stops'] > 0:\n",
    "                    solution.append(current_route)\n",
    "                \n",
    "                # Create new route\n",
    "                current_route = {\n",
    "                    'route_id': str(len(solution) + 1),\n",
    "                    'vehicle_number': len(solution) + 1,\n",
    "                    'depot_name': self.depot['depot_name'],\n",
    "                    'location_ids': ['DEPOT_START', loc_id, 'DEPOT_END'],\n",
    "                    'weight': loc_weight,\n",
    "                    'distance_km': 0,\n",
    "                    'time_min': 90,\n",
    "                    'stops': 1\n",
    "                }\n",
    "        \n",
    "        # Add last route if it has any stops\n",
    "        if current_route['stops'] > 0:\n",
    "            self.recalculate_route_metrics(current_route)\n",
    "            solution.append(current_route)\n",
    "        \n",
    "        return solution\n",
    "    \n",
    "    def evaluate_solution(self, solution):\n",
    "        \"\"\"Evaluate solution quality (lower is better)\"\"\"\n",
    "        if not solution:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Cost components\n",
    "        total_distance = sum(route['distance_km'] for route in solution)\n",
    "        total_time = sum(route['time_min'] for route in solution)\n",
    "        vehicle_count = len(solution)\n",
    "        \n",
    "        # Calculate cost (weighted sum)\n",
    "        cost = 0.4 * total_distance + 0.3 * (total_time / 60) + 0.3 * vehicle_count * 100\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def select_operator(self, weights):\n",
    "        \"\"\"Select operator based on weights\"\"\"\n",
    "        total = sum(weights)\n",
    "        r = random.random() * total\n",
    "        \n",
    "        cumulative = 0\n",
    "        for i, weight in enumerate(weights):\n",
    "            cumulative += weight\n",
    "            if r <= cumulative:\n",
    "                return i\n",
    "        \n",
    "        return len(weights) - 1\n",
    "    \n",
    "    def update_weights(self, weights, scores):\n",
    "        \"\"\"Update operator weights based on scores\"\"\"\n",
    "        # Reaction factor (how quickly weights adapt)\n",
    "        reaction = 0.8\n",
    "        \n",
    "        # Apply score adjustments\n",
    "        new_weights = []\n",
    "        for i, (weight, score) in enumerate(zip(weights, scores)):\n",
    "            # Avoid division by zero\n",
    "            if sum(scores) > 0:\n",
    "                new_weight = weight * (1 - reaction) + reaction * (score / sum(scores))\n",
    "            else:\n",
    "                new_weight = weight\n",
    "            \n",
    "            # Ensure minimum weight\n",
    "            new_weight = max(0.1, new_weight)\n",
    "            new_weights.append(new_weight)\n",
    "        \n",
    "        return new_weights\n",
    "\n",
    "def process_cluster_matrix(cluster_id, pairs, locations_df, api_key, cache_db_path):\n",
    "    \"\"\"Process distance matrix for a cluster using individual API calls\"\"\"\n",
    "    # Skip if no pairs\n",
    "    if not pairs:\n",
    "        return cluster_id, None\n",
    "        \n",
    "    print(f\"Processing {len(pairs)} location pairs for cluster {cluster_id}...\")\n",
    "    \n",
    "    # Get batch results using standard routing API with progress bar\n",
    "    batch_results = {}\n",
    "    with tqdm(total=len(pairs), desc=f\"Cluster {cluster_id}\", unit=\"pair\") as pbar:\n",
    "        batch_size = 50\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            batch_result = batch_route_request(batch, api_key)\n",
    "            batch_results.update(batch_result)\n",
    "            pbar.update(len(batch))\n",
    "    \n",
    "    # Convert to distance/time matrices\n",
    "    cluster_locs = locations_df[locations_df['geo_cluster'] == cluster_id]\n",
    "    indices = cluster_locs.index.tolist()\n",
    "    n_locs = len(indices)\n",
    "    \n",
    "    # Initialize matrices\n",
    "    dist_matrix = np.zeros((n_locs, n_locs))\n",
    "    time_matrix = np.zeros((n_locs, n_locs))\n",
    "    \n",
    "    # Fill matrices with results\n",
    "    for i, idx1 in enumerate(indices):\n",
    "        loc1 = cluster_locs.loc[idx1]\n",
    "        \n",
    "        for j, idx2 in enumerate(indices):\n",
    "            if i == j:\n",
    "                # Zero distance for same location\n",
    "                dist_matrix[i, j] = 0\n",
    "                time_matrix[i, j] = 0\n",
    "                continue\n",
    "                \n",
    "            loc2 = cluster_locs.loc[idx2]\n",
    "            \n",
    "            # Try to get from batch results\n",
    "            pair_key = (loc1['latitude'], loc1['longitude'], loc2['latitude'], loc2['longitude'])\n",
    "            \n",
    "            # Try reverse key if not found\n",
    "            if pair_key not in batch_results:\n",
    "                pair_key = (loc2['latitude'], loc2['longitude'], loc1['latitude'], loc1['longitude'])\n",
    "            \n",
    "            if pair_key in batch_results:\n",
    "                distance, time = batch_results[pair_key]\n",
    "                dist_matrix[i, j] = distance\n",
    "                time_matrix[i, j] = time\n",
    "            else:\n",
    "                # If not in batch results, get individual result\n",
    "                result = get_route_segment_metrics(\n",
    "                    {'latitude': loc1['latitude'], 'longitude': loc1['longitude']},\n",
    "                    {'latitude': loc2['latitude'], 'longitude': loc2['longitude']},\n",
    "                    api_key, cache_db_path\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    distance, time = result\n",
    "                    dist_matrix[i, j] = distance\n",
    "                    time_matrix[i, j] = time\n",
    "                else:\n",
    "                    # If no result, use placeholder values\n",
    "                    dist_matrix[i, j] = float('inf')  # Make this path unattractive for routing\n",
    "                    time_matrix[i, j] = float('inf')\n",
    "    \n",
    "    # Return matrices with indices\n",
    "    return cluster_id, {\n",
    "        'indices': indices,\n",
    "        'dist_matrix': dist_matrix,\n",
    "        'time_matrix': time_matrix\n",
    "    }\n",
    "\n",
    "def optimize_routes_global(locations_df, depot_info, max_weight, max_time, service_time_per_unit, depot_service_time=45):\n",
    "    \"\"\"\n",
    "    Global optimization algorithm for vehicle routing problem.\n",
    "    Creates efficiently packed routes that minimize total distance and time.\n",
    "    Includes improved hierarchical distance calculation and ALNS optimization.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    import time as timing_module\n",
    "    import concurrent.futures\n",
    "    \n",
    "    # Record start time for performance tracking\n",
    "    start_time = timing_module.time()\n",
    "    \n",
    "    # Make sure we have a valid HERE API key\n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        print(\"❌ ERROR: No HERE API key found. Route optimization requires HERE API.\")\n",
    "        print(\"Please check the api_keys.json file and try again.\")\n",
    "        raise ValueError(\"Missing HERE API key required for route optimization\")\n",
    "    \n",
    "    # Setup the distance cache\n",
    "    cache_db_path = setup_distance_cache()\n",
    "    \n",
    "    print(f\"Starting global optimization with {len(locations_df)} locations\")\n",
    "    print(f\"Constraints: max_weight={max_weight}, max_time={max_time} min, depot_service_time={depot_service_time} min\")\n",
    "    \n",
    "    # Calculate weight for each location\n",
    "    locations_df = locations_df.copy()\n",
    "    \n",
    "    # Compute weights with fallback options\n",
    "    def compute_weight(row):\n",
    "        if 'Net Weight' in row and pd.notna(row['Net Weight']):\n",
    "            return float(row['Net Weight'])\n",
    "        elif 'weight' in row and pd.notna(row['weight']):\n",
    "            return float(row['weight'])\n",
    "        else:\n",
    "            return 1.0\n",
    "    \n",
    "    locations_df['_weight'] = locations_df.apply(compute_weight, axis=1)\n",
    "    locations_df['_service_time'] = locations_df['_weight'] * service_time_per_unit\n",
    "    \n",
    "    # STEP 1: HIERARCHICAL CLUSTERING APPROACH\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\nSTEP 1: Hierarchical clustering of locations...\")\n",
    "    \n",
    "    # Extract coordinates for clustering\n",
    "    coordinates = locations_df[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Calculate total weight\n",
    "    total_weight = locations_df['_weight'].sum()\n",
    "    \n",
    "    # Determine minimum number of vehicles needed based on weight\n",
    "    min_vehicles_needed = max(1, int(np.ceil(total_weight / max_weight)))\n",
    "    print(f\"Minimum vehicles needed based on total weight ({total_weight:.1f}): {min_vehicles_needed}\")\n",
    "    \n",
    "    # Determine clustering approach based on dataset size\n",
    "    if len(locations_df) <= 10:\n",
    "        # For small datasets, use simple clustering (or none if all fit in one vehicle)\n",
    "        if total_weight <= max_weight * 0.9:\n",
    "            num_clusters = 1\n",
    "            print(f\"All locations can fit in a single vehicle (total weight: {total_weight:.1f} / {max_weight})\")\n",
    "        else:\n",
    "            num_clusters = min_vehicles_needed\n",
    "            print(f\"Using {num_clusters} clusters for small dataset\")\n",
    "        \n",
    "        # Use K-means for small datasets\n",
    "        if num_clusters > 1:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "            locations_df['geo_cluster'] = kmeans.fit_predict(coordinates)\n",
    "        else:\n",
    "            locations_df['geo_cluster'] = 0\n",
    "    else:\n",
    "        # For larger datasets, use a hierarchical approach\n",
    "        print(\"Using hierarchical clustering approach for larger dataset\")\n",
    "        \n",
    "        # Step 1.1: First use DBSCAN to identify dense regions and outliers\n",
    "        eps = 0.1  # approximately 10km\n",
    "        min_samples = 3\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='haversine')\n",
    "        locations_df['density_cluster'] = dbscan.fit_predict(np.radians(coordinates))\n",
    "        \n",
    "        # Count outliers (-1 cluster)\n",
    "        n_outliers = sum(locations_df['density_cluster'] == -1)\n",
    "        n_clusters = len(set(locations_df['density_cluster'])) - (1 if -1 in locations_df['density_cluster'] else 0)\n",
    "        print(f\"DBSCAN identified {n_clusters} dense regions and {n_outliers} outliers\")\n",
    "        \n",
    "        # Step 1.2: For each dense region, apply K-means to create sub-clusters\n",
    "        final_clusters = []\n",
    "        next_cluster_id = 0\n",
    "        \n",
    "        # Process dense regions\n",
    "        for cluster_id in set(locations_df['density_cluster']):\n",
    "            if cluster_id == -1:\n",
    "                continue  # Handle outliers separately\n",
    "                \n",
    "            # Get locations in this dense region\n",
    "            cluster_df = locations_df[locations_df['density_cluster'] == cluster_id]\n",
    "            cluster_coords = cluster_df[['latitude', 'longitude']].values\n",
    "            cluster_weight = cluster_df['_weight'].sum()\n",
    "            \n",
    "            # Determine number of sub-clusters needed\n",
    "            n_subclusters = max(1, int(np.ceil(cluster_weight / max_weight)))\n",
    "            \n",
    "            if len(cluster_df) <= n_subclusters:\n",
    "                # If few locations, assign each to its own cluster\n",
    "                for i, idx in enumerate(cluster_df.index):\n",
    "                    locations_df.loc[idx, 'geo_cluster'] = next_cluster_id + i\n",
    "                next_cluster_id += len(cluster_df)\n",
    "            else:\n",
    "                # Apply K-means to create sub-clusters\n",
    "                sub_kmeans = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)\n",
    "                sub_clusters = sub_kmeans.fit_predict(cluster_coords)\n",
    "                \n",
    "                # Assign sub-cluster IDs\n",
    "                for i, idx in enumerate(cluster_df.index):\n",
    "                    locations_df.loc[idx, 'geo_cluster'] = next_cluster_id + sub_clusters[i]\n",
    "                \n",
    "                next_cluster_id += n_subclusters\n",
    "        \n",
    "        # Step 1.3: Handle outliers\n",
    "        outlier_df = locations_df[locations_df['density_cluster'] == -1]\n",
    "        \n",
    "        if len(outlier_df) > 0:\n",
    "            # For outliers, use K-means with higher number of clusters\n",
    "            outlier_coords = outlier_df[['latitude', 'longitude']].values\n",
    "            outlier_weight = outlier_df['_weight'].sum()\n",
    "            \n",
    "            # Determine number of clusters for outliers\n",
    "            n_outlier_clusters = max(1, int(np.ceil(outlier_weight / max_weight)))\n",
    "            \n",
    "            if len(outlier_df) <= n_outlier_clusters:\n",
    "                # If few outliers, assign each to its own cluster\n",
    "                for i, idx in enumerate(outlier_df.index):\n",
    "                    locations_df.loc[idx, 'geo_cluster'] = next_cluster_id + i\n",
    "                next_cluster_id += len(outlier_df)\n",
    "            else:\n",
    "                # Apply K-means for outliers\n",
    "                outlier_kmeans = KMeans(n_clusters=n_outlier_clusters, random_state=42, n_init=10)\n",
    "                outlier_clusters = outlier_kmeans.fit_predict(outlier_coords)\n",
    "                \n",
    "                # Assign outlier cluster IDs\n",
    "                for i, idx in enumerate(outlier_df.index):\n",
    "                    locations_df.loc[idx, 'geo_cluster'] = next_cluster_id + outlier_clusters[i]\n",
    "                \n",
    "                next_cluster_id += n_outlier_clusters\n",
    "    \n",
    "    # Final cluster information\n",
    "    num_clusters = len(locations_df['geo_cluster'].unique())\n",
    "    print(f\"Final clustering: {num_clusters} clusters for optimization\")\n",
    "    \n",
    "    # STEP 2: EFFICIENT BATCH DISTANCE CALCULATION\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\nSTEP 2: Building distance matrices with batch processing...\")\n",
    "    \n",
    "    # Calculate depot distances for all locations\n",
    "    print(f\"Calculating distances from depot to all {len(locations_df)} locations using batch API...\")\n",
    "    \n",
    "    # Prepare location pairs for batch processing\n",
    "    depot_location_pairs = []\n",
    "    \n",
    "    for _, loc in locations_df.iterrows():\n",
    "        depot_location_pairs.append((\n",
    "            {'latitude': depot_info['latitude'], 'longitude': depot_info['longitude']},\n",
    "            {'latitude': loc['latitude'], 'longitude': loc['longitude']}\n",
    "        ))\n",
    "    \n",
    "    # Process depot distances in batches\n",
    "    depot_batch_results = batch_route_request(depot_location_pairs, api_key)\n",
    "    \n",
    "    # Extract distances and times from batch results\n",
    "    distances_from_depot = []\n",
    "    times_from_depot = []\n",
    "    \n",
    "    for _, loc in locations_df.iterrows():\n",
    "        pair_key = (depot_info['latitude'], depot_info['longitude'], loc['latitude'], loc['longitude'])\n",
    "        \n",
    "        if pair_key in depot_batch_results:\n",
    "            distance, time = depot_batch_results[pair_key]\n",
    "            distances_from_depot.append(distance)\n",
    "            times_from_depot.append(time)\n",
    "        else:\n",
    "            # If not in batch results, get individual result\n",
    "            result = get_route_segment_metrics(\n",
    "                {'latitude': depot_info['latitude'], 'longitude': depot_info['longitude']},\n",
    "                {'latitude': loc['latitude'], 'longitude': loc['longitude']},\n",
    "                api_key, cache_db_path\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                distance, time = result\n",
    "                distances_from_depot.append(distance)\n",
    "                times_from_depot.append(time)\n",
    "            else:\n",
    "                # If no result, use placeholder values\n",
    "                print(f\"❌ ERROR: Unable to get depot distance for location ({loc['latitude']}, {loc['longitude']})\")\n",
    "                distances_from_depot.append(None)\n",
    "                times_from_depot.append(None)\n",
    "    \n",
    "    # Add to dataframe\n",
    "    locations_df['_distance_from_depot'] = distances_from_depot\n",
    "    locations_df['_time_from_depot'] = times_from_depot\n",
    "    \n",
    "    # Remove locations with no valid depot distances\n",
    "    invalid_locations = locations_df[locations_df['_distance_from_depot'].isna()]\n",
    "    if not invalid_locations.empty:\n",
    "        print(f\"⚠️ Warning: Removing {len(invalid_locations)} locations with no valid depot distances\")\n",
    "        locations_df = locations_df[~locations_df['_distance_from_depot'].isna()]\n",
    "    \n",
    "    # Calculate intra-cluster distance matrices\n",
    "    print(\"Building distance matrices within each cluster using batch API...\")\n",
    "    \n",
    "    # Create a dictionary to hold location pairs by cluster\n",
    "    cluster_pairs = {}\n",
    "    \n",
    "    # Group locations by cluster\n",
    "    for cluster_id in locations_df['geo_cluster'].unique():\n",
    "        cluster_locs = locations_df[locations_df['geo_cluster'] == cluster_id]\n",
    "        \n",
    "        if len(cluster_locs) <= 1:\n",
    "            continue  # Skip singleton clusters\n",
    "            \n",
    "        # Create pairs of locations within this cluster\n",
    "        pairs = []\n",
    "        indices = cluster_locs.index.tolist()\n",
    "        \n",
    "        for i, idx1 in enumerate(indices):\n",
    "            loc1 = cluster_locs.loc[idx1]\n",
    "            \n",
    "            for j, idx2 in enumerate(indices):\n",
    "                if i >= j:\n",
    "                    continue  # Skip duplicate and self pairs\n",
    "                    \n",
    "                loc2 = cluster_locs.loc[idx2]\n",
    "                \n",
    "                # Apply filtering based on aerial distance\n",
    "                aerial_dist = haversine_distance(\n",
    "                    loc1['latitude'], loc1['longitude'],\n",
    "                    loc2['latitude'], loc2['longitude']\n",
    "                )\n",
    "                \n",
    "                # Only include if aerial distance is reasonable\n",
    "                if aerial_dist < 50:  # 50km threshold\n",
    "                    pairs.append((\n",
    "                        {'latitude': loc1['latitude'], 'longitude': loc1['longitude']},\n",
    "                        {'latitude': loc2['latitude'], 'longitude': loc2['longitude']}\n",
    "                    ))\n",
    "        \n",
    "        cluster_pairs[cluster_id] = pairs\n",
    "    \n",
    "    # Process distance matrices using concurrent processing\n",
    "    cluster_distance_matrices = {}\n",
    "    \n",
    "    # Process clusters (limiting concurrency to avoid rate limiting)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_cluster = {\n",
    "            executor.submit(process_cluster_matrix, cluster_id, pairs, locations_df, api_key, cache_db_path): cluster_id\n",
    "            for cluster_id, pairs in cluster_pairs.items()\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_cluster):\n",
    "            cluster_id = future_to_cluster[future]\n",
    "            try:\n",
    "                cluster_id, matrix_data = future.result()\n",
    "                if matrix_data:\n",
    "                    cluster_distance_matrices[cluster_id] = matrix_data\n",
    "                    print(f\"✓ Completed distance matrix for cluster {cluster_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing cluster {cluster_id}: {e}\")\n",
    "    \n",
    "    # STEP 3: ADAPTIVE LARGE NEIGHBORHOOD SEARCH (ALNS)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\nSTEP 3: Running Adaptive Large Neighborhood Search optimization...\")\n",
    "    \n",
    "    # Use ALNS for optimization\n",
    "    alns = ALNS(depot_info, locations_df, max_weight, max_time, service_time_per_unit, api_key, cache_db_path)\n",
    "    all_routes = alns.run(iterations=50)  # Adjust iteration count as needed\n",
    "    \n",
    "    # STEP 4: ROUTE CONSOLIDATION WITH PROXIMITY PRIORITIZATION\n",
    "    # ------------------------------------------------------------\n",
    "    if len(all_routes) > 1:\n",
    "        print(\"\\nSTEP 4: Attempting to consolidate routes with proximity prioritization...\")\n",
    "        all_routes = consolidate_routes_with_proximity(all_routes, max_weight, max_time, api_key, cache_db_path)\n",
    "    \n",
    "    # Generate final summary\n",
    "    total_distance = sum(route['distance_km'] for route in all_routes)\n",
    "    total_time = sum(route['time_min'] for route in all_routes)\n",
    "    total_weight = sum(route.get('weight', 0) for route in all_routes)\n",
    "    \n",
    "    print(f\"\\n✅ Optimization complete in {timing_module.time() - start_time:.1f} seconds!\")\n",
    "    print(f\"Created {len(all_routes)} routes with total distance: {total_distance:.2f}km\")\n",
    "    print(f\"Total time: {total_time/60:.2f}hrs, Total weight: {total_weight:.2f}\")\n",
    "    \n",
    "    # Return the created routes and no remaining locations\n",
    "    return all_routes, total_distance, total_time, len(all_routes), pd.DataFrame()\n",
    "\n",
    "def consolidate_routes_with_proximity(routes, max_weight, max_time, api_key, cache_db_path):\n",
    "    \"\"\"\n",
    "    Try to consolidate routes using geographic proximity and time compatibility.\n",
    "    Prioritizes merging geographically close routes with compatible time windows.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    \n",
    "    if len(routes) <= 1:\n",
    "        return routes\n",
    "    \n",
    "    print(f\"Starting with {len(routes)} routes, attempting consolidation...\")\n",
    "    \n",
    "    # Calculate route centers of gravity\n",
    "    route_centers = []\n",
    "    for route in routes:\n",
    "        coords = []\n",
    "        weights = []\n",
    "        \n",
    "        # Get coordinates of all stops\n",
    "        if 'route' in route:\n",
    "            for lat, lng in route['route'][1:-1]:  # Skip depot\n",
    "                coords.append((lat, lng))\n",
    "                weights.append(1)\n",
    "        \n",
    "        # If no route coordinates, skip\n",
    "        if not coords:\n",
    "            route_centers.append((None, None))\n",
    "            continue\n",
    "        \n",
    "        # Calculate weighted center\n",
    "        lat_sum = sum(coord[0] * weight for coord, weight in zip(coords, weights))\n",
    "        lng_sum = sum(coord[1] * weight for coord, weight in zip(coords, weights))\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            center = (lat_sum / total_weight, lng_sum / total_weight)\n",
    "        else:\n",
    "            center = (None, None)\n",
    "            \n",
    "        route_centers.append(center)\n",
    "    \n",
    "    # Calculate aerial distances between all route centers\n",
    "    route_distances = {}\n",
    "    for i in range(len(routes)):\n",
    "        for j in range(i+1, len(routes)):\n",
    "            center_i = route_centers[i]\n",
    "            center_j = route_centers[j]\n",
    "            \n",
    "            # Skip if any center is invalid\n",
    "            if None in center_i or None in center_j:\n",
    "                continue\n",
    "                \n",
    "            # Calculate aerial distance\n",
    "            dist = haversine_distance(center_i[0], center_i[1], center_j[0], center_j[1])\n",
    "            route_distances[(i, j)] = dist\n",
    "    \n",
    "    # Sort route pairs by distance\n",
    "    sorted_pairs = sorted(route_distances.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Start merging from closest pairs\n",
    "    improvement = True\n",
    "    iteration = 0\n",
    "    max_iterations = 5\n",
    "    \n",
    "    while improvement and iteration < max_iterations:\n",
    "        improvement = False\n",
    "        iteration += 1\n",
    "        \n",
    "        # Make a copy of routes for this iteration\n",
    "        current_routes = copy.deepcopy(routes)\n",
    "        \n",
    "        # Add _consolidated flag to track merged routes\n",
    "        for route in current_routes:\n",
    "            route['_consolidated'] = False\n",
    "        \n",
    "        # Try merging in order of proximity\n",
    "        for (i, j), distance in sorted_pairs:\n",
    "            # Skip if already consolidated\n",
    "            if current_routes[i].get('_consolidated', False) or current_routes[j].get('_consolidated', False):\n",
    "                continue\n",
    "            \n",
    "            route1 = current_routes[i]\n",
    "            route2 = current_routes[j]\n",
    "            \n",
    "            # Check if combined weight would exceed capacity\n",
    "            combined_weight = route1.get('weight', 0) + route2.get('weight', 0)\n",
    "            if combined_weight > max_weight:\n",
    "                continue\n",
    "            \n",
    "            # Check time compatibility\n",
    "            # For now, simple check of total time - but could be enhanced with detailed time windows\n",
    "            combined_time = route1.get('time_min', 0) + route2.get('time_min', 0) - 90  # Remove duplicate depot time\n",
    "            if combined_time > max_time:\n",
    "                continue\n",
    "            \n",
    "            # Try to merge routes\n",
    "            merged_route = merge_routes_enhanced(route1, route2, api_key, cache_db_path)\n",
    "            \n",
    "            # Check if merged route is feasible\n",
    "            if merged_route and merged_route.get('time_min', float('inf')) <= max_time:\n",
    "                print(f\"Found feasible merge: Routes {route1['route_id']} + {route2['route_id']} → New route with {merged_route['stops']} stops\")\n",
    "                \n",
    "                # Mark original routes as consolidated\n",
    "                route1['_consolidated'] = True\n",
    "                route2['_consolidated'] = True\n",
    "                \n",
    "                # Add the merged route\n",
    "                merged_route['_consolidated'] = False\n",
    "                current_routes.append(merged_route)\n",
    "                \n",
    "                # Note improvement\n",
    "                improvement = True\n",
    "                break\n",
    "        \n",
    "        if improvement:\n",
    "            # Create new routes list without the consolidated ones\n",
    "            routes = [r for r in current_routes if not r.get('_consolidated', False)]\n",
    "            \n",
    "            # Recalculate route centers and distances for next iteration\n",
    "            route_centers = []\n",
    "            for route in routes:\n",
    "                coords = []\n",
    "                weights = []\n",
    "                \n",
    "                # Get coordinates of all stops\n",
    "                if 'route' in route:\n",
    "                    for lat, lng in route['route'][1:-1]:  # Skip depot\n",
    "                        coords.append((lat, lng))\n",
    "                        weights.append(1)\n",
    "                \n",
    "                # If no route coordinates, skip\n",
    "                if not coords:\n",
    "                    route_centers.append((None, None))\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted center\n",
    "                lat_sum = sum(coord[0] * weight for coord, weight in zip(coords, weights))\n",
    "                lng_sum = sum(coord[1] * weight for coord, weight in zip(coords, weights))\n",
    "                total_weight = sum(weights)\n",
    "                \n",
    "                if total_weight > 0:\n",
    "                    center = (lat_sum / total_weight, lng_sum / total_weight)\n",
    "                else:\n",
    "                    center = (None, None)\n",
    "                    \n",
    "                route_centers.append(center)\n",
    "            \n",
    "            # Recalculate distances\n",
    "            route_distances = {}\n",
    "            for i in range(len(routes)):\n",
    "                for j in range(i+1, len(routes)):\n",
    "                    center_i = route_centers[i]\n",
    "                    center_j = route_centers[j]\n",
    "                    \n",
    "                    # Skip if any center is invalid\n",
    "                    if None in center_i or None in center_j:\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate aerial distance\n",
    "                    dist = haversine_distance(center_i[0], center_i[1], center_j[0], center_j[1])\n",
    "                    route_distances[(i, j)] = dist\n",
    "            \n",
    "            # Resort pairs\n",
    "            sorted_pairs = sorted(route_distances.items(), key=lambda x: x[1])\n",
    "            \n",
    "            print(f\"Consolidated to {len(routes)} routes\")\n",
    "    \n",
    "    # Ensure consistent route IDs and vehicle numbers\n",
    "    for i, route in enumerate(routes):\n",
    "        route_id = str(i + 1)\n",
    "        route['route_id'] = route_id\n",
    "        route['vehicle_number'] = i + 1\n",
    "        # Remove any internal flags\n",
    "        if '_consolidated' in route:\n",
    "            del route['_consolidated']\n",
    "    \n",
    "    print(f\"Final route count after consolidation: {len(routes)}\")\n",
    "    return routes\n",
    "\n",
    "def merge_routes_enhanced(route1, route2, api_key, cache_db_path):\n",
    "    \"\"\"\n",
    "    Enhanced route merging with improved sequence optimization.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    \n",
    "    # Get all stops excluding the depot for both routes\n",
    "    stops1 = route1.get('location_ids', [])[1:-1]  # Skip DEPOT_START and DEPOT_END\n",
    "    stops2 = route2.get('location_ids', [])[1:-1]\n",
    "    \n",
    "    if not stops1 or not stops2:\n",
    "        return None  # Can't merge if either has no stops\n",
    "    \n",
    "    # Create a new route\n",
    "    merged_route = copy.deepcopy(route1)\n",
    "    \n",
    "    # Try different merging strategies and pick the best\n",
    "    merge_options = []\n",
    "    \n",
    "    # Option 1: Append route2 to route1\n",
    "    option1 = ['DEPOT_START'] + stops1 + stops2 + ['DEPOT_END']\n",
    "    merge_options.append(option1)\n",
    "    \n",
    "    # Option 2: Append route1 to route2\n",
    "    option2 = ['DEPOT_START'] + stops2 + stops1 + ['DEPOT_END']\n",
    "    merge_options.append(option2)\n",
    "    \n",
    "    # Option 3: Try to find a better ordering based on proximity\n",
    "    # Simplified approach for now\n",
    "    option3 = ['DEPOT_START'] + stops1 + stops2 + ['DEPOT_END']\n",
    "    merge_options.append(option3)\n",
    "    \n",
    "    # Evaluate each option\n",
    "    best_option = option1  # Default to option 1\n",
    "    \n",
    "    # Use best option for merge\n",
    "    merged_route['location_ids'] = best_option\n",
    "    \n",
    "    # Merge weights\n",
    "    merged_route['weight'] = route1.get('weight', 0) + route2.get('weight', 0)\n",
    "    merged_route['stops'] = len(best_option) - 2  # Exclude DEPOT_START and DEPOT_END\n",
    "    \n",
    "    # Merge route coordinates\n",
    "    if 'route' in route1 and 'route' in route2:\n",
    "        # Simple approach - actual merging would depend on the selected option\n",
    "        merged_route['route'] = [route1['route'][0]] + route1['route'][1:-1] + route2['route'][1:-1] + [route1['route'][-1]]\n",
    "    \n",
    "    # Set combined cost estimates\n",
    "    merged_route['distance_km'] = route1.get('distance_km', 0) + route2.get('distance_km', 0) - 5.0  # Approximate saving\n",
    "    merged_route['time_min'] = route1.get('time_min', 0) + route2.get('time_min', 0) - 90.0  # Remove duplicate depot time\n",
    "    \n",
    "    # Set new route ID\n",
    "    merged_route['route_id'] = f\"{min(int(route1['route_id']), int(route2['route_id']))}\"\n",
    "    merged_route['vehicle_number'] = int(merged_route['route_id'])\n",
    "    \n",
    "    return merged_route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5d4a0-bf79-4065-a1f0-26ca9080e0fd",
   "metadata": {},
   "source": [
    "## Cell 9: Route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31ead5f-eb53-48d6-b657-78690c7faaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_routes(locations_df, depot_info, max_weight, max_time, service_time_per_unit, depot_service_time=45, start_vehicle_count=0):\n",
    "    \"\"\"\n",
    "    Create multiple routes to serve all locations from a depot.\n",
    "    This version fully uses the global optimization approach from Cell 8.\n",
    "    \"\"\"\n",
    "    print(f\"Using global optimization approach for {len(locations_df)} locations\")\n",
    "    \n",
    "    # Call the global optimization function directly\n",
    "    all_routes, total_distance, total_time, vehicle_count, remaining_locations = optimize_routes_global(\n",
    "        locations_df, \n",
    "        depot_info, \n",
    "        max_weight, \n",
    "        max_time, \n",
    "        service_time_per_unit, \n",
    "        depot_service_time\n",
    "    )\n",
    "    \n",
    "    # Apply vehicle numbering based on start_vehicle_count\n",
    "    if start_vehicle_count > 0:\n",
    "        for i, route in enumerate(all_routes):\n",
    "            route['vehicle_number'] = start_vehicle_count + i + 1\n",
    "            route['route_id'] = str(start_vehicle_count + i + 1)\n",
    "    \n",
    "    return all_routes, total_distance, total_time, vehicle_count, remaining_locations\n",
    "\n",
    "\n",
    "def run_optimization(df_clean, depot_df, optimization_params, status_updater=None):\n",
    "    \"\"\"Run the optimization for each depot - with reduced concurrency\"\"\"\n",
    "    print(\"\\n=== OPTIMIZING ROUTES ===\")\n",
    "    print(\"Starting route optimization process...\")\n",
    "\n",
    "    # Update status if function provided\n",
    "    if status_updater:\n",
    "        status_updater('Optimization', 'Testing API connection')\n",
    "    \n",
    "    # Initialize results containers\n",
    "    all_routes = []\n",
    "    routes_summary = []\n",
    "    locations_not_visited = []\n",
    "    \n",
    "    # Performance tracking\n",
    "    optimization_start_time = time.time()\n",
    "    \n",
    "    # Global vehicle counter to ensure unique IDs across depots\n",
    "    global_vehicle_count = 0\n",
    "    \n",
    "    # Test HERE API connectivity once at the beginning\n",
    "    api_key = load_api_key()\n",
    "    if api_key:\n",
    "        # Setup distance cache\n",
    "        cache_db_path = setup_distance_cache()\n",
    "        \n",
    "        # Use coordinates from the first depot for testing\n",
    "        first_depot = depot_df.iloc[0]\n",
    "        test_here_api_connection(\n",
    "            api_key, \n",
    "            first_depot['latitude'], \n",
    "            first_depot['longitude']\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚠️ No API key available for optimization\")\n",
    "        return [], [], []\n",
    "    \n",
    "    # Process depots sequentially to avoid rate limiting\n",
    "    for idx, depot_row in depot_df.iterrows():\n",
    "        depot_name = depot_row['depot_name']\n",
    "        print(f\"\\nProcessing depot: {depot_name}\")\n",
    "        \n",
    "        # Get locations assigned to this depot using cluster_name\n",
    "        depot_locations = df_clean[df_clean['cluster_name'] == depot_name]\n",
    "        \n",
    "        print(f\"Found {len(depot_locations)} locations assigned to this depot\")\n",
    "        \n",
    "        # Skip if no locations for this depot\n",
    "        if len(depot_locations) == 0:\n",
    "            print(f\"⚠️ No locations assigned to depot: {depot_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Optimizing routes for {len(depot_locations)} locations...\")\n",
    "        \n",
    "        # Run optimization with the global vehicle count to ensure unique IDs\n",
    "        depot_routes, total_distance, total_time, vehicle_count, unvisited = create_routes(\n",
    "            depot_locations, \n",
    "            depot_row, \n",
    "            optimization_params['vehicle_capacity'], \n",
    "            optimization_params['max_route_time_min'], \n",
    "            optimization_params['service_time_per_unit_min'],\n",
    "            optimization_params['depot_service_time_min'],\n",
    "            global_vehicle_count  # Pass the current global vehicle count\n",
    "        )\n",
    "        \n",
    "        # Update the global vehicle count\n",
    "        global_vehicle_count += vehicle_count\n",
    "        \n",
    "        # Update results\n",
    "        all_routes.extend(depot_routes)\n",
    "        \n",
    "        # Create summary\n",
    "        summary = {\n",
    "            'depot_name': depot_name,\n",
    "            'total_locations': len(depot_locations),\n",
    "            'locations_visited': len(depot_locations) - len(unvisited),\n",
    "            'locations_unvisited': len(unvisited),\n",
    "            'routes_created': vehicle_count,\n",
    "            'total_distance_km': total_distance,\n",
    "            'total_time_hours': total_time / 60\n",
    "        }\n",
    "        routes_summary.append(summary)\n",
    "        \n",
    "        # Record unvisited locations\n",
    "        if not unvisited.empty:\n",
    "            for i, loc in unvisited.iterrows():\n",
    "                locations_not_visited.append({\n",
    "                    'location_id': i,\n",
    "                    'depot_name': depot_name,\n",
    "                    'latitude': loc['latitude'],\n",
    "                    'longitude': loc['longitude'],\n",
    "                    'weight': loc.get('weight', 1)\n",
    "                })\n",
    "    \n",
    "    total_optimization_time = time.time() - optimization_start_time\n",
    "    print(f\"\\n✅ Optimization complete in {total_optimization_time:.1f} seconds!\")\n",
    "    \n",
    "    return all_routes, routes_summary, locations_not_visited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a198b5-f274-47ae-b5a0-c477b636cd3e",
   "metadata": {},
   "source": [
    "## Cell 10: Data Enchancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6d76942-c0fe-4983-a75d-782eef405652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_original_data(df_clean, all_routes, depot_df=None):\n",
    "    \"\"\"\n",
    "    Enhance the original dataset with route information, ensuring consistent route positions.\n",
    "    Uses on-demand route calculations and parallel processing for efficiency.\n",
    "    \"\"\"\n",
    "    # Create a copy of the original data\n",
    "    enhanced_df = df_clean.copy()\n",
    "    \n",
    "    # Add the new columns with default values\n",
    "    enhanced_df['route_id'] = None  # Use None instead of np.nan for string values\n",
    "    enhanced_df['route_position'] = np.nan\n",
    "    enhanced_df['route_distance_km'] = np.nan\n",
    "    enhanced_df['travel_time_from_last_stop'] = np.nan\n",
    "    enhanced_df['service_time_min'] = np.nan\n",
    "    enhanced_df['cumulative_time_min'] = np.nan\n",
    "    enhanced_df['location_type'] = \"CUSTOMER\"  # Default location type\n",
    "    \n",
    "    # Initialize a list to collect all stops, including depots\n",
    "    all_stops = []\n",
    "    \n",
    "    # Setup distance cache\n",
    "    cache_db_path = setup_distance_cache()\n",
    "    api_key = load_api_key()\n",
    "    \n",
    "    # First, enrich routes with depot coordinates from depot_df\n",
    "    if depot_df is not None:\n",
    "        print(\"Enriching routes with depot information...\")\n",
    "        for route in all_routes:\n",
    "            if 'depot_name' in route:\n",
    "                # Look up depot coordinates from depot_df\n",
    "                matching_depots = depot_df[depot_df['depot_name'] == route['depot_name']]\n",
    "                if not matching_depots.empty:\n",
    "                    depot_row = matching_depots.iloc[0]\n",
    "                    route['depot_latitude'] = depot_row['latitude']\n",
    "                    route['depot_longitude'] = depot_row['longitude']\n",
    "                    \n",
    "                    # Add route key with coordinates if it doesn't exist\n",
    "                    if 'route' not in route:\n",
    "                        # Add depot at beginning and end of route\n",
    "                        route['route'] = [(depot_row['latitude'], depot_row['longitude'])]\n",
    "                        \n",
    "                        # Add customer locations in between (if we have them)\n",
    "                        for loc_id in route.get('location_ids', [])[1:-1]:  # Skip DEPOT_START and DEPOT_END\n",
    "                            found = False\n",
    "                            for idx, row in df_clean.iterrows():\n",
    "                                if ('ABS Custumer no' in row and row['ABS Custumer no'] == loc_id) or \\\n",
    "                                   ('customer_id' in row and row['customer_id'] == loc_id) or \\\n",
    "                                   idx == loc_id:\n",
    "                                    route['route'].append((row['latitude'], row['longitude']))\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            \n",
    "                            if not found:\n",
    "                                print(f\"⚠️ Could not find coordinates for location {loc_id} in route {route['route_id']}\")\n",
    "                        \n",
    "                        # Add depot at end\n",
    "                        route['route'].append((depot_row['latitude'], depot_row['longitude']))\n",
    "                else:\n",
    "                    print(f\"⚠️ Could not find depot information for {route['depot_name']}\")\n",
    "\n",
    "    # Define a function to process a single route\n",
    "    def process_route(route_info):\n",
    "        route_id = route_info['route_id']\n",
    "        depot_name = route_info['depot_name']\n",
    "        \n",
    "        print(f\"Processing route {route_id} for depot {depot_name}\")\n",
    "        \n",
    "        # Get depot coordinates for adding depot entries\n",
    "        depot_coords = None\n",
    "        depot_lat = None\n",
    "        depot_lng = None\n",
    "        if 'route' in route_info and len(route_info['route']) >= 2:\n",
    "            try:\n",
    "                depot_coords = route_info['route'][0]  # First coordinate is depot\n",
    "                depot_lat = float(depot_coords[0])\n",
    "                depot_lng = float(depot_coords[1])\n",
    "                \n",
    "                # Validate ranges\n",
    "                if not (-90 <= depot_lat <= 90) or not (-180 <= depot_lng <= 180):\n",
    "                    print(f\"⚠️ Invalid depot coordinates: {depot_lat}, {depot_lng}\")\n",
    "                    depot_lat = depot_lng = None\n",
    "            except (ValueError, TypeError, IndexError) as e:\n",
    "                print(f\"⚠️ Error parsing depot coordinates: {depot_coords}, {e}\")\n",
    "                depot_lat = depot_lng = None\n",
    "        \n",
    "        # If no valid route coordinates, try to get from depot_info\n",
    "        if depot_lat is None:\n",
    "            # Try to get from route attributes directly instead of using hasattr\n",
    "            if 'depot_latitude' in route_info and 'depot_longitude' in route_info:\n",
    "                try:\n",
    "                    depot_lat = float(route_info['depot_latitude'])\n",
    "                    depot_lng = float(route_info['depot_longitude'])\n",
    "                    \n",
    "                    # Validate ranges\n",
    "                    if not (-90 <= depot_lat <= 90) or not (-180 <= depot_lng <= 180):\n",
    "                        print(f\"⚠️ Invalid depot info coordinates: {depot_lat}, {depot_lng}\")\n",
    "                        depot_lat = depot_lng = None\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"⚠️ Error parsing depot info coordinates: {e}\")\n",
    "        \n",
    "        # Skip routes with no stops\n",
    "        if 'location_ids' not in route_info or not route_info['location_ids']:\n",
    "            print(f\"Skipping route {route_id} - no location_ids found\")\n",
    "            return []\n",
    "            \n",
    "        # Get location IDs for this route - these are the customer/location identifiers\n",
    "        location_ids = route_info['location_ids']\n",
    "        print(f\"Route {route_id} has {len(location_ids)} location_ids: {location_ids}\")\n",
    "        \n",
    "        # Get segment-specific distances and times\n",
    "        segment_distances = route_info.get('segment_distances', [])\n",
    "        segment_times = route_info.get('segment_times', [])\n",
    "        service_times = route_info.get('location_service_times', [])\n",
    "        \n",
    "        # If we don't have pre-calculated segments, calculate them on-demand\n",
    "        if len(segment_distances) != len(location_ids) - 1 or len(segment_times) != len(location_ids) - 1:\n",
    "            print(f\"Calculating missing segment data for route {route_id}\")\n",
    "            \n",
    "            # Re-calculate all segments\n",
    "            segment_distances = []\n",
    "            segment_times = []\n",
    "            service_times = []\n",
    "            \n",
    "            # Calculate depot service time\n",
    "            depot_service_time = 45  # default\n",
    "            if 'depot_service_time_min' in route_info:\n",
    "                depot_service_time = route_info['depot_service_time_min']\n",
    "            service_times.append(depot_service_time)\n",
    "            \n",
    "            # Process all locations to get their data\n",
    "            route_locs = []\n",
    "            \n",
    "            # Add depot as first location with validated coordinates\n",
    "            if depot_lat is not None and depot_lng is not None:\n",
    "                depot_loc = {\n",
    "                    'latitude': depot_lat,\n",
    "                    'longitude': depot_lng,\n",
    "                    'is_depot': True\n",
    "                }\n",
    "                route_locs.append(depot_loc)\n",
    "            else:\n",
    "                print(f\"⚠️ Missing valid depot coordinates for route {route_id}\")\n",
    "                # Create a placeholder to maintain sequence\n",
    "                depot_loc = {\n",
    "                    'latitude': 0.0,  # Invalid value that will be caught by validation\n",
    "                    'longitude': 0.0,\n",
    "                    'is_depot': True\n",
    "                }\n",
    "                route_locs.append(depot_loc)\n",
    "            \n",
    "            # Find and add all customer locations\n",
    "            for loc_id in location_ids[1:-1]:  # Skip DEPOT_START and DEPOT_END\n",
    "                # Find this location in the dataset\n",
    "                loc_data = None\n",
    "                matches = None\n",
    "                \n",
    "                # Try different strategies to match the location ID\n",
    "                if isinstance(loc_id, (int, float)):\n",
    "                    # Look for this ID in common ID columns\n",
    "                    if 'ABS Custumer no' in enhanced_df.columns:\n",
    "                        matches = enhanced_df['ABS Custumer no'] == loc_id\n",
    "                    elif 'ABS Customer no' in enhanced_df.columns:\n",
    "                        matches = enhanced_df['ABS Customer no'] == loc_id\n",
    "                    elif 'customer_id' in enhanced_df.columns:\n",
    "                        matches = enhanced_df['customer_id'] == loc_id\n",
    "                    else:\n",
    "                        # Fallback to index if it's numeric\n",
    "                        matches = enhanced_df.index == loc_id\n",
    "                else:\n",
    "                    # If it's not numeric, it might be the index as string\n",
    "                    try:\n",
    "                        # Try to convert to numeric if it's a string representation of a number\n",
    "                        numeric_id = float(loc_id) if isinstance(loc_id, str) else loc_id\n",
    "                        matches = enhanced_df.index == numeric_id\n",
    "                    except (ValueError, TypeError):\n",
    "                        # If conversion fails, it might be a string ID\n",
    "                        if 'customer_id' in enhanced_df.columns and enhanced_df['customer_id'].dtype == 'object':\n",
    "                            matches = enhanced_df['customer_id'] == loc_id\n",
    "                        else:\n",
    "                            # No matches if we can't figure out how to match\n",
    "                            matches = pd.Series(False, index=enhanced_df.index)\n",
    "                \n",
    "                # If we found a match, get the location data\n",
    "                if matches is not None and matches.any():\n",
    "                    loc_row = enhanced_df[matches].iloc[0]\n",
    "                    # Validate coordinates before adding\n",
    "                    try:\n",
    "                        lat = float(loc_row['latitude'])\n",
    "                        lng = float(loc_row['longitude'])\n",
    "                        \n",
    "                        # Check valid ranges\n",
    "                        if not (-90 <= lat <= 90) or not (-180 <= lng <= 180):\n",
    "                            print(f\"⚠️ Invalid location coordinates for {loc_id}: {lat}, {lng}\")\n",
    "                            continue\n",
    "                            \n",
    "                        loc_data = {\n",
    "                            'latitude': lat,\n",
    "                            'longitude': lng,\n",
    "                            'is_depot': False\n",
    "                        }\n",
    "                        \n",
    "                        # Calculate service time\n",
    "                        if 'Net Weight' in loc_row and pd.notna(loc_row['Net Weight']):\n",
    "                            weight = float(loc_row['Net Weight'])\n",
    "                        elif 'weight' in loc_row and pd.notna(loc_row['weight']):\n",
    "                            weight = float(loc_row['weight'])\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        \n",
    "                        service_time = weight * 0.5  # Default service time per unit\n",
    "                        if 'service_time_per_unit_min' in route_info:\n",
    "                            service_time = weight * route_info['service_time_per_unit_min']\n",
    "                        \n",
    "                        service_times.append(service_time)\n",
    "                        route_locs.append(loc_data)\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"⚠️ Error parsing location coordinates for {loc_id}: {e}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: Could not find match for location_id {loc_id} in route {route_id}\")\n",
    "            \n",
    "            # Add depot as last location\n",
    "            route_locs.append(depot_loc if depot_loc else {\n",
    "                'latitude': 0.0,\n",
    "                'longitude': 0.0,\n",
    "                'is_depot': True\n",
    "            })\n",
    "            service_times.append(depot_service_time)\n",
    "            \n",
    "            # Calculate segments with progress indication\n",
    "            print(f\"Calculating {len(route_locs)-1} segments for route {route_id}\")\n",
    "            for i in range(len(route_locs) - 1):\n",
    "                origin = route_locs[i]\n",
    "                destination = route_locs[i + 1]\n",
    "                \n",
    "                # Get routing metrics from HERE API (or cache)\n",
    "                result = get_route_segment_metrics(origin, destination, api_key, cache_db_path)\n",
    "                \n",
    "                if result is not None:\n",
    "                    distance_km, time_min = result\n",
    "                    segment_distances.append(distance_km)\n",
    "                    segment_times.append(time_min)\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: Could not get routing metrics for segment {i} in route {route_id}\")\n",
    "                    # Use fallback values based on aerial distance\n",
    "                    try:\n",
    "                        aerial_dist = haversine_distance(\n",
    "                            float(origin['latitude']), float(origin['longitude']),\n",
    "                            float(destination['latitude']), float(destination['longitude'])\n",
    "                        )\n",
    "                        # Approximate time based on average speed of 50km/h\n",
    "                        aerial_time = aerial_dist * 60 / 50  # minutes\n",
    "                        segment_distances.append(aerial_dist)\n",
    "                        segment_times.append(aerial_time)\n",
    "                        print(f\"Using aerial distance fallback: {aerial_dist:.2f}km, {aerial_time:.2f}min\")\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"⚠️ Error calculating aerial distance: {e}\")\n",
    "                        segment_distances.append(0)\n",
    "                        segment_times.append(0)\n",
    "        \n",
    "        # ===== PHASE 1: Find and calculate basic values for all stops =====\n",
    "        # Collect all stops in this route\n",
    "        route_stops = []\n",
    "        sequence_position = 0\n",
    "        processed_location_ids = set()\n",
    "        \n",
    "        # Process DEPOT_START\n",
    "        depot_service_time = service_times[0] if len(service_times) > 0 else 45  # default 45 min\n",
    "        \n",
    "        if depot_coords and location_ids[0] == \"DEPOT_START\":\n",
    "            depot_start_row = {\n",
    "                'route_id': route_id,\n",
    "                'sequence_idx': -1,  # Special index to ensure it sorts first\n",
    "                'raw_data': {\n",
    "                    'route_distance_km': 0,\n",
    "                    'travel_time_from_last_stop': 0,\n",
    "                    'service_time_min': depot_service_time,\n",
    "                },\n",
    "                'is_depot': True,\n",
    "                'is_depot_start': True,\n",
    "                'location_type': \"DEPOT_START\",\n",
    "                'cluster_name': depot_name,\n",
    "                'latitude': depot_lat,\n",
    "                'longitude': depot_lng,\n",
    "                'distance_to_depot_km': 0,\n",
    "                'depot_latitude': depot_lat,\n",
    "                'depot_longitude': depot_lng,\n",
    "                'depot_formatted_address': f\"Depot {depot_name}\"\n",
    "            }\n",
    "            route_stops.append(depot_start_row)\n",
    "            print(f\"Added DEPOT_START for route {route_id}\")\n",
    "        \n",
    "        # Process all customer stops\n",
    "        customer_count = 0\n",
    "        for idx, location_id in enumerate(location_ids[1:-1], 1):  # Skip DEPOT_START and DEPOT_END\n",
    "            # Increment the sequence position for this stop in the route\n",
    "            sequence_position += 1\n",
    "            \n",
    "            # Check if this location_id has already been processed\n",
    "            if location_id in processed_location_ids:\n",
    "                print(f\"⚠️ Skipping duplicate location_id {location_id} in route {route_id}\")\n",
    "                continue\n",
    "                \n",
    "            # Track that we've processed this location_id\n",
    "            processed_location_ids.add(location_id)\n",
    "            \n",
    "            # Try to find this location in the dataset\n",
    "            matches = None\n",
    "            \n",
    "            # Try different strategies to match the location ID\n",
    "            if isinstance(location_id, (int, float)):\n",
    "                # Look for this ID in common ID columns\n",
    "                if 'ABS Custumer no' in enhanced_df.columns:\n",
    "                    matches = enhanced_df['ABS Custumer no'] == location_id\n",
    "                elif 'ABS Customer no' in enhanced_df.columns:\n",
    "                    matches = enhanced_df['ABS Customer no'] == location_id\n",
    "                elif 'customer_id' in enhanced_df.columns:\n",
    "                    matches = enhanced_df['customer_id'] == location_id\n",
    "                else:\n",
    "                    # Fallback to index if it's numeric\n",
    "                    matches = enhanced_df.index == location_id\n",
    "            else:\n",
    "                # If it's not numeric, it might be the index as string\n",
    "                try:\n",
    "                    # Try to convert to numeric if it's a string representation of a number\n",
    "                    numeric_id = float(location_id) if isinstance(location_id, str) else location_id\n",
    "                    matches = enhanced_df.index == numeric_id\n",
    "                except (ValueError, TypeError):\n",
    "                    # If conversion fails, it might be a string ID\n",
    "                    if 'customer_id' in enhanced_df.columns and enhanced_df['customer_id'].dtype == 'object':\n",
    "                        matches = enhanced_df['customer_id'] == location_id\n",
    "                    else:\n",
    "                        # No matches if we can't figure out how to match\n",
    "                        matches = pd.Series(False, index=enhanced_df.index)\n",
    "            \n",
    "            # If we found matches, process this stop\n",
    "            if matches is not None and matches.any():\n",
    "                matching_count = matches.sum()\n",
    "                customer_count += 1\n",
    "                \n",
    "                # Calculate segment distance and travel time - use actual HERE API data\n",
    "                segment_idx = idx - 1  # Index into segment arrays (excluding depot start)\n",
    "                \n",
    "                if segment_idx < len(segment_distances):\n",
    "                    segment_distance = segment_distances[segment_idx]\n",
    "                    segment_travel_time = segment_times[segment_idx]\n",
    "                else:\n",
    "                    print(f\"❌ Missing segment data for stop {idx} in route {route_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each matching row\n",
    "                for match_idx in enhanced_df[matches].index:\n",
    "                    # Calculate service time\n",
    "                    segment_service_time = 0\n",
    "                    if len(service_times) > idx:\n",
    "                        segment_service_time = service_times[idx]\n",
    "                    else:\n",
    "                        # Try to calculate it from the data\n",
    "                        try:\n",
    "                            # Get the specific row\n",
    "                            row = enhanced_df.loc[match_idx]\n",
    "                            \n",
    "                            # Extract weight with fallback options\n",
    "                            weight = None\n",
    "                            if 'Net Weight' in row and pd.notna(row['Net Weight']):\n",
    "                                weight = float(row['Net Weight'])\n",
    "                            elif 'weight' in row and pd.notna(row['weight']):\n",
    "                                weight = float(row['weight'])\n",
    "                            else:\n",
    "                                weight = 1.0\n",
    "                            \n",
    "                            # Calculate service time based on weight\n",
    "                            service_time_per_unit = 0.5  # Default value\n",
    "                            if 'service_time_per_unit_min' in route_info:\n",
    "                                service_time_per_unit = route_info['service_time_per_unit_min']\n",
    "                            \n",
    "                            segment_service_time = weight * service_time_per_unit\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error calculating service time: {e}\")\n",
    "                            segment_service_time = 0.5\n",
    "                    \n",
    "                    # Store all data for this stop\n",
    "                    stop_data = {\n",
    "                        'route_id': route_id,\n",
    "                        'sequence_idx': sequence_position,  # Store sequence for sorting\n",
    "                        'match_criteria': pd.Series([True if i == match_idx else False for i in enhanced_df.index], \n",
    "                                                   index=enhanced_df.index),\n",
    "                        'raw_data': {\n",
    "                            'route_distance_km': segment_distance,\n",
    "                            'travel_time_from_last_stop': segment_travel_time,\n",
    "                            'service_time_min': segment_service_time,\n",
    "                        },\n",
    "                        'is_depot': False,\n",
    "                        'is_multiple_match': matching_count > 1,\n",
    "                        'location_type': \"CUSTOMER\",\n",
    "                        'location_id': location_id,\n",
    "                        'depot_lat': depot_lat,\n",
    "                        'depot_lng': depot_lng\n",
    "                    }\n",
    "                    route_stops.append(stop_data)\n",
    "                    print(f\"Added customer stop for location_id {location_id} in route {route_id}, sequence {sequence_position}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: Could not find match for location_id {location_id} in route {route_id}\")\n",
    "        \n",
    "        # Process DEPOT_END\n",
    "        if depot_coords and location_ids[-1] == \"DEPOT_END\":\n",
    "            # Use the appropriate index for last segment\n",
    "            last_idx = len(segment_distances) - 1\n",
    "            if last_idx >= 0 and last_idx < len(segment_distances):\n",
    "                last_segment_distance = segment_distances[last_idx]\n",
    "                last_segment_time = segment_times[last_idx]\n",
    "                depot_end_service_time = service_times[-1] if len(service_times) > 0 else 45  # default 45 min\n",
    "            else:\n",
    "                print(f\"❌ Missing segment data for return to depot in route {route_id}\")\n",
    "                return route_stops\n",
    "            \n",
    "            depot_end_row = {\n",
    "                'route_id': route_id,\n",
    "                'sequence_idx': 999999,  # Special index to ensure it sorts last\n",
    "                'raw_data': {\n",
    "                    'route_distance_km': last_segment_distance,\n",
    "                    'travel_time_from_last_stop': last_segment_time,\n",
    "                    'service_time_min': depot_end_service_time,\n",
    "                },\n",
    "                'is_depot': True,\n",
    "                'is_depot_end': True,\n",
    "                'location_type': \"DEPOT_END\",\n",
    "                'cluster_name': depot_name,\n",
    "                'latitude': depot_lat,\n",
    "                'longitude': depot_lng,\n",
    "                'distance_to_depot_km': 0,\n",
    "                'depot_latitude': depot_lat,\n",
    "                'depot_longitude': depot_lng,\n",
    "                'depot_formatted_address': f\"Depot {depot_name}\",\n",
    "                'depot_geocode_confidence': 1.0  # Add a default confidence value for the depot\n",
    "            }\n",
    "            route_stops.append(depot_end_row)\n",
    "            print(f\"Added DEPOT_END for route {route_id}\")\n",
    "        \n",
    "        # Print diagnostics\n",
    "        print(f\"Route {route_id} has {len(route_stops)} total stops ({customer_count} customers)\")\n",
    "        \n",
    "        # ===== PHASE 2: Assign consistent route positions =====\n",
    "        # Sort stops by sequence_idx to ensure correct order\n",
    "        route_stops.sort(key=lambda x: x['sequence_idx'])\n",
    "        \n",
    "        # Assign clean, sequential positions\n",
    "        position = 0  # Start at 0 for DEPOT_START, then increment\n",
    "        \n",
    "        for stop in route_stops:\n",
    "            # Always assign sequential positions, regardless of stop type\n",
    "            stop['route_position'] = position\n",
    "            position += 1\n",
    "            \n",
    "            # For diagnostics\n",
    "            stop_type = \"DEPOT_START\" if stop.get('is_depot_start', False) else \\\n",
    "                       \"DEPOT_END\" if stop.get('is_depot_end', False) else \\\n",
    "                       \"CUSTOMER\"\n",
    "            print(f\"Assigned position {stop['route_position']} to {stop_type} in route {route_id}\")\n",
    "        \n",
    "        # ===== PHASE 3: Calculate cumulative times =====\n",
    "        cumulative_time = 0\n",
    "        \n",
    "        for i, stop in enumerate(route_stops):\n",
    "            travel_time = stop['raw_data']['travel_time_from_last_stop']\n",
    "            service_time = stop['raw_data']['service_time_min']\n",
    "            \n",
    "            if i == 0:\n",
    "                # First stop (DEPOT_START) - just service time\n",
    "                cumulative_time = service_time\n",
    "            else:\n",
    "                # All subsequent stops: add travel time and service time\n",
    "                cumulative_time += travel_time + service_time\n",
    "            \n",
    "            stop['cumulative_time_min'] = cumulative_time\n",
    "            print(f\"Stop {stop['route_position']} in route {route_id}: travel={travel_time}, service={service_time}, cumulative={cumulative_time}\")\n",
    "        \n",
    "        return route_stops\n",
    "    \n",
    "    # Process all routes with reduced concurrency to avoid rate limiting\n",
    "    print(f\"Processing {len(all_routes)} routes to enhance data...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_route = {executor.submit(process_route, route): route['route_id'] for route in all_routes}\n",
    "        \n",
    "        # Process results as they complete with a progress bar\n",
    "        with tqdm(total=len(future_to_route), desc=\"Processing routes\", unit=\"route\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_route):\n",
    "                route_id = future_to_route[future]\n",
    "                try:\n",
    "                    route_stops = future.result()\n",
    "                    all_stops.extend(route_stops)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing route {route_id}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # ===== PHASE 4: Update the dataframe with all calculated values =====\n",
    "    # First process the customer stops\n",
    "    print(f\"Updating dataframe with route information...\")\n",
    "    for stop in all_stops:\n",
    "        if not stop.get('is_depot', False):\n",
    "            matches = stop['match_criteria']\n",
    "            \n",
    "            # Update all route information at once\n",
    "            enhanced_df.loc[matches, 'route_id'] = str(stop['route_id'])\n",
    "            enhanced_df.loc[matches, 'route_position'] = stop['route_position']\n",
    "            enhanced_df.loc[matches, 'route_distance_km'] = stop['raw_data']['route_distance_km']\n",
    "            enhanced_df.loc[matches, 'travel_time_from_last_stop'] = stop['raw_data']['travel_time_from_last_stop']\n",
    "            enhanced_df.loc[matches, 'service_time_min'] = stop['raw_data']['service_time_min']\n",
    "            enhanced_df.loc[matches, 'cumulative_time_min'] = stop['cumulative_time_min']\n",
    "            enhanced_df.loc[matches, 'location_type'] = stop['location_type']\n",
    "            \n",
    "            # Add depot data if needed\n",
    "            if 'distance_to_depot_km' not in enhanced_df.columns:\n",
    "                enhanced_df['distance_to_depot_km'] = np.nan\n",
    "            \n",
    "            # Add other depot-related columns if needed\n",
    "            if 'depot_latitude' not in enhanced_df.columns and stop.get('depot_lat') is not None:\n",
    "                enhanced_df['depot_latitude'] = np.nan\n",
    "                enhanced_df.loc[matches, 'depot_latitude'] = stop['depot_lat']\n",
    "                \n",
    "            if 'depot_longitude' not in enhanced_df.columns and stop.get('depot_lng') is not None:\n",
    "                enhanced_df['depot_longitude'] = np.nan\n",
    "                enhanced_df.loc[matches, 'depot_longitude'] = stop['depot_lng']\n",
    "    \n",
    "    # Round numeric columns for better readability\n",
    "    print(\"Finalizing output format...\")\n",
    "    if 'route_distance_km' in enhanced_df.columns:\n",
    "        enhanced_df['route_distance_km'] = enhanced_df['route_distance_km'].round(2)\n",
    "    if 'travel_time_from_last_stop' in enhanced_df.columns:\n",
    "        enhanced_df['travel_time_from_last_stop'] = enhanced_df['travel_time_from_last_stop'].round(1)\n",
    "    if 'service_time_min' in enhanced_df.columns:\n",
    "        enhanced_df['service_time_min'] = enhanced_df['service_time_min'].round(1)\n",
    "    if 'cumulative_time_min' in enhanced_df.columns:\n",
    "        enhanced_df['cumulative_time_min'] = enhanced_df['cumulative_time_min'].round(1)\n",
    "    if 'distance_to_depot_km' in enhanced_df.columns:\n",
    "        enhanced_df['distance_to_depot_km'] = enhanced_df['distance_to_depot_km'].round(2)\n",
    "    \n",
    "    # Create a DataFrame for the depot stops\n",
    "    depot_rows = []\n",
    "    for stop in all_stops:\n",
    "        if stop.get('is_depot', False):\n",
    "            depot_row = {\n",
    "                'route_id': stop['route_id'],\n",
    "                'route_position': stop['route_position'],\n",
    "                'route_distance_km': stop['raw_data']['route_distance_km'],\n",
    "                'travel_time_from_last_stop': stop['raw_data']['travel_time_from_last_stop'],\n",
    "                'service_time_min': stop['raw_data']['service_time_min'],\n",
    "                'cumulative_time_min': stop['cumulative_time_min'],\n",
    "                'location_type': stop['location_type'],\n",
    "                'cluster_name': stop['cluster_name'],\n",
    "                'latitude': stop['latitude'],\n",
    "                'longitude': stop['longitude'],\n",
    "                'distance_to_depot_km': stop['distance_to_depot_km'],\n",
    "                'depot_latitude': stop['depot_latitude'],\n",
    "                'depot_longitude': stop['depot_longitude'],\n",
    "                'depot_formatted_address': stop['depot_formatted_address'],\n",
    "                'depot_geocode_confidence': stop.get('depot_geocode_confidence', 1.0)  # Ensure this field exists\n",
    "            }\n",
    "            depot_rows.append(depot_row)\n",
    "    \n",
    "    if depot_rows:\n",
    "        print(f\"Adding {len(depot_rows)} depot stops to final dataset...\")\n",
    "        # Create a DataFrame from depot rows\n",
    "        depot_df = pd.DataFrame(depot_rows)\n",
    "        \n",
    "        # Make sure it has the same columns as the original dataframe\n",
    "        for col in enhanced_df.columns:\n",
    "            if col not in depot_df.columns:\n",
    "                depot_df[col] = np.nan\n",
    "                \n",
    "        # Round numeric columns in depot_df\n",
    "        if 'route_distance_km' in depot_df.columns:\n",
    "            depot_df['route_distance_km'] = depot_df['route_distance_km'].round(2)\n",
    "        if 'travel_time_from_last_stop' in depot_df.columns:\n",
    "            depot_df['travel_time_from_last_stop'] = depot_df['travel_time_from_last_stop'].round(1)\n",
    "        if 'service_time_min' in depot_df.columns:\n",
    "            depot_df['service_time_min'] = depot_df['service_time_min'].round(1)\n",
    "        if 'cumulative_time_min' in depot_df.columns:\n",
    "            depot_df['cumulative_time_min'] = depot_df['cumulative_time_min'].round(1)\n",
    "        if 'distance_to_depot_km' in depot_df.columns:\n",
    "            depot_df['distance_to_depot_km'] = depot_df['distance_to_depot_km'].round(2)\n",
    "        \n",
    "        # Concatenate the enhanced dataframe with the depot rows\n",
    "        combined_df = pd.concat([enhanced_df, depot_df], ignore_index=True)\n",
    "        \n",
    "        # Sort by route_id and route_position for a clean output\n",
    "        combined_df = combined_df.sort_values(by=['route_id', 'route_position']).reset_index(drop=True)\n",
    "        \n",
    "        print(\"✅ Enhancement complete!\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        # Sort the enhanced dataframe by route_id and route_position\n",
    "        enhanced_df = enhanced_df.sort_values(by=['route_id', 'route_position']).reset_index(drop=True)\n",
    "        print(\"✅ Enhancement complete!\")\n",
    "        return enhanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91ae35-d4e9-43a0-a4f9-04563bdb7ee9",
   "metadata": {},
   "source": [
    "## Cell 11: Process Results with Enhanced Persistent Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f698d987-0a98-4160-be4c-f22b263a6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(all_routes, routes_summary, locations_not_visited, file_path, output_path, df_clean, depot_df=None):\n",
    "    \"\"\"Process and display optimization results with enhanced original data - Single file output\"\"\"\n",
    "    # Create enhanced data with route information - pass depot_df\n",
    "    enhanced_df = enhance_original_data(df_clean, all_routes, depot_df)\n",
    "    \n",
    "    # Rename 'Route Number' to 'legacy_route_number'\n",
    "    if 'Route Number' in enhanced_df.columns:\n",
    "        enhanced_df = enhanced_df.rename(columns={'Route Number': 'legacy_route_number'})\n",
    "    \n",
    "    # Clean up and enhance the export data:\n",
    "    # 1. Fill in empty 'customer' cells with depot names for depot rows\n",
    "    mask_depot = enhanced_df['location_type'].isin(['DEPOT_START', 'DEPOT_END'])\n",
    "    if 'Customer' in enhanced_df.columns:  # Note the capitalization in the original data\n",
    "        enhanced_df.loc[mask_depot, 'Customer'] = enhanced_df.loc[mask_depot, 'cluster_name'] + \\\n",
    "            \" (\" + enhanced_df.loc[mask_depot, 'location_type'] + \")\"\n",
    "    \n",
    "    # 2. Fill in empty 'formatted_address' cells with depot addresses\n",
    "    if 'formatted_address' in enhanced_df.columns and 'depot_formatted_address' in enhanced_df.columns:\n",
    "        enhanced_df.loc[mask_depot, 'formatted_address'] = enhanced_df.loc[mask_depot, 'depot_formatted_address']\n",
    "    \n",
    "    # 3. Fill in empty 'geocode_confidence' cells with depot geocode confidence\n",
    "    if 'geocode_confidence' in enhanced_df.columns and 'depot_geocode_confidence' in enhanced_df.columns:\n",
    "        enhanced_df.loc[mask_depot, 'geocode_confidence'] = enhanced_df.loc[mask_depot, 'depot_geocode_confidence']\n",
    "    \n",
    "    # 4. Remove specified columns\n",
    "    columns_to_remove = [\n",
    "        'row_no', \n",
    "        'Full address', \n",
    "        'depot_latitude', \n",
    "        'depot_longitude', \n",
    "        'depot_formatted_address', \n",
    "        'depot_geocode_confidence'\n",
    "    ]\n",
    "    \n",
    "    # Only remove columns that exist in the dataframe\n",
    "    columns_to_remove = [col for col in columns_to_remove if col in enhanced_df.columns]\n",
    "    if columns_to_remove:\n",
    "        enhanced_df = enhanced_df.drop(columns=columns_to_remove)\n",
    "    \n",
    "    # Summary dataframes for display purposes only\n",
    "    routes_df = pd.DataFrame(all_routes) if all_routes else pd.DataFrame()\n",
    "    summary_df = pd.DataFrame(routes_summary) if routes_summary else pd.DataFrame()\n",
    "    unvisited_df = pd.DataFrame(locations_not_visited) if locations_not_visited else pd.DataFrame()\n",
    "    \n",
    "    # Print optimization results\n",
    "    if not all_routes:\n",
    "        print(\"\\\\n❌ No routes could be created with the given constraints.\")\n",
    "        print(\"Try adjusting your parameters (increase vehicle capacity, driving time, etc.)\")\n",
    "    else:\n",
    "        print(\"\\\\n=== OPTIMIZATION RESULTS ===\")\n",
    "        print(f\"Total routes created: {len(all_routes)}\")\n",
    "        \n",
    "        # Use safer summation with error handling\n",
    "        try:\n",
    "            total_distance = summary_df['total_distance_km'].sum()\n",
    "            total_time = summary_df['total_time_hours'].sum()\n",
    "            print(f\"Total distance: {total_distance:.1f} km\")\n",
    "            print(f\"Total time: {total_time:.1f} hours\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating totals: {e}\")\n",
    "        \n",
    "        # Safely calculate visited percentage\n",
    "        try:\n",
    "            visited = summary_df['locations_visited'].sum() \n",
    "            total = summary_df['total_locations'].sum()\n",
    "            percent = (visited / total) * 100 if total > 0 else 0\n",
    "            print(f\"Locations served: {visited} out of {total} ({percent:.1f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating location statistics: {e}\")\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        try:\n",
    "            if visited > 0 and total_distance > 0:\n",
    "                stops_per_km = visited / total_distance\n",
    "                print(f\"Efficiency: {stops_per_km:.2f} stops per km\")\n",
    "            \n",
    "            if total_time > 0:\n",
    "                stops_per_hour = visited / total_time\n",
    "                print(f\"Productivity: {stops_per_hour:.2f} stops per hour\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating efficiency metrics: {e}\")\n",
    "        \n",
    "        # Display routes summary\n",
    "        if not summary_df.empty:\n",
    "            # Format the columns for better display\n",
    "            try:\n",
    "                summary_display = summary_df.copy()\n",
    "                summary_display['total_distance_km'] = summary_display['total_distance_km'].round(1)\n",
    "                summary_display['total_time_hours'] = summary_display['total_time_hours'].round(1)\n",
    "                \n",
    "                print(\"\\\\nRoutes summary by depot:\")\n",
    "                display_cols = [\n",
    "                    'depot_name', \n",
    "                    'routes_created', \n",
    "                    'locations_visited', \n",
    "                    'locations_unvisited',\n",
    "                    'total_distance_km',\n",
    "                    'total_time_hours'\n",
    "                ]\n",
    "                \n",
    "                # Check all columns exist before displaying\n",
    "                missing_cols = [col for col in display_cols if col not in summary_display.columns]\n",
    "                if missing_cols:\n",
    "                    print(f\"⚠️ Warning: Missing columns in summary: {missing_cols}\")\n",
    "                    display_cols = [col for col in display_cols if col in summary_display.columns]\n",
    "                \n",
    "                # Display the summary table with all metrics\n",
    "                if display_cols:\n",
    "                    print(summary_display[display_cols].to_string(index=False, \n",
    "                                                             formatters={\n",
    "                                                                 'total_distance_km': '{:.1f} km'.format,\n",
    "                                                                 'total_time_hours': '{:.1f} hrs'.format\n",
    "                                                             }))\n",
    "                else:\n",
    "                    print(\"No valid columns to display in summary\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying route summary: {e}\")\n",
    "        \n",
    "        # Show details about unvisited locations if any\n",
    "        if not unvisited_df.empty:\n",
    "            unvisited_count = len(unvisited_df)\n",
    "            print(f\"\\\\nUnvisited locations: {unvisited_count}\")\n",
    "            print(\"Reasons may include:\")\n",
    "            print(\"- Weight constraints exceeded\")\n",
    "            print(\"- Time constraints exceeded\")\n",
    "            print(\"- Locations too distant from depots\")\n",
    "            \n",
    "            # Calculate percentage of unvisited locations\n",
    "            if total > 0:\n",
    "                unvisited_pct = (unvisited_count / total) * 100\n",
    "                print(f\"Unvisited percentage: {unvisited_pct:.1f}%\")\n",
    "    \n",
    "    # Save the enhanced original data - single CSV output\n",
    "    output_filename = f\"{file_path.stem}_with_routes.csv\"\n",
    "    output_file = output_path / output_filename\n",
    "    \n",
    "    try:\n",
    "        # Save enhanced data to CSV\n",
    "        enhanced_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\\\n✅ Enhanced data with route details saved to: {output_file}\")\n",
    "        \n",
    "        # Cache route data for future use\n",
    "        cache_dir = Path.cwd() / 'cache'\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        routes_cache_file = cache_dir / f\"{file_path.stem}_routes.pkl\"\n",
    "        \n",
    "        with open(routes_cache_file, 'wb') as f:\n",
    "            pickle.dump(all_routes, f)\n",
    "        \n",
    "        print(f\"✅ Route data cached for future use at: {routes_cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "    \n",
    "    return enhanced_df, len(all_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab4845-3dd3-4fc7-b192-49eea7d81eae",
   "metadata": {},
   "source": [
    "## Cell 12: Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fcea408-4981-4e11-99bb-d1a30e44315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROUTE OPTIMIZATION WITH HERE API ===\n",
      "STATUS: Starting - Initializing [0%]                                                                ✅ HERE API key loaded successfully\n",
      "✅ Distance cache setup complete at C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "Using distance cache at: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "Project setup complete. \n",
      " Input path: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\02 Data\\Processed_data \n",
      " Output path: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\02 Data\\Processed_data\n",
      "Available files:\n",
      "1: 03_1_depot_centered_clusters.csv\n",
      "2: 03_1_depot_centered_clusters_test.csv\n",
      "3: time_and_km.csv\n",
      "STATUS: Data Loading - Reading input file [0%]                                                      "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose file number (1-3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting encoding for 03_1_depot_centered_clusters_test.csv...\n",
      "Detected encoding: UTF-8-SIG (confidence: 100.0%)\n",
      "\n",
      "Analyzing potential delimiters:\n",
      "\n",
      "1: Delimiter ','\n",
      "   Found 1 columns\n",
      "   Preview with option 1:\n",
      "                                                                                                                                                                                                                                                                                            ABS Custumer no;Route Number;Customer;Full address;Service;DeliveryQty;Net Weight;latitude;longitude;formatted_address;geocode_confidence;cluster_id;cluster_name;distance_to_depot_km;depot_latitude;depot_longitude;depot_formatted_address;depot_geocode_confidence\n",
      "128.0;4424.0;Tallinn                               Noorkuu 2//4//6// Vana -Rannamõisa tee 1e KÜ (3... Haabersti Tallinn             13516 Harju Maakond                                Eesti;1.0;1;Production LOO;18.28512461023954;59... Loo       Jõelähtme           74201 Harju Maakond                                         Eesti;0.99                                                                                                                                                                                                                    \n",
      "3319758.0;4024.0;KOTZEBUE TN 14 KÜ (3319758);Ko...  Põhja-Tallinn                                     Tallinn   10412 Harju Maakond Eesti;0.92;1;Production LOO;11.575645350519029;... Loo                                                Jõelähtme 74201 Harju Maakond Eesti;0.99                                                        None                                                                                                                                                                                                                    \n",
      "3340737.0;4414.0;J.KUNDERI 33 KORTERIÜHISTU         TALLINN (3340737);J.Kunderi 33 Tallinn;MAS;1.0... Kesklinn  Tallinn             10121 Harju Maakond                                Eesti;0.96;1;Production LOO;9.536006870399772;5... Loo       Jõelähtme           74201 Harju Maakond                                         Eesti;0.99                                                                                                                                                                                                                    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2: Delimiter ';'\n",
      "   Found 18 columns\n",
      "   Preview with option 2:\n",
      "   ABS Custumer no  Route Number  \\\n",
      "0            128.0        4424.0   \n",
      "1        3319758.0        4024.0   \n",
      "2        3340737.0        4414.0   \n",
      "\n",
      "                                            Customer  \\\n",
      "0  Tallinn,Noorkuu 2//4//6// Vana -Rannamõisa tee...   \n",
      "1                        KOTZEBUE TN 14 KÜ (3319758)   \n",
      "2      J.KUNDERI 33 KORTERIÜHISTU, TALLINN (3340737)   \n",
      "\n",
      "                                        Full address Service  DeliveryQty  \\\n",
      "0  Noorkuu 2/4/6 Vana-Rannamõisa tee 1e/1;1e/2 Ta...     MAS          5.0   \n",
      "1                             Kotzebue tn 14 TALLINN     MAS          3.0   \n",
      "2                               J.Kunderi 33 Tallinn     MAS          1.0   \n",
      "\n",
      "   Net Weight  latitude  longitude  \\\n",
      "0         6.1  59.42991   24.62090   \n",
      "1        14.9  59.44307   24.73922   \n",
      "2         6.1  59.43122   24.77581   \n",
      "\n",
      "                                   formatted_address  geocode_confidence  \\\n",
      "0  Vana-Rannamõisa tee 1e, Haabersti, Tallinn, 13...                1.00   \n",
      "1  Kotzebue 14, Põhja-Tallinn, Tallinn, 10412 Har...                0.92   \n",
      "2  Juhan Kunderi 33, Kesklinn, Tallinn, 10121 Har...                0.96   \n",
      "\n",
      "   cluster_id    cluster_name  distance_to_depot_km  depot_latitude  \\\n",
      "0           1  Production LOO             18.285125        59.43878   \n",
      "1           1  Production LOO             11.575645        59.43878   \n",
      "2           1  Production LOO              9.536007        59.43878   \n",
      "\n",
      "   depot_longitude                            depot_formatted_address  \\\n",
      "0          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "1          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "2          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "\n",
      "   depot_geocode_confidence  \n",
      "0                      0.99  \n",
      "1                      0.99  \n",
      "2                      0.99  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3: Delimiter '\\t'\n",
      "   Found 1 columns\n",
      "   Preview with option 3:\n",
      "  ABS Custumer no;Route Number;Customer;Full address;Service;DeliveryQty;Net Weight;latitude;longitude;formatted_address;geocode_confidence;cluster_id;cluster_name;distance_to_depot_km;depot_latitude;depot_longitude;depot_formatted_address;depot_geocode_confidence\n",
      "0  128.0;4424.0;Tallinn,Noorkuu 2//4//6// Vana -R...                                                                                                                                                                                                                    \n",
      "1  3319758.0;4024.0;KOTZEBUE TN 14 KÜ (3319758);K...                                                                                                                                                                                                                    \n",
      "2  3340737.0;4414.0;J.KUNDERI 33 KORTERIÜHISTU, T...                                                                                                                                                                                                                    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4: Delimiter '|'\n",
      "   Found 1 columns\n",
      "   Preview with option 4:\n",
      "  ABS Custumer no;Route Number;Customer;Full address;Service;DeliveryQty;Net Weight;latitude;longitude;formatted_address;geocode_confidence;cluster_id;cluster_name;distance_to_depot_km;depot_latitude;depot_longitude;depot_formatted_address;depot_geocode_confidence\n",
      "0  128.0;4424.0;Tallinn,Noorkuu 2//4//6// Vana -R...                                                                                                                                                                                                                    \n",
      "1  3319758.0;4024.0;KOTZEBUE TN 14 KÜ (3319758);K...                                                                                                                                                                                                                    \n",
      "2  3340737.0;4414.0;J.KUNDERI 33 KORTERIÜHISTU, T...                                                                                                                                                                                                                    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Suggested option: 2 (';') with 18 columns\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose delimiter option (1-4) [default: 2]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using delimiter: ';'\n",
      "\n",
      "✅ Loaded 10 rows × 18 columns from 03_1_depot_centered_clusters_test.csv\n",
      "\n",
      "Data Overview:\n",
      "Column names: ABS Custumer no, Route Number, Customer, Full address, Service, ... (and 13 more columns)\n",
      "\n",
      "Data types (first 5 columns):\n",
      "ABS Custumer no    float64\n",
      "Route Number       float64\n",
      "Customer            object\n",
      "Full address        object\n",
      "Service             object\n",
      "dtype: object\n",
      "... (and 13 more columns)\n",
      "\n",
      "Sample data:\n",
      "   ABS Custumer no  Route Number  \\\n",
      "0            128.0        4424.0   \n",
      "1        3319758.0        4024.0   \n",
      "2        3340737.0        4414.0   \n",
      "3        3424427.0        4434.0   \n",
      "4        3623344.0        1021.0   \n",
      "\n",
      "                                            Customer  \\\n",
      "0  Tallinn,Noorkuu 2//4//6// Vana -Rannamõisa tee...   \n",
      "1                        KOTZEBUE TN 14 KÜ (3319758)   \n",
      "2      J.KUNDERI 33 KORTERIÜHISTU, TALLINN (3340737)   \n",
      "3                HOUSE HALDUS OÜ/JAKOBI 28 (3424427)   \n",
      "4                               FLEXOIL OÜ (3623344)   \n",
      "\n",
      "                                        Full address Service  DeliveryQty  \\\n",
      "0  Noorkuu 2/4/6 Vana-Rannamõisa tee 1e/1;1e/2 Ta...     MAS          5.0   \n",
      "1                             Kotzebue tn 14 TALLINN     MAS          3.0   \n",
      "2                               J.Kunderi 33 Tallinn     MAS          1.0   \n",
      "3                                  Jakobi 28 Tallinn     MAS          3.0   \n",
      "4                                    Piirimäe 2 SAKU     MAS          2.0   \n",
      "\n",
      "   Net Weight  latitude  longitude  \\\n",
      "0         6.1  59.42991   24.62090   \n",
      "1        14.9  59.44307   24.73922   \n",
      "2         6.1  59.43122   24.77581   \n",
      "3        16.5  59.42738   24.77133   \n",
      "4        12.6  59.34130   24.62181   \n",
      "\n",
      "                                   formatted_address  geocode_confidence  \\\n",
      "0  Vana-Rannamõisa tee 1e, Haabersti, Tallinn, 13...                1.00   \n",
      "1  Kotzebue 14, Põhja-Tallinn, Tallinn, 10412 Har...                0.92   \n",
      "2  Juhan Kunderi 33, Kesklinn, Tallinn, 10121 Har...                0.96   \n",
      "3  Jakobi 28, Kesklinn, Tallinn, 10144 Harju Maak...                1.00   \n",
      "4  Piirimäe 2, Tänassilma, Saku, 76406 Harju Maak...                1.00   \n",
      "\n",
      "   cluster_id    cluster_name  distance_to_depot_km  depot_latitude  \\\n",
      "0           1  Production LOO             18.285125        59.43878   \n",
      "1           1  Production LOO             11.575645        59.43878   \n",
      "2           1  Production LOO              9.536007        59.43878   \n",
      "3           1  Production LOO              9.834789        59.43878   \n",
      "4           1  Production LOO             21.209769        59.43878   \n",
      "\n",
      "   depot_longitude                            depot_formatted_address  \\\n",
      "0          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "1          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "2          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "3          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "4          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "\n",
      "   depot_geocode_confidence  \n",
      "0                      0.99  \n",
      "1                      0.99  \n",
      "2                      0.99  \n",
      "3                      0.99  \n",
      "4                      0.99  \n",
      "Found cached routes for 03_1_depot_centered_clusters_test with 1 routes\n",
      "STATUS: Data Loading - Checking for cached routes [0%]                                              "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to use cached routes? This will skip optimization. (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cached routes, will run full optimization\n",
      "\n",
      "=== VALIDATING DATA QUALITY ===\n",
      "✅ Required location columns present\n",
      "✅ ID column(s) found: ABS Custumer no\n",
      "✅ 10 out of 10 rows (100.0%) have valid coordinates\n",
      "ℹ️ Found 2 rows with duplicate coordinates\n",
      "   This might be expected if multiple deliveries go to the same location\n",
      "   Example: These 2 rows share coordinates (59.39643, 24.7973):\n",
      "   ABS Custumer no  Route Number                   Customer  \\\n",
      "8        3642198.0        1421.0  HOUSE HALDUS OÜ (3642198)   \n",
      "9        3642198.0        4424.0  HOUSE HALDUS OÜ (3642198)   \n",
      "\n",
      "           Full address Service  DeliveryQty  Net Weight  latitude  longitude  \\\n",
      "8  Kopli tee 40, Peetri     MAS          1.0         6.1  59.39643    24.7973   \n",
      "9  Kopli tee 40, Peetri     MAS          1.0         6.1  59.39643    24.7973   \n",
      "\n",
      "                                   formatted_address  geocode_confidence  \\\n",
      "8  Kopli tee 40, Peetri, Rae, 75312 Harju Maakond...                 1.0   \n",
      "9  Kopli tee 40, Peetri, Rae, 75312 Harju Maakond...                 1.0   \n",
      "\n",
      "   cluster_id    cluster_name  distance_to_depot_km  depot_latitude  \\\n",
      "8           1  Production LOO              9.532399        59.43878   \n",
      "9           1  Production LOO              9.532399        59.43878   \n",
      "\n",
      "   depot_longitude                            depot_formatted_address  \\\n",
      "8          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "9          24.9438  Kuusiku tee 26, Loo, Jõelähtme, 74201 Harju Ma...   \n",
      "\n",
      "   depot_geocode_confidence  \n",
      "8                      0.99  \n",
      "9                      0.99  \n",
      "\n",
      "Validating depot assignment column: 'cluster_name'\n",
      "- Column data type: object\n",
      "- Unique values: 1 (showing first 5): ['Production LOO']\n",
      "- Missing values: 0 (0.0%)\n",
      "\n",
      "✅ Data validation complete\n",
      "\n",
      "=== DEPOT IDENTIFICATION ===\n",
      "Found 1 unique depots: Production LOO\n",
      "\n",
      "✅ Created information for 1 depots\n",
      "\n",
      "Depot information:\n",
      "   cluster_id      depot_name  latitude  longitude  location_count\n",
      "0           0  Production LOO  59.43878    24.9438              10\n",
      "\n",
      "=== OPERATIONAL CONSTRAINTS ===\n",
      "Now we'll set some parameters for the route optimization.\n",
      "These define the vehicle and time constraints for each route.\n",
      "STATUS: Configuration - Setting constraints [0%]                                                    "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vehicle weight/package capacity (default 800):  700\n",
      "Maximum route time in hours (default 8):  9\n",
      "Service time at depot (loading/unloading) in minutes (default 45):  \n",
      "Service time per unit in minutes (default 0.5):  0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization parameters:\n",
      "- Vehicle capacity: 700.0 units\n",
      "- Maximum route time: 9.0 hours (540 minutes)\n",
      "- Depot service time: 45 minutes\n",
      "- Service time per unit: 0.97 minutes\n",
      "\n",
      "=== OPTIMIZING ROUTES ===\n",
      "Starting route optimization process...\n",
      "✅ Distance cache setup complete at C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "\n",
      "Testing HERE API connectivity...\n",
      "✅ HERE API connection successful!\n",
      "Received 1 route sections in response\n",
      "\n",
      "Processing depot: Production LOO\n",
      "Found 10 locations assigned to this depot\n",
      "Optimizing routes for 10 locations...\n",
      "Using global optimization approach for 10 locations\n",
      "✅ Distance cache setup complete at C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "Starting global optimization with 10 locations\n",
      "Constraints: max_weight=700.0, max_time=540.0 min, depot_service_time=45 min\n",
      "\n",
      "STEP 1: Hierarchical clustering of locations...\n",
      "Minimum vehicles needed based on total weight (96.9): 1\n",
      "All locations can fit in a single vehicle (total weight: 96.9 / 700.0)\n",
      "Final clustering: 1 clusters for optimization\n",
      "\n",
      "STEP 2: Building distance matrices with batch processing...\n",
      "Calculating distances from depot to all 10 locations using batch API...\n",
      "Using individual routing requests instead of batch matrix API (slower)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating routes:   0%|          | 0/10 [00:00<?, ?pair/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: Optimization: Routing - Optimizing routes for 10 locations [Depot 1/1]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating routes: 100%|██████████| 10/10 [00:05<00:00,  1.86pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9 out of 10 route segments\n",
      "Building distance matrices within each cluster using batch API...\n",
      "Processing 45 location pairs for cluster 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster 0:   0%|          | 0/45 [00:00<?, ?pair/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using individual routing requests instead of batch matrix API (slower)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating routes:   0%|          | 0/45 [00:00<?, ?pair/s]\u001b[A\n",
      "Calculating routes:   2%|▏         | 1/45 [00:00<00:24,  1.83pair/s]\u001b[A\n",
      "Calculating routes:   4%|▍         | 2/45 [00:01<00:23,  1.86pair/s]\u001b[A\n",
      "Calculating routes:   7%|▋         | 3/45 [00:01<00:22,  1.85pair/s]\u001b[A\n",
      "Calculating routes:   9%|▉         | 4/45 [00:02<00:22,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  11%|█         | 5/45 [00:02<00:21,  1.84pair/s]\u001b[A\n",
      "Calculating routes:  13%|█▎        | 6/45 [00:03<00:21,  1.85pair/s]\u001b[A\n",
      "Calculating routes:  16%|█▌        | 7/45 [00:03<00:20,  1.85pair/s]\u001b[A\n",
      "Calculating routes:  18%|█▊        | 8/45 [00:04<00:20,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  20%|██        | 9/45 [00:04<00:19,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  22%|██▏       | 10/45 [00:05<00:19,  1.76pair/s]\u001b[A\n",
      "Calculating routes:  24%|██▍       | 11/45 [00:06<00:19,  1.78pair/s]\u001b[A\n",
      "Calculating routes:  27%|██▋       | 12/45 [00:06<00:18,  1.78pair/s]\u001b[A\n",
      "Calculating routes:  29%|██▉       | 13/45 [00:07<00:17,  1.79pair/s]\u001b[A\n",
      "Calculating routes:  31%|███       | 14/45 [00:07<00:17,  1.79pair/s]\u001b[A\n",
      "Calculating routes:  33%|███▎      | 15/45 [00:08<00:16,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  36%|███▌      | 16/45 [00:08<00:15,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  38%|███▊      | 17/45 [00:09<00:15,  1.81pair/s]\u001b[A\n",
      "Calculating routes:  40%|████      | 18/45 [00:09<00:14,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  42%|████▏     | 19/45 [00:10<00:14,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  44%|████▍     | 20/45 [00:11<00:13,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  47%|████▋     | 21/45 [00:11<00:13,  1.84pair/s]\u001b[A\n",
      "Calculating routes:  49%|████▉     | 22/45 [00:12<00:12,  1.84pair/s]\u001b[A\n",
      "Calculating routes:  51%|█████     | 23/45 [00:12<00:11,  1.84pair/s]\u001b[A\n",
      "Calculating routes:  53%|█████▎    | 24/45 [00:13<00:11,  1.85pair/s]\u001b[A\n",
      "Calculating routes:  56%|█████▌    | 25/45 [00:13<00:10,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  58%|█████▊    | 26/45 [00:14<00:10,  1.82pair/s]\u001b[A\n",
      "Calculating routes:  60%|██████    | 27/45 [00:14<00:09,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  62%|██████▏   | 28/45 [00:15<00:09,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  64%|██████▍   | 29/45 [00:15<00:08,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  67%|██████▋   | 30/45 [00:16<00:08,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  69%|██████▉   | 31/45 [00:17<00:07,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  71%|███████   | 32/45 [00:17<00:07,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  73%|███████▎  | 33/45 [00:18<00:06,  1.85pair/s]\u001b[A\n",
      "Calculating routes:  76%|███████▌  | 34/45 [00:18<00:06,  1.76pair/s]\u001b[A\n",
      "Calculating routes:  78%|███████▊  | 35/45 [00:19<00:05,  1.80pair/s]\u001b[A\n",
      "Calculating routes:  80%|████████  | 36/45 [00:19<00:04,  1.80pair/s]\u001b[A\n",
      "Calculating routes:  82%|████████▏ | 37/45 [00:20<00:04,  1.81pair/s]\u001b[A\n",
      "Calculating routes:  84%|████████▍ | 38/45 [00:20<00:03,  1.81pair/s]\u001b[A\n",
      "Calculating routes:  87%|████████▋ | 39/45 [00:21<00:03,  1.84pair/s]\u001b[A\n",
      "Calculating routes:  89%|████████▉ | 40/45 [00:21<00:02,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  91%|█████████ | 41/45 [00:22<00:02,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  93%|█████████▎| 42/45 [00:23<00:01,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  96%|█████████▌| 43/45 [00:23<00:01,  1.83pair/s]\u001b[A\n",
      "Calculating routes:  98%|█████████▊| 44/45 [00:24<00:00,  1.85pair/s]\u001b[A\n",
      "Calculating routes: 100%|██████████| 45/45 [00:24<00:00,  1.82pair/s]\u001b[A\n",
      "Cluster 0: 100%|██████████| 45/45 [00:24<00:00,  1.82pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 37 out of 45 route segments\n",
      "✓ Completed distance matrix for cluster 0\n",
      "\n",
      "STEP 3: Running Adaptive Large Neighborhood Search optimization...\n",
      "Creating initial solution with greedy insertion...\n",
      "Starting ALNS with 50 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New best! Cost: 64.17: 100%|██████████| 50/50 [00:07<00:00,  6.48iter/s, best_cost=64.17, current_cost=71.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALNS completed. Best solution has 1 routes with total cost 64.17\n",
      "\n",
      "✅ Optimization complete in 39.0 seconds!\n",
      "Created 1 routes with total distance: 82.52km\n",
      "Total time: 3.88hrs, Total weight: 104.00\n",
      "\n",
      "✅ Optimization complete in 39.3 seconds!\n",
      "✅ Distance cache setup complete at C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "Enriching routes with depot information...\n",
      "Processing 1 routes to enhance data...\n",
      "Processing route 1 for depot Production LOO\n",
      "Route 1 has 12 location_ids: ['DEPOT_START', 3639184.0, 3424427.0, 3623344.0, 128.0, 3638912.0, 3319758.0, 3340737.0, 3642198.0, 3642198.0, 3632290.0, 'DEPOT_END']\n",
      "Calculating missing segment data for route 1\n",
      "Calculating 11 segments for route 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing routes: 100%|██████████| 1/1 [00:00<00:00, 83.47route/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added DEPOT_START for route 1\n",
      "Added customer stop for location_id 3639184.0 in route 1, sequence 1\n",
      "Added customer stop for location_id 3424427.0 in route 1, sequence 2\n",
      "Added customer stop for location_id 3623344.0 in route 1, sequence 3\n",
      "Added customer stop for location_id 128.0 in route 1, sequence 4\n",
      "Added customer stop for location_id 3638912.0 in route 1, sequence 5\n",
      "Added customer stop for location_id 3319758.0 in route 1, sequence 6\n",
      "Added customer stop for location_id 3340737.0 in route 1, sequence 7\n",
      "Added customer stop for location_id 3642198.0 in route 1, sequence 8\n",
      "Added customer stop for location_id 3642198.0 in route 1, sequence 8\n",
      "⚠️ Skipping duplicate location_id 3642198.0 in route 1\n",
      "Added customer stop for location_id 3632290.0 in route 1, sequence 10\n",
      "Added DEPOT_END for route 1\n",
      "Route 1 has 12 total stops (9 customers)\n",
      "Assigned position 0 to DEPOT_START in route 1\n",
      "Assigned position 1 to CUSTOMER in route 1\n",
      "Assigned position 2 to CUSTOMER in route 1\n",
      "Assigned position 3 to CUSTOMER in route 1\n",
      "Assigned position 4 to CUSTOMER in route 1\n",
      "Assigned position 5 to CUSTOMER in route 1\n",
      "Assigned position 6 to CUSTOMER in route 1\n",
      "Assigned position 7 to CUSTOMER in route 1\n",
      "Assigned position 8 to CUSTOMER in route 1\n",
      "Assigned position 9 to CUSTOMER in route 1\n",
      "Assigned position 10 to CUSTOMER in route 1\n",
      "Assigned position 11 to DEPOT_END in route 1\n",
      "Stop 0 in route 1: travel=0, service=45, cumulative=45\n",
      "Stop 1 in route 1: travel=15.933333333333334, service=1.7, cumulative=62.63333333333333\n",
      "Stop 2 in route 1: travel=4.7, service=8.25, cumulative=75.58333333333333\n",
      "Stop 3 in route 1: travel=21.05, service=6.3, cumulative=102.93333333333334\n",
      "Stop 4 in route 1: travel=22.466666666666665, service=3.05, cumulative=128.45\n",
      "Stop 5 in route 1: travel=14.366666666666667, service=4.75, cumulative=147.56666666666666\n",
      "Stop 6 in route 1: travel=8.466666666666667, service=7.45, cumulative=163.48333333333332\n",
      "Stop 7 in route 1: travel=12.883333333333333, service=3.05, cumulative=179.41666666666666\n",
      "Stop 8 in route 1: travel=11.916666666666666, service=3.05, cumulative=194.38333333333333\n",
      "Stop 9 in route 1: travel=11.916666666666666, service=3.05, cumulative=209.35\n",
      "Stop 10 in route 1: travel=14.15, service=7.8, cumulative=231.29999999999998\n",
      "Stop 11 in route 1: travel=11.8, service=45, cumulative=288.09999999999997\n",
      "Updating dataframe with route information...\n",
      "Finalizing output format...\n",
      "Adding 2 depot stops to final dataset...\n",
      "✅ Enhancement complete!\n",
      "\\n=== OPTIMIZATION RESULTS ===\n",
      "Total routes created: 1\n",
      "Total distance: 82.5 km\n",
      "Total time: 3.9 hours\n",
      "Locations served: 10 out of 10 (100.0%)\n",
      "Efficiency: 0.12 stops per km\n",
      "Productivity: 2.58 stops per hour\n",
      "\\nRoutes summary by depot:\n",
      "    depot_name  routes_created  locations_visited  locations_unvisited total_distance_km total_time_hours\n",
      "Production LOO               1                 10                    0           82.5 km          3.9 hrs\n",
      "\\n✅ Enhanced data with route details saved to: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\02 Data\\Processed_data\\03_1_depot_centered_clusters_test_with_routes.csv\n",
      "✅ Route data cached for future use at: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\03_1_depot_centered_clusters_test_routes.pkl\n",
      "\n",
      "=== OPTIMIZATION COMPLETE ===\n",
      "Created 1 routes for 12 locations\n",
      "STATUS: Complete - Analyzing cache statistics [100%]                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CACHE STATISTICS ===\n",
      "Distance cache contains 26871 pre-calculated routes\n",
      "Cache is stored at: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\06 Sourcing data\\Anaconda\\03 Scripts\\cache\\distance_cache.db\n",
      "Next time you run optimization on this data, it will be much faster!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the route optimization\"\"\"\n",
    "    print(\"=== ROUTE OPTIMIZATION WITH HERE API ===\")\n",
    "    \n",
    "    # Create a status dictionary to track overall progress\n",
    "    status_dict = {\n",
    "        'stage': 'Starting',\n",
    "        'detail': 'Initializing',\n",
    "        'progress': '0%',\n",
    "        'done': False\n",
    "    }\n",
    "    \n",
    "    # Start a thread to display status\n",
    "    status_thread = threading.Thread(target=display_overall_status, args=(status_dict,), daemon=True)\n",
    "    status_thread.start()\n",
    "    \n",
    "    try:\n",
    "        # Load the HERE API key first to verify it's available\n",
    "        status_dict.update({'stage': 'Setup', 'detail': 'Checking API key'})\n",
    "        api_key = load_api_key()\n",
    "        if not api_key:\n",
    "            print(\"❌ ERROR: No HERE API key found. Route optimization requires HERE API.\")\n",
    "            print(\"Please create an api_keys.json file with a valid HERE API key.\")\n",
    "            status_dict['done'] = True\n",
    "            return\n",
    "        else:\n",
    "            print(f\"✅ HERE API key loaded successfully\")\n",
    "        \n",
    "        # Setup distance cache\n",
    "        status_dict.update({'detail': 'Setting up distance cache'})\n",
    "        cache_db_path = setup_distance_cache()\n",
    "        print(f\"Using distance cache at: {cache_db_path}\")\n",
    "        \n",
    "        # Setup project\n",
    "        status_dict.update({'detail': 'Setting up project paths'})\n",
    "        input_path, output_path = setup_project()\n",
    "        \n",
    "        # Load data\n",
    "        status_dict.update({'stage': 'Data Loading', 'detail': 'Reading input file'})\n",
    "        df, file_path = load_data(input_path)\n",
    "        \n",
    "        # Check if we have cached routes for this file\n",
    "        status_dict.update({'detail': 'Checking for cached routes'})\n",
    "        cache_dir = Path.cwd() / 'cache'\n",
    "        routes_cache_file = cache_dir / f\"{file_path.stem}_routes.pkl\"\n",
    "        cached_routes = None\n",
    "        \n",
    "        if routes_cache_file.exists():\n",
    "            try:\n",
    "                with open(routes_cache_file, 'rb') as f:\n",
    "                    cached_routes = pickle.load(f)\n",
    "                print(f\"Found cached routes for {file_path.stem} with {len(cached_routes)} routes\")\n",
    "                use_cache = input(\"Do you want to use cached routes? This will skip optimization. (y/n): \").strip().lower()\n",
    "                if use_cache != 'y':\n",
    "                    cached_routes = None\n",
    "                    print(\"Skipping cached routes, will run full optimization\")\n",
    "                else:\n",
    "                    print(\"Using cached routes to save time and API calls\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error loading cached routes: {e}\")\n",
    "                cached_routes = None\n",
    "        \n",
    "        # Validate data\n",
    "        status_dict.update({'stage': 'Data Preparation', 'detail': 'Validating data'})\n",
    "        df_clean = validate_data(df)\n",
    "        \n",
    "        # Create depot dataframe\n",
    "        status_dict.update({'detail': 'Identifying depots'})\n",
    "        depot_df = create_depot_dataframe(df_clean)\n",
    "        \n",
    "        # If using cached routes, skip optimization\n",
    "        if cached_routes:\n",
    "            status_dict.update({'stage': 'Using Cached Results', 'detail': 'Skipping optimization'})\n",
    "            print(\"Using cached routes. Skipping optimization.\")\n",
    "            all_routes = cached_routes\n",
    "            \n",
    "            # Create a placeholder summary\n",
    "            routes_summary = []\n",
    "            for route in all_routes:\n",
    "                depot_name = route.get('depot_name', 'Unknown')\n",
    "                # Count depot locations\n",
    "                depot_locs = df_clean[df_clean['cluster_name'] == depot_name]\n",
    "                \n",
    "                # Find if this depot is already in the summary\n",
    "                existing = next((s for s in routes_summary if s['depot_name'] == depot_name), None)\n",
    "                \n",
    "                if existing:\n",
    "                    existing['routes_created'] += 1\n",
    "                    existing['total_distance_km'] += route.get('distance_km', 0)\n",
    "                    existing['total_time_hours'] += route.get('time_min', 0) / 60\n",
    "                else:\n",
    "                    routes_summary.append({\n",
    "                        'depot_name': depot_name,\n",
    "                        'total_locations': len(depot_locs),\n",
    "                        'locations_visited': len(depot_locs),  # Assuming all visited\n",
    "                        'locations_unvisited': 0,\n",
    "                        'routes_created': 1,\n",
    "                        'total_distance_km': route.get('distance_km', 0),\n",
    "                        'total_time_hours': route.get('time_min', 0) / 60\n",
    "                    })\n",
    "            \n",
    "            # No unvisited locations when using cached routes\n",
    "            locations_not_visited = []\n",
    "        else:\n",
    "            # Get operational constraints\n",
    "            status_dict.update({'stage': 'Configuration', 'detail': 'Setting constraints'})\n",
    "            optimization_params = get_operational_constraints()\n",
    "            \n",
    "            # Run optimization with status updates\n",
    "            status_dict.update({'stage': 'Optimization', 'detail': 'Starting optimization process'})\n",
    "            try:\n",
    "                # Define a function to update status from within run_optimization\n",
    "                def update_optimization_status(stage, detail, progress=''):\n",
    "                    status_dict.update({\n",
    "                        'stage': f'Optimization: {stage}', \n",
    "                        'detail': detail, \n",
    "                        'progress': progress\n",
    "                    })\n",
    "                \n",
    "                # Modified run_optimization function to update status\n",
    "                def run_optimization_with_status(df_clean, depot_df, optimization_params):\n",
    "                    \"\"\"Run the optimization for each depot - with added status updates\"\"\"\n",
    "                    print(\"\\n=== OPTIMIZING ROUTES ===\")\n",
    "                    print(\"Starting route optimization process...\")\n",
    "                    \n",
    "                    # Initialize results containers\n",
    "                    all_routes = []\n",
    "                    routes_summary = []\n",
    "                    locations_not_visited = []\n",
    "                    \n",
    "                    # Performance tracking\n",
    "                    optimization_start_time = time.time()\n",
    "                    \n",
    "                    update_optimization_status('Setup', 'Testing API connection')\n",
    "                    \n",
    "                    # Global vehicle counter to ensure unique IDs across depots\n",
    "                    global_vehicle_count = 0\n",
    "                    \n",
    "                    # Test HERE API connectivity once at the beginning\n",
    "                    api_key = load_api_key()\n",
    "                    if api_key:\n",
    "                        # Setup distance cache\n",
    "                        cache_db_path = setup_distance_cache()\n",
    "                        \n",
    "                        # Use coordinates from the first depot for testing\n",
    "                        first_depot = depot_df.iloc[0]\n",
    "                        test_here_api_connection(\n",
    "                            api_key, \n",
    "                            first_depot['latitude'], \n",
    "                            first_depot['longitude']\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\"⚠️ No API key available for optimization\")\n",
    "                        return [], [], []\n",
    "                    \n",
    "                    # Process depots sequentially to avoid rate limiting\n",
    "                    total_depots = len(depot_df)\n",
    "                    for idx, depot_row in depot_df.iterrows():\n",
    "                        depot_name = depot_row['depot_name']\n",
    "                        update_optimization_status('Depots', f'Processing depot: {depot_name}', f'{idx+1}/{total_depots}')\n",
    "                        \n",
    "                        print(f\"\\nProcessing depot: {depot_name}\")\n",
    "                        \n",
    "                        # Get locations assigned to this depot using cluster_name\n",
    "                        depot_locations = df_clean[df_clean['cluster_name'] == depot_name]\n",
    "                        \n",
    "                        print(f\"Found {len(depot_locations)} locations assigned to this depot\")\n",
    "                        \n",
    "                        # Skip if no locations for this depot\n",
    "                        if len(depot_locations) == 0:\n",
    "                            print(f\"⚠️ No locations assigned to depot: {depot_name}\")\n",
    "                            continue\n",
    "                        \n",
    "                        print(f\"Optimizing routes for {len(depot_locations)} locations...\")\n",
    "                        update_optimization_status('Routing', f'Optimizing routes for {len(depot_locations)} locations', f'Depot {idx+1}/{total_depots}')\n",
    "                        \n",
    "                        # Run optimization with the global vehicle count to ensure unique IDs\n",
    "                        depot_routes, total_distance, total_time, vehicle_count, unvisited = create_routes(\n",
    "                            depot_locations, \n",
    "                            depot_row, \n",
    "                            optimization_params['vehicle_capacity'], \n",
    "                            optimization_params['max_route_time_min'], \n",
    "                            optimization_params['service_time_per_unit_min'],\n",
    "                            optimization_params['depot_service_time_min'],\n",
    "                            global_vehicle_count  # Pass the current global vehicle count\n",
    "                        )\n",
    "                        \n",
    "                        # Update the global vehicle count\n",
    "                        global_vehicle_count += vehicle_count\n",
    "                        \n",
    "                        # Update results\n",
    "                        all_routes.extend(depot_routes)\n",
    "                        \n",
    "                        # Create summary\n",
    "                        summary = {\n",
    "                            'depot_name': depot_name,\n",
    "                            'total_locations': len(depot_locations),\n",
    "                            'locations_visited': len(depot_locations) - len(unvisited),\n",
    "                            'locations_unvisited': len(unvisited),\n",
    "                            'routes_created': vehicle_count,\n",
    "                            'total_distance_km': total_distance,\n",
    "                            'total_time_hours': total_time / 60\n",
    "                        }\n",
    "                        routes_summary.append(summary)\n",
    "                        \n",
    "                        # Record unvisited locations\n",
    "                        if not unvisited.empty:\n",
    "                            for i, loc in unvisited.iterrows():\n",
    "                                locations_not_visited.append({\n",
    "                                    'location_id': i,\n",
    "                                    'depot_name': depot_name,\n",
    "                                    'latitude': loc['latitude'],\n",
    "                                    'longitude': loc['longitude'],\n",
    "                                    'weight': loc.get('weight', 1)\n",
    "                                })\n",
    "                    \n",
    "                    total_optimization_time = time.time() - optimization_start_time\n",
    "                    print(f\"\\n✅ Optimization complete in {total_optimization_time:.1f} seconds!\")\n",
    "                    \n",
    "                    update_optimization_status('Complete', f'Created {len(all_routes)} routes', '100%')\n",
    "                    \n",
    "                    return all_routes, routes_summary, locations_not_visited\n",
    "                \n",
    "                # Run the optimization with status updates\n",
    "                all_routes, routes_summary, locations_not_visited = run_optimization_with_status(\n",
    "                    df_clean, depot_df, optimization_params\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ ERROR during optimization: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                print(\"Please check your HERE API key, input data, and try again.\")\n",
    "                status_dict['done'] = True\n",
    "                return\n",
    "        \n",
    "        # Process results\n",
    "        status_dict.update({'stage': 'Finalizing', 'detail': 'Processing results', 'progress': '90%'})\n",
    "        enhanced_df, route_count = process_results(\n",
    "            all_routes, routes_summary, locations_not_visited, file_path, output_path, df_clean, depot_df\n",
    "        )\n",
    "        \n",
    "        status_dict.update({'stage': 'Complete', 'detail': f'Created {route_count} routes', 'progress': '100%'})\n",
    "        print(\"\\n=== OPTIMIZATION COMPLETE ===\")\n",
    "        print(f\"Created {route_count} routes for {len(enhanced_df)} locations\")\n",
    "        \n",
    "        # Analyze the distance cache usage\n",
    "        status_dict.update({'detail': 'Analyzing cache statistics'})\n",
    "        try:\n",
    "            conn = sqlite3.connect(cache_db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM distance_cache\")\n",
    "            cache_size = cursor.fetchone()[0]\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"\\n=== CACHE STATISTICS ===\")\n",
    "            print(f\"Distance cache contains {cache_size} pre-calculated routes\")\n",
    "            print(f\"Cache is stored at: {cache_db_path}\")\n",
    "            print(f\"Next time you run optimization on this data, it will be much faster!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing cache: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Ensure status thread is terminated\n",
    "        status_dict['done'] = True\n",
    "        time.sleep(1.5)  # Give status thread time to finish\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
